{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score,roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "from gensim.models import Word2Vec, FastText\n",
    "import os\n",
    "import itertools\n",
    "import multiprocessing\n",
    "warnings.filterwarnings('ignore')\n",
    "print(multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(DATA_PATH):\n",
    "    train_label = pd.read_csv(DATA_PATH+'train_label.csv')\n",
    "    train_base = pd.read_csv(DATA_PATH+'train_base.csv')\n",
    "    test_base = pd.read_csv(DATA_PATH+'test_a_base.csv')\n",
    "\n",
    "    train_op = pd.read_csv(DATA_PATH+'train_op.csv')\n",
    "    train_trans = pd.read_csv(DATA_PATH+'train_trans.csv')\n",
    "    test_op = pd.read_csv(DATA_PATH+'test_a_op.csv')\n",
    "    test_trans = pd.read_csv(DATA_PATH+'test_a_trans.csv')\n",
    "\n",
    "    return train_label, train_base, test_base, train_op, train_trans, test_op, test_trans\n",
    "\n",
    "\n",
    "def transform_time(x):\n",
    "    day = int(x.split(' ')[0])\n",
    "    hour = int(x.split(' ')[2].split('.')[0].split(':')[0])\n",
    "    minute = int(x.split(' ')[2].split('.')[0].split(':')[1])\n",
    "    second = int(x.split(' ')[2].split('.')[0].split(':')[2])\n",
    "    return 86400*day+3600*hour+60*minute+second\n",
    "\n",
    "\n",
    "def data_preprocess(DATA_PATH):\n",
    "    train_label, train_base, test_base, train_op, train_trans, test_op, test_trans = load_dataset(DATA_PATH=DATA_PATH)\n",
    "    # 拼接数据\n",
    "    train_df = train_base.copy()\n",
    "    test_df = test_base.copy()\n",
    "    train_df = train_label.merge(train_df, on=['user'], how='left')\n",
    "    del train_base, test_base\n",
    "\n",
    "    op_df = pd.concat([train_op, test_op], axis=0, ignore_index=True)\n",
    "    trans_df = pd.concat([train_trans, test_trans], axis=0, ignore_index=True)\n",
    "    data = pd.concat([train_df, test_df], axis=0, ignore_index=True)\n",
    "    del train_op, test_op, train_df, test_df\n",
    "    # 时间维度的处理\n",
    "    op_df['days_diff'] = op_df['tm_diff'].apply(lambda x: int(x.split(' ')[0]))\n",
    "    trans_df['days_diff'] = trans_df['tm_diff'].apply(lambda x: int(x.split(' ')[0]))\n",
    "    op_df['timestamp'] = op_df['tm_diff'].apply(lambda x: transform_time(x))\n",
    "    trans_df['timestamp'] = trans_df['tm_diff'].apply(lambda x: transform_time(x))\n",
    "    op_df['hour'] = op_df['tm_diff'].apply(lambda x: int(x.split(' ')[2].split('.')[0].split(':')[0]))\n",
    "    trans_df['hour'] = trans_df['tm_diff'].apply(lambda x: int(x.split(' ')[2].split('.')[0].split(':')[0]))\n",
    "    trans_df['week'] = trans_df['days_diff'].apply(lambda x: x % 7)\n",
    "    # 排序\n",
    "    trans_df = trans_df.sort_values(by=['user', 'timestamp'])\n",
    "    op_df = op_df.sort_values(by=['user', 'timestamp'])\n",
    "    trans_df.reset_index(inplace=True, drop=True)\n",
    "    op_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    gc.collect()\n",
    "    return data, op_df, trans_df\n",
    "\n",
    "\n",
    "#用户单特征的统计特征\n",
    "def gen_user_single_features(df,col):\n",
    "    group_df = df.groupby(['user']).agg(\n",
    "         mean=(col,'mean'),\n",
    "         std=(col,'std'),\n",
    "         max=(col,'max'),\n",
    "         min=(col,'min'),\n",
    "         sum=(col,'sum'),\n",
    "         median=(col,'median'),\n",
    "         count=(col,'count'),\n",
    "         skew=(col,'skew'),\n",
    "         var=(col,'var'),\n",
    "        ).reset_index().rename(columns={\"mean\":'user_{}_mean'.format(col),\n",
    "                        \"std\":'user_{}_std'.format(col),\n",
    "                        \"max\":'user_{}_max'.format(col),\n",
    "                        \"min\":'user_{}_min'.format(window),\n",
    "                        \"sum\":'user_{}_sum'.format(col),\n",
    "                        \"median\":'user_{}_median'.format(col),\n",
    "                        \"count\":'user_{}_count'.format(col),\n",
    "                        \"skew\":'user_{}_skew'.format(col),\n",
    "                        \"var\":'user_{}_var'.format(col),\n",
    "                        })\n",
    "    return group_df\n",
    "\n",
    "\n",
    "#trans#用户交易消费的特征\n",
    "def gen_user_amount_group_features(df,col,val):\n",
    "    df = df[ df[col]==val ]\n",
    "    l = ['mean','std','max','min','sum','median','count','skew','var']\n",
    "    group_df = df.groupby('user')['amount'].agg(l).reset_index()\n",
    "    group_df[ 'user_'+col+'_'+val+'_amount_jicha'] = group_df['max']-group_df['min']\n",
    "    rename_col = {}\n",
    "    for i in l:\n",
    "        rename_col[i] = 'user_'+col+'_'+val+'_amount_'+i\n",
    "    group_df = group_df.rename(columns=rename_col)\n",
    "    return group_df\n",
    "\n",
    "#trans#用户交易消费的特征\n",
    "def gen_user_amount_features(df):\n",
    "    group_df = df.groupby(['user']).agg(\n",
    "         user_amount_mean=('amount','mean'),\n",
    "         user_amount_std=('amount','std'),\n",
    "         user_amount_max=('amount','max'),\n",
    "         user_amount_min=('amount','min'),\n",
    "         user_amount_sum=('amount','sum'),\n",
    "         user_amount_med=('amount','median'),\n",
    "         user_amount_cnt=('amount','count'),\n",
    "         user_amount_skew=('amount','skew'),\n",
    "         user_amount_var=('amount','var'),\n",
    "        ).reset_index()\n",
    "    group_df['user_amount_jicha'] = group_df['user_amount_max'] - group_df['user_amount_min']\n",
    "    return group_df\n",
    "\n",
    "def gen_user_days_diff_group_features(df,col,val):\n",
    "    df = df[ df[col]==val ]\n",
    "    l = ['mean','std','max','min','sum','median','count','skew','var']\n",
    "    group_df = df.groupby('user')['days_diff'].agg(l).reset_index()\n",
    "    group_df[ 'user_'+col+'_'+val+'_days_diff_jicha'] = group_df['max']-group_df['min']\n",
    "    rename_col = {}\n",
    "    for i in l:\n",
    "        rename_col[i] = 'user_'+col+'_'+val+'_days_diff_'+i\n",
    "    group_df = group_df.rename(columns=rename_col)\n",
    "    return group_df\n",
    "\n",
    "#trans 用户在某平台或者用某ip的消费额特征\n",
    "def gen_user_group_amount_features(df, value):\n",
    "    group_df = df.pivot_table(index='user',\n",
    "                              columns=value,\n",
    "                              values='amount',\n",
    "                              dropna=False,\n",
    "                              aggfunc=['count','sum','median',\"skew\",'var','mean','std','max','min']).fillna(0)\n",
    "    group_df.columns = ['user_{}_{}_amount_{}'.format(value, f[1], f[0]) for f in group_df.columns]\n",
    "    group_df.reset_index(inplace=True)\n",
    "\n",
    "    return group_df\n",
    "\n",
    "\n",
    "#用户在某个时间段内的消费额特征 大于天数的窗口\n",
    "def gen_user_window_amount_features(df, window):\n",
    "    group_df = df[df['days_diff']>window].groupby('user').agg(\n",
    "        mean=('amount','mean'),\n",
    "        std=('amount','std'),\n",
    "        max=('amount','max'),\n",
    "        min=('amount','min'),\n",
    "        median=('amount','median'),\n",
    "        count=('amount','count'),\n",
    "        skew=('amount','skew'),\n",
    "        var=('amount','var'),\n",
    "        ).reset_index().rename(columns={\"mean\":'user_amount_mean_{}d'.format(window),\n",
    "                        \"std\":'user_amount_std_{}d'.format(window),\n",
    "                        \"max\":'user_amount_max_{}d'.format(window),\n",
    "                        \"min\":'user_amount_min_{}d'.format(window),\n",
    "                        \"sum\":'user_amount_sum_{}d'.format(window),\n",
    "                        \"median\":'user_amount_median_{}d'.format(window),\n",
    "                        \"count\":'user_amount_count_{}d'.format(window),\n",
    "                        \"skew\":'user_amount_skew_{}d'.format(window),\n",
    "                        \"var\":'user_amount_var_{}d'.format(window),\n",
    "                        })\n",
    "    return group_df\n",
    "\n",
    "\n",
    "#用户在某个时间段内的消费额特征 大于小时的窗口\n",
    "def gen_user_hourwindow_amount_features(df, window):\n",
    "    group_df = df[df['hour']>=window].groupby('user').agg(\n",
    "        mean=('amount','mean'),\n",
    "        count=('amount','count'),\n",
    "        ).reset_index().rename(columns={\"mean\":'user_amount_mean_{}h'.format(window),\n",
    "                        \"count\":'user_amount_count_{}h'.format(window),\n",
    "                        })\n",
    "    return group_df\n",
    "\n",
    "\n",
    "#trans用户  ['days_diff', 'platform', 'tunnel_in', 'tunnel_out', 'type1', 'type2', 'ip', 'ip_3']交易用到各字段的类数\n",
    "def gen_user_nunique_features(df, value, prefix):\n",
    "    group_df = df.groupby(['user'])[value].agg(\n",
    "        ['nunique','count']\n",
    "    ).reset_index().rename(columns={\"nunique\":'user_{}_{}_nuniq'.format(prefix, value),\n",
    "                                   'count':'user_{}_{}_cnt'.format(prefix, value)})\n",
    "    return group_df\n",
    "\n",
    "\n",
    "#trans用户无IP 这个特征贼奇怪 怎么会交易没有ip呢\n",
    "def gen_user_null_features(df, value, prefix):\n",
    "    df['is_null'] = 0\n",
    "    df.loc[df[value].isnull(), 'is_null'] = 1\n",
    "\n",
    "    group_df = df.groupby(['user'])['is_null'].agg(sum='sum',\n",
    "                            mean='mean').reset_index().rename(columns={\"sum\":'user_{}_{}_null_cnt'.format(prefix, value),\n",
    "                                                        \"mean\":'user_{}_{}_null_ratio'.format(prefix, value)})\n",
    "    return group_df\n",
    "\n",
    "\n",
    "# op op_mode op_tyep op_device net_type channel\n",
    "def gen_user_tfidf_features(df, value):\n",
    "    #填充缺失值\n",
    "    df[value].replace(' ', np.nan, inplace=True)\n",
    "    df[value] = df[value].astype(str)\n",
    "    df[value].fillna('-1', inplace=True)\n",
    "\n",
    "    #\n",
    "    group_df = df.groupby(['user']).apply(lambda x: x[value].tolist()).reset_index()\n",
    "    group_df.columns = ['user', 'list']\n",
    "    group_df['list'] = group_df['list'].apply(lambda x: ','.join(x))\n",
    "    enc_vec = TfidfVectorizer()\n",
    "    tfidf_vec = enc_vec.fit_transform(group_df['list'])\n",
    "    svd_enc = TruncatedSVD(n_components=10, n_iter=20, random_state=2020)\n",
    "    vec_svd = svd_enc.fit_transform(tfidf_vec)\n",
    "    vec_svd = pd.DataFrame(vec_svd)\n",
    "    vec_svd.columns = ['svd_tfidf_{}_{}'.format(value, i) for i in range(10)]\n",
    "    group_df = pd.concat([group_df, vec_svd], axis=1)\n",
    "    del group_df['list']\n",
    "    return group_df\n",
    "\n",
    "\n",
    "# op op_mode op_tyep op_device net_type channel\n",
    "def gen_user_countvec_features(df, value):\n",
    "    #填充缺失值\n",
    "    df[value].replace(' ', np.nan, inplace=True)\n",
    "    df[value] = df[value].astype(str)\n",
    "    df[value].fillna('-1', inplace=True)\n",
    "    #\n",
    "    group_df = df.groupby(['user']).apply(lambda x: x[value].tolist()).reset_index()\n",
    "    group_df.columns = ['user', 'list']\n",
    "    group_df['list'] = group_df['list'].apply(lambda x: ','.join(x))\n",
    "    enc_vec = CountVectorizer()\n",
    "    tfidf_vec = enc_vec.fit_transform(group_df['list'])\n",
    "    svd_enc = TruncatedSVD(n_components=10, n_iter=20, random_state=2020)\n",
    "    vec_svd = svd_enc.fit_transform(tfidf_vec)\n",
    "    vec_svd = pd.DataFrame(vec_svd)\n",
    "    vec_svd.columns = ['svd_countvec_{}_{}'.format(value, i) for i in range(10)]\n",
    "    group_df = pd.concat([group_df, vec_svd], axis=1)\n",
    "    del group_df['list']\n",
    "    return group_df\n",
    "\n",
    "\n",
    "#target_encode_cols = ['province', 'city', 'city_level', 'city_balance_avg']\n",
    "#target_encode\n",
    "def kfold_stats_feature(train, test, feats, k):\n",
    "    folds = StratifiedKFold(n_splits=k, shuffle=True, random_state=2020)  # 这里最好和后面模型的K折交叉验证保持一致\n",
    "\n",
    "    train['fold'] = None\n",
    "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(train, train['label'])):\n",
    "        train.loc[val_idx, 'fold'] = fold_\n",
    "\n",
    "    kfold_features = []\n",
    "    for feat in feats:\n",
    "        nums_columns = ['label']\n",
    "        #只有一个还用for?\n",
    "        for f in nums_columns:\n",
    "            colname = feat + '_' + f + '_kfold_mean'\n",
    "            kfold_features.append(colname)\n",
    "            train[colname] = None\n",
    "            for fold_, (trn_idx, val_idx) in enumerate(folds.split(train, train['label'])):\n",
    "                tmp_trn = train.iloc[trn_idx]\n",
    "                order_label = tmp_trn.groupby([feat])[f].mean()\n",
    "\n",
    "                tmp = train.loc[train.fold == fold_, [feat]]\n",
    "                train.loc[train.fold == fold_, colname] = tmp[feat].map(order_label)#其实就是用kfold方式对该特征的label标签mean化，然后建个新列\n",
    "                \n",
    "                # fillna 平均值填充\n",
    "                global_mean = train[f].mean()\n",
    "                train.loc[train.fold == fold_, colname] = train.loc[train.fold == fold_, colname].fillna(global_mean)\n",
    "            train[colname] = train[colname].astype(float)\n",
    "\n",
    "        for f in nums_columns:\n",
    "            colname = feat + '_' + f + '_kfold_mean'\n",
    "            test[colname] = None\n",
    "            order_label = train.groupby([feat])[f].mean()\n",
    "            test[colname] = test[feat].map(order_label)\n",
    "            # fillna\n",
    "            global_mean = train[f].mean()\n",
    "            test[colname] = test[colname].fillna(global_mean)\n",
    "            test[colname] = test[colname].astype(float)\n",
    "    del train['fold']\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def w2v_feat(df, feat, mode):\n",
    "    data_frame=df.copy()\n",
    "    # 转化为str\n",
    "    for i in feat:\n",
    "        if data_frame[i].dtype != 'object':\n",
    "            data_frame[i] = data_frame[i].astype(str)\n",
    "    data_frame.fillna('nan', inplace=True) # 甜宠\n",
    "\n",
    "    print(f'Start {mode} word2vec ...')\n",
    "    model = Word2Vec(data_frame[feat].values.tolist(), size=10, window=2, min_count=1,\n",
    "                     workers=multiprocessing.cpu_count(), iter=10)\n",
    "    stat_list = ['min', 'max', 'mean', 'std']\n",
    "    new_all = pd.DataFrame()\n",
    "    for m, t in enumerate(feat):\n",
    "        print(f'Start gen feat of {t} ...')\n",
    "        tmp = []\n",
    "        for i in data_frame[t].unique():\n",
    "            tmp_v = [i]\n",
    "            tmp_v.extend(model[i])\n",
    "            tmp.append(tmp_v)\n",
    "        tmp_df = pd.DataFrame(tmp)\n",
    "        w2c_list = [f'w2c_{t}_{n}' for n in range(10)]\n",
    "        tmp_df.columns = [t] + w2c_list\n",
    "        tmp_df = data_frame[['user', t]].merge(tmp_df, on=t)\n",
    "        tmp_df = tmp_df.drop_duplicates().groupby('user').agg(stat_list).reset_index()\n",
    "        tmp_df.columns = ['user'] + [f'{p}_{q}' for p in w2c_list for q in stat_list]\n",
    "        if m == 0:\n",
    "            new_all = pd.concat([new_all, tmp_df], axis=1)\n",
    "        else:\n",
    "            new_all = pd.merge(new_all, tmp_df, how='left', on='user')\n",
    "    return new_all\n",
    "\n",
    "\n",
    "def fasttext(df, feat, mode):\n",
    "    data_frame=df.copy()\n",
    "    # 转化为str\n",
    "    for i in feat:\n",
    "        if data_frame[i].dtype != 'object':\n",
    "            data_frame[i] = data_frame[i].astype(str)\n",
    "    data_frame.fillna('nan', inplace=True) # 甜宠\n",
    "\n",
    "    print(f'Start {mode} FastText ...')\n",
    "    model = FastText(data_frame[feat].values.tolist(),  size=10, window=3, min_count=1, \n",
    "                     workers=multiprocessing.cpu_count(), iter=10,min_n = 3 , max_n = 6,word_ngrams = 0)\n",
    "    stat_list = ['min', 'max', 'mean', 'std']\n",
    "    new_all = pd.DataFrame()\n",
    "    for m, t in enumerate(feat):\n",
    "        print(f'Start gen feat of {t} ...')\n",
    "        tmp = []\n",
    "        for i in data_frame[t].unique():\n",
    "            tmp_v = [i]\n",
    "            tmp_v.extend(model[i])\n",
    "            tmp.append(tmp_v)\n",
    "        tmp_df = pd.DataFrame(tmp)\n",
    "        w2c_list = [f'fast_text_{t}_{n}' for n in range(10)]\n",
    "        tmp_df.columns = [t] + w2c_list\n",
    "        tmp_df = data_frame[['user', t]].merge(tmp_df, on=t)\n",
    "        tmp_df = tmp_df.drop_duplicates().groupby('user').agg(stat_list).reset_index()\n",
    "        tmp_df.columns = ['user'] + [f'{p}_{q}' for p in w2c_list for q in stat_list]\n",
    "        if m == 0:\n",
    "            new_all = pd.concat([new_all, tmp_df], axis=1)\n",
    "        else:\n",
    "            new_all = pd.merge(new_all, tmp_df, how='left', on='user')\n",
    "    return new_all\n",
    "\n",
    "def gen_features(df, op, trans):\n",
    "\n",
    "    # base\n",
    "    print(\"base\")\n",
    "    df['product7_fail_ratio'] = df['product7_fail_cnt'] / df['product7_cnt']\n",
    "    df['city_count'] = df.groupby(['city'])['user'].transform('count')\n",
    "    df['province_count'] = df.groupby(['province'])['user'].transform('count')\n",
    "    df['op_cnt_avg'] = (df[\"op1_cnt\"]+df[\"op2_cnt\"])/df[\"using_time\"]\n",
    "    df['ip_cnt_avg'] = df[\"ip_cnt\"]/df[\"using_time\"]\n",
    "    df['card_a_cnt_avg'] = df[\"card_a_cnt\"]/df[\"using_time\"]\n",
    "    df['card_b_cnt_avg'] = df[\"card_b_cnt\"]/df[\"using_time\"]\n",
    "    df['card_c_cnt_avg'] = df[\"card_c_cnt\"]/df[\"using_time\"]\n",
    "    df['card_d_cnt_avg'] = df[\"card_d_cnt\"]/df[\"using_time\"]\n",
    "    df['login_cnt_didive'] = df[\"login_cnt_period1\"]/df[\"login_cnt_period2\"]\n",
    "    df['ip_per_day_cnt'] = df[\"ip_cnt\"]/df[\"login_days_cnt\"]\n",
    "    df['acc_count_per_time'] = df[\"acc_count\"]/df[\"using_time\"]\n",
    "    df['agreement_total_per_using_time'] = df[\"agreement_total\"]/df[\"using_time\"]\n",
    "    df['login_cnt'] = df[\"using_time\"]*df[\"login_cnt_avg\"]\n",
    "    df['agreement_total_per_account_cnt'] = df[\"agreement_total\"]/df[\"acc_count\"]\n",
    "    df['ip_cnt_per_acc_count'] = df[\"ip_cnt\"]/df[\"acc_count\"]\n",
    "    df['service1_amt_per_using_time'] = df[\"service1_amt\"]/df[\"using_time\"]\n",
    "    df[\"service_cnt_per_time\"]=(df[\"service1_cnt\"]+df[\"service1_cnt\"])/df[\"using_time\"]\n",
    "    \n",
    "    df['login_cnt_period_var']=df[['login_cnt_period1','login_cnt_period2']].std(axis=1)\n",
    "    df['login_cnt_period_CV']=df['login_cnt_period_var'] / df[['login_cnt_period1','login_cnt_period2']].mean(axis=1)\n",
    "    df['card_var']=df[['card_a_cnt','card_b_cnt','card_c_cnt','card_d_cnt']].std(axis=1)\n",
    "    df['card_CV']=df['card_var'] / df[['card_a_cnt','card_b_cnt','card_c_cnt','card_d_cnt']].mean(axis=1)\n",
    "    df['op_cnt_var']=df[['op1_cnt','op2_cnt']].std(axis=1)\n",
    "    df['op_cnt_CV']=df['op_cnt_var'] / df[['op1_cnt','op2_cnt']].mean(axis=1)\n",
    "    df['login_var']=df[['login_cnt_period1','login_cnt_period2','login_cnt_avg']].std(axis=1)\n",
    "    df['login_CV']=df['login_var'] / df[['login_cnt_period1','login_cnt_period2','login_cnt_avg']].mean(axis=1)\n",
    "    \n",
    "\n",
    "    # trans\n",
    "    df = df.merge(gen_user_amount_features(trans), on=['user'], how='left')\n",
    "    for col in tqdm(['days_diff','platform', 'tunnel_in', 'tunnel_out', 'type1', 'type2', 'ip', 'ip_3']):\n",
    "        df = df.merge(gen_user_nunique_features(df=trans, value=col, prefix='trans'), on=['user'], how='left')\n",
    "    df['user_amount_per_days'] = df['user_amount_sum'] / df['user_trans_days_diff_nuniq']\n",
    "    df['user_amount_per_cnt'] = df['user_amount_sum'] / df['user_amount_cnt']\n",
    "    \n",
    "    print(\"trans group_amount_features\") \n",
    "#     df = df.merge(gen_user_group_amount_features(df=trans, value='platform'), on=['user'], how='left')\n",
    "#     df = df.merge(gen_user_group_amount_features(df=trans, value='type1'), on=['user'], how='left')\n",
    "#     df = df.merge(gen_user_group_amount_features(df=trans, value='type2'), on=['user'], how='left')\n",
    "#     df = df.merge(gen_user_group_amount_features(df=trans, value='tunnel_in'), on=['user'], how='left')\n",
    "#     df = df.merge(gen_user_group_amount_features(df=trans, value='tunnel_out'), on=['user'], how='left')\n",
    "    print(\"trans window\") \n",
    "    df = df.merge(gen_user_window_amount_features(df=trans, window=27), on=['user'], how='left')\n",
    "    df = df.merge(gen_user_window_amount_features(df=trans, window=23), on=['user'], how='left')\n",
    "    df = df.merge(gen_user_window_amount_features(df=trans, window=15), on=['user'], how='left')\n",
    "    df = df.merge(gen_user_hourwindow_amount_features(df=trans, window=20), on=['user'], how='left')\n",
    "    df = df.merge(gen_user_hourwindow_amount_features(df=trans, window=15), on=['user'], how='left')\n",
    "    df = df.merge(gen_user_hourwindow_amount_features(df=trans, window=10), on=['user'], how='left')\n",
    "    print(\"trans null_features\")\n",
    "    df = df.merge(gen_user_null_features(df=trans, value='type2', prefix='trans'), on=['user'], how='left')\n",
    "    df = df.merge(gen_user_null_features(df=trans, value='ip_3', prefix='trans'), on=['user'], how='left')\n",
    "    df = df.merge(gen_user_null_features(df=trans, value='tunnel_in', prefix='trans'), on=['user'], how='left')\n",
    "    df = df.merge(gen_user_null_features(df=trans, value='tunnel_out', prefix='trans'), on=['user'], how='left')\n",
    "\n",
    "    print(\"type1\")\n",
    "    for i in [\"674e8d5860bc033d\",'443b0fd0860c21b6','45a1168437c708ff','f67d4b5a05a1352a','443b0fd0860c21b6',\n",
    "              '3146295fbf43c0cb','8adb3dcfea9dcf5e','33e9d4cef01499e1','19d44f1a51919482','0a3cf8dac7dca9d1']:\n",
    "        df = df.merge(gen_user_days_diff_group_features(trans,'type1',i), on=['user'], how='left')\n",
    "        df = df.merge(gen_user_amount_group_features(trans,'type1',i), on=['user'], how='left')\n",
    "    \n",
    "    print(\"type2\")\n",
    "    for i in ['11a213398ee0c623','2ee592ab06090eb5','b5a8be737a50b171','2ee592ab06090eb5']:\n",
    "        df = df.merge(gen_user_days_diff_group_features(trans,'type1',i), on=['user'], how='left')\n",
    "        df = df.merge(gen_user_amount_group_features(trans,'type1',i), on=['user'], how='left')\n",
    "        \n",
    "    print(\"tunnel_in\")\n",
    "    for i in ['b2e7fa260df4998d']:\n",
    "        df = df.merge(gen_user_days_diff_group_features(trans,'type1',i), on=['user'], how='left')\n",
    "        df = df.merge(gen_user_amount_group_features(trans,'type1',i), on=['user'], how='left')\n",
    "        \n",
    "    print(\"tunnel_out\")\n",
    "    for i in ['6ee790756007e69a','4c8524fb01d8b204']:\n",
    "        df = df.merge(gen_user_days_diff_group_features(trans,'type1',i), on=['user'], how='left')\n",
    "        df = df.merge(gen_user_amount_group_features(trans,'type1',i), on=['user'], how='left')\n",
    "    \n",
    "    df = df.merge(gen_user_tfidf_features(df=trans, value='amount'), on=['user'], how='left')\n",
    "    df = df.merge(gen_user_countvec_features(df=trans, value='amount'), on=['user'], how='left')\n",
    "    \n",
    "    df=df.merge(fasttext(trans,['amount','type1',\"type2\"],\"train\"),on=['user'],how='left')\n",
    "\n",
    "    print(\"op\")\n",
    "    # op\n",
    "    print(\"op null features\")\n",
    "    df = df.merge(gen_user_null_features(df=op, value=\"net_type\", prefix='op'), on=['user'], how='left')\n",
    "    df = df.merge(gen_user_null_features(df=op, value=\"op_device\", prefix='op'), on=['user'], how='left')\n",
    "    df = df.merge(gen_user_null_features(df=op, value=\"ip\", prefix='op'), on=['user'], how='left') \n",
    "\n",
    "    df=df.merge(op.groupby(\"user\").agg(user_op_days_diff_mean=('days_diff','mean')),on=['user'], how='left')\n",
    "    df=df.merge(op.groupby(\"user\").agg(user_op_hour_mean=('hour','mean')),on=['user'], how='left')\n",
    "\n",
    "    print(\"op nunique features\")\n",
    "    for col in tqdm(['days_diff','hour', 'op_mode', 'op_type', 'op_device', 'channel', 'ip',\"ip_3\"]):\n",
    "        df = df.merge(gen_user_nunique_features(df=op, value=col, prefix='op'), on=['user'], how='left')\n",
    "\n",
    "    print(\"op w2v tfidf countvec encoder features\")\n",
    "    df = df.merge(gen_user_tfidf_features(df=op, value='op_mode'), on=['user'], how='left')\n",
    "    df = df.merge(gen_user_tfidf_features(df=op, value='op_type'), on=['user'], how='left')\n",
    "\n",
    "    df = df.merge(gen_user_countvec_features(df=op, value='op_mode'), on=['user'], how='left')\n",
    "    df = df.merge(gen_user_countvec_features(df=op, value='op_type'), on=['user'], how='left')\n",
    "\n",
    "    df=df.merge(fasttext(op,['op_mode','op_type',\"op_device\",\"channel\",'ip_3',],\"train\"),on=['user'],how='left')\n",
    "    \n",
    "    for col in tqdm([f for f in df.select_dtypes('object').columns if f not in ['user']]):\n",
    "        le = LabelEncoder()\n",
    "        df[col] = df[col].apply(lambda x:str(x))\n",
    "        df[col].fillna('-1', inplace=True)\n",
    "        df[col] = le.fit_transform(df[col])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def lgb_model(train, target, test, k):\n",
    "    feats = [f for f in train.columns if f not in ['user', 'label']]\n",
    "    print('Current num of features:', len(feats))\n",
    "    folds = StratifiedKFold(n_splits=k, shuffle=True, random_state=2020)\n",
    "    oof_probs = np.zeros(train.shape[0])\n",
    "    output_preds = 0\n",
    "    offline_score = []\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    parameters = {\n",
    "        'learning_rate': 0.03,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'num_leaves': 50,\n",
    "        'feature_fraction': 0.8,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'min_data_in_leaf': 20,\n",
    "        'reg_alpha':10,\n",
    "        'reg_lambda':8,\n",
    "        'verbose': -1,\n",
    "        'nthread': 8,\n",
    "        'colsample_bytree':0.77,\n",
    "        'min_child_weight':4,\n",
    "        'min_child_samples':10,\n",
    "        'min_split_gain':0,\n",
    "        'lambda_l1': 0.8,\n",
    "    }\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(folds.split(train, target)):\n",
    "        train_y, test_y = target[train_index], target[test_index]\n",
    "        train_X, test_X = train[feats].iloc[train_index, :], train[feats].iloc[test_index, :]\n",
    "\n",
    "        dtrain = lgb.Dataset(train_X,\n",
    "                             label=train_y)\n",
    "        dval = lgb.Dataset(test_X,\n",
    "                           label=test_y)\n",
    "        lgb_model = lgb.train(\n",
    "                parameters,\n",
    "                dtrain,\n",
    "                num_boost_round=5000,\n",
    "                valid_sets=[dval],\n",
    "                early_stopping_rounds=100,\n",
    "                verbose_eval=100,\n",
    "        )\n",
    "        oof_probs[test_index] = lgb_model.predict(test_X[feats], num_iteration=lgb_model.best_iteration)\n",
    "        offline_score.append(lgb_model.best_score['valid_0']['auc'])\n",
    "        output_preds += lgb_model.predict(test[feats], num_iteration=lgb_model.best_iteration)/folds.n_splits\n",
    "        print(offline_score)\n",
    "        # feature importance\n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        print(fold_importance_df.shape)\n",
    "        fold_importance_df[\"feature\"] = feats\n",
    "        fold_importance_df[\"importance\"] = lgb_model.feature_importance(importance_type='gain')\n",
    "        fold_importance_df[\"fold\"] = i + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    print('OOF-MEAN-AUC:%.6f, OOF-STD-AUC:%.6f' % (np.mean(offline_score), np.std(offline_score)))\n",
    "    print('feature importance:')\n",
    "    print(feature_importance_df.groupby(['feature'])['importance'].mean().sort_values(ascending=False).head(50))\n",
    "    feature_importance_df.groupby(['feature'])['importance'].mean().to_csv(\"feature_importance.csv\")\n",
    "    return output_preds, oof_probs, np.mean(offline_score),feature_importance_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "读取数据...\n",
      "开始特征工程...\n",
      "base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:06<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trans group_amount_features\n",
      "trans window\n",
      "trans null_features\n",
      "type1\n",
      "type2\n",
      "tunnel_in\n",
      "tunnel_out\n",
      "op\n",
      "op null features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "op nunique features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:27<00:00,  3.40s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:02<00:00, 11.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 52s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "DATA_PATH = 'dataset/'\n",
    "print('读取数据...')\n",
    "data, op_df, trans_df = data_preprocess(DATA_PATH=DATA_PATH)\n",
    "\n",
    "print('开始特征工程...')\n",
    "data = gen_features(data, op_df, trans_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "op w2v tfidata countvec encoder features\n",
      "Start train FastText ...\n",
      "Start gen feat of op_mode ...\n",
      "Start gen feat of op_type ...\n",
      "Start gen feat of op_device ...\n",
      "Start gen feat of channel ...\n",
      "Start gen feat of ip_3 ...\n"
     ]
    }
   ],
   "source": [
    "# data = data.merge(fasttext(trans_df,['amount','type1',\"type2\"],\"train\"),on=['user'],how='left')\n",
    "print(\"op w2v tfidata countvec encoder features\")\n",
    "data = data.merge(gen_user_tfidf_features(df=op_df, value='op_mode'), on=['user'], how='left')\n",
    "data = data.merge(gen_user_tfidf_features(df=op_df, value='op_type'), on=['user'], how='left')\n",
    "\n",
    "data = data.merge(gen_user_countvec_features(df=op_df, value='op_mode'), on=['user'], how='left')\n",
    "data = data.merge(gen_user_countvec_features(df=op_df, value='op_type'), on=['user'], how='left')\n",
    "\n",
    "data=data.merge(fasttext(op_df,['op_mode','op_type',\"op_device\",\"channel\",'ip_3',],\"train\"),on=['user'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>label</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>provider</th>\n",
       "      <th>level</th>\n",
       "      <th>verified</th>\n",
       "      <th>using_time</th>\n",
       "      <th>regist_type</th>\n",
       "      <th>card_a_cnt</th>\n",
       "      <th>...</th>\n",
       "      <th>fast_text_ip_3_7_mean</th>\n",
       "      <th>fast_text_ip_3_7_std</th>\n",
       "      <th>fast_text_ip_3_8_min</th>\n",
       "      <th>fast_text_ip_3_8_max</th>\n",
       "      <th>fast_text_ip_3_8_mean</th>\n",
       "      <th>fast_text_ip_3_8_std</th>\n",
       "      <th>fast_text_ip_3_9_min</th>\n",
       "      <th>fast_text_ip_3_9_max</th>\n",
       "      <th>fast_text_ip_3_9_mean</th>\n",
       "      <th>fast_text_ip_3_9_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Train_00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>24871</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>24712</td>\n",
       "      <td>1</td>\n",
       "      <td>24712</td>\n",
       "      <td>...</td>\n",
       "      <td>14.432362</td>\n",
       "      <td>13.592972</td>\n",
       "      <td>2.098201</td>\n",
       "      <td>6.468091</td>\n",
       "      <td>3.293974</td>\n",
       "      <td>2.118353</td>\n",
       "      <td>-2.647424</td>\n",
       "      <td>0.377193</td>\n",
       "      <td>-0.457421</td>\n",
       "      <td>1.467357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Train_00001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>24889</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>24716</td>\n",
       "      <td>1</td>\n",
       "      <td>24719</td>\n",
       "      <td>...</td>\n",
       "      <td>8.177504</td>\n",
       "      <td>0.740996</td>\n",
       "      <td>1.372291</td>\n",
       "      <td>2.251632</td>\n",
       "      <td>1.811962</td>\n",
       "      <td>0.621788</td>\n",
       "      <td>-0.397058</td>\n",
       "      <td>0.694136</td>\n",
       "      <td>0.148539</td>\n",
       "      <td>0.771591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Train_00002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>24963</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>24736</td>\n",
       "      <td>7</td>\n",
       "      <td>24712</td>\n",
       "      <td>...</td>\n",
       "      <td>8.799004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.272885</td>\n",
       "      <td>2.272885</td>\n",
       "      <td>2.272885</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.218462</td>\n",
       "      <td>0.218462</td>\n",
       "      <td>0.218462</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Train_00003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>24840</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>24719</td>\n",
       "      <td>3</td>\n",
       "      <td>24712</td>\n",
       "      <td>...</td>\n",
       "      <td>12.871061</td>\n",
       "      <td>10.766720</td>\n",
       "      <td>1.843446</td>\n",
       "      <td>6.468091</td>\n",
       "      <td>2.795300</td>\n",
       "      <td>1.805512</td>\n",
       "      <td>-2.647424</td>\n",
       "      <td>0.882987</td>\n",
       "      <td>0.028378</td>\n",
       "      <td>1.333109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Train_00004</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>24871</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>24707</td>\n",
       "      <td>3</td>\n",
       "      <td>24712</td>\n",
       "      <td>...</td>\n",
       "      <td>9.730333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.434301</td>\n",
       "      <td>1.434301</td>\n",
       "      <td>1.434301</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.365158</td>\n",
       "      <td>-0.365158</td>\n",
       "      <td>-0.365158</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72092</th>\n",
       "      <td>TestA_19183</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>24859</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>24741</td>\n",
       "      <td>7</td>\n",
       "      <td>24719</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72093</th>\n",
       "      <td>TestA_12697</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>24889</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>24714</td>\n",
       "      <td>3</td>\n",
       "      <td>24725</td>\n",
       "      <td>...</td>\n",
       "      <td>14.579079</td>\n",
       "      <td>13.493845</td>\n",
       "      <td>1.459726</td>\n",
       "      <td>6.468091</td>\n",
       "      <td>2.983793</td>\n",
       "      <td>2.340422</td>\n",
       "      <td>-2.647424</td>\n",
       "      <td>0.625268</td>\n",
       "      <td>-0.425036</td>\n",
       "      <td>1.511289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72094</th>\n",
       "      <td>TestA_07983</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>24950</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>24709</td>\n",
       "      <td>3</td>\n",
       "      <td>24719</td>\n",
       "      <td>...</td>\n",
       "      <td>21.379618</td>\n",
       "      <td>19.004328</td>\n",
       "      <td>2.440553</td>\n",
       "      <td>6.468091</td>\n",
       "      <td>4.454322</td>\n",
       "      <td>2.847899</td>\n",
       "      <td>-2.647424</td>\n",
       "      <td>0.537634</td>\n",
       "      <td>-1.054895</td>\n",
       "      <td>2.252176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72095</th>\n",
       "      <td>TestA_01730</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>24925</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>24740</td>\n",
       "      <td>7</td>\n",
       "      <td>24731</td>\n",
       "      <td>...</td>\n",
       "      <td>7.782197</td>\n",
       "      <td>0.520957</td>\n",
       "      <td>2.227375</td>\n",
       "      <td>2.277093</td>\n",
       "      <td>2.252234</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.690002</td>\n",
       "      <td>0.794858</td>\n",
       "      <td>0.742430</td>\n",
       "      <td>0.074145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72096</th>\n",
       "      <td>TestA_00199</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>24871</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>24721</td>\n",
       "      <td>3</td>\n",
       "      <td>24712</td>\n",
       "      <td>...</td>\n",
       "      <td>8.040286</td>\n",
       "      <td>0.193831</td>\n",
       "      <td>1.790262</td>\n",
       "      <td>2.113945</td>\n",
       "      <td>2.002041</td>\n",
       "      <td>0.150721</td>\n",
       "      <td>0.420949</td>\n",
       "      <td>0.991437</td>\n",
       "      <td>0.627973</td>\n",
       "      <td>0.259810</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72097 rows × 883 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              user  label  sex    age  provider  level  verified  using_time  \\\n",
       "0      Train_00000    0.0    0  24871         0      1         0       24712   \n",
       "1      Train_00001    1.0    0  24889         0      2         0       24716   \n",
       "2      Train_00002    0.0    0  24963         0      2         0       24736   \n",
       "3      Train_00003    0.0    0  24840         2      2         0       24719   \n",
       "4      Train_00004    0.0    0  24871         2      1         0       24707   \n",
       "...            ...    ...  ...    ...       ...    ...       ...         ...   \n",
       "72092  TestA_19183    NaN    0  24859         0      1         0       24741   \n",
       "72093  TestA_12697    NaN    1  24889         0      2         0       24714   \n",
       "72094  TestA_07983    NaN    0  24950         0      2         0       24709   \n",
       "72095  TestA_01730    NaN    0  24925         0      2         0       24740   \n",
       "72096  TestA_00199    NaN    0  24871         1      1         0       24721   \n",
       "\n",
       "       regist_type  card_a_cnt  ...  fast_text_ip_3_7_mean  \\\n",
       "0                1       24712  ...              14.432362   \n",
       "1                1       24719  ...               8.177504   \n",
       "2                7       24712  ...               8.799004   \n",
       "3                3       24712  ...              12.871061   \n",
       "4                3       24712  ...               9.730333   \n",
       "...            ...         ...  ...                    ...   \n",
       "72092            7       24719  ...                    NaN   \n",
       "72093            3       24725  ...              14.579079   \n",
       "72094            3       24719  ...              21.379618   \n",
       "72095            7       24731  ...               7.782197   \n",
       "72096            3       24712  ...               8.040286   \n",
       "\n",
       "       fast_text_ip_3_7_std  fast_text_ip_3_8_min  fast_text_ip_3_8_max  \\\n",
       "0                 13.592972              2.098201              6.468091   \n",
       "1                  0.740996              1.372291              2.251632   \n",
       "2                       NaN              2.272885              2.272885   \n",
       "3                 10.766720              1.843446              6.468091   \n",
       "4                       NaN              1.434301              1.434301   \n",
       "...                     ...                   ...                   ...   \n",
       "72092                   NaN                   NaN                   NaN   \n",
       "72093             13.493845              1.459726              6.468091   \n",
       "72094             19.004328              2.440553              6.468091   \n",
       "72095              0.520957              2.227375              2.277093   \n",
       "72096              0.193831              1.790262              2.113945   \n",
       "\n",
       "       fast_text_ip_3_8_mean  fast_text_ip_3_8_std  fast_text_ip_3_9_min  \\\n",
       "0                   3.293974              2.118353             -2.647424   \n",
       "1                   1.811962              0.621788             -0.397058   \n",
       "2                   2.272885                   NaN              0.218462   \n",
       "3                   2.795300              1.805512             -2.647424   \n",
       "4                   1.434301                   NaN             -0.365158   \n",
       "...                      ...                   ...                   ...   \n",
       "72092                    NaN                   NaN                   NaN   \n",
       "72093               2.983793              2.340422             -2.647424   \n",
       "72094               4.454322              2.847899             -2.647424   \n",
       "72095               2.252234              0.035156              0.690002   \n",
       "72096               2.002041              0.150721              0.420949   \n",
       "\n",
       "       fast_text_ip_3_9_max  fast_text_ip_3_9_mean  fast_text_ip_3_9_std  \n",
       "0                  0.377193              -0.457421              1.467357  \n",
       "1                  0.694136               0.148539              0.771591  \n",
       "2                  0.218462               0.218462                   NaN  \n",
       "3                  0.882987               0.028378              1.333109  \n",
       "4                 -0.365158              -0.365158                   NaN  \n",
       "...                     ...                    ...                   ...  \n",
       "72092                   NaN                    NaN                   NaN  \n",
       "72093              0.625268              -0.425036              1.511289  \n",
       "72094              0.537634              -1.054895              2.252176  \n",
       "72095              0.794858               0.742430              0.074145  \n",
       "72096              0.991437               0.627973              0.259810  \n",
       "\n",
       "[72097 rows x 883 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target encoder...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data['city_level'] = data['city'].map(str) + '_' + data['level'].map(str)\n",
    "data['city_product1_amount'] = data['city'].map(str) + '_' + data['product1_amount'].map(str)\n",
    "data['city_balance1_avg'] = data['city'].map(str) + '_' + data['balance1_avg'].map(str)\n",
    "data['city_balance2_avg'] = data['city'].map(str) + '_' + data['balance2_avg'].map(str)\n",
    "data['city_balance'] = data['city'].map(str) + '_' + data['balance'].map(str)\n",
    "data['city_card_a_cnt'] = data['city'].map(str) + '_' + data['card_a_cnt'].map(str)\n",
    "\n",
    "\n",
    "train = data[~data['label'].isnull()].copy()\n",
    "target = train['label']\n",
    "test = data[data['label'].isnull()].copy()\n",
    "\n",
    "print(\"target encoder...\")\n",
    "  \n",
    "target_encode_cols = ['province','city',\"city_level\",'city_balance1_avg',\n",
    "                      'city_balance2_avg',\"city_product1_amount\",'city_balance','city_card_a_cnt']\n",
    "\n",
    "train, test = kfold_stats_feature(train, test, target_encode_cols, 10)\n",
    "train.drop(['province', \"city\",\"service3_level\",\"city_level\",'city_balance1_avg','city_balance2_avg',\n",
    "              \"city_product1_amount\",'city_balance','city_card_a_cnt'], axis=1, inplace=True)\n",
    "test.drop(['province',\"city\",\"service3_level\",\"city_level\",'city_balance1_avg','city_balance2_avg',\n",
    "              \"city_product1_amount\",'city_balance','city_card_a_cnt'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('train.csv',index=False)\n",
    "target.to_csv('target.csv',index=False)\n",
    "test.to_csv('test.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始模型训练...\n",
      "Current num of features: 886\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's auc: 0.739149\n",
      "[200]\tvalid_0's auc: 0.74447\n",
      "[300]\tvalid_0's auc: 0.744226\n",
      "Early stopping, best iteration is:\n",
      "[251]\tvalid_0's auc: 0.744901\n",
      "[0.7449013172628429]\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's auc: 0.746218\n",
      "[200]\tvalid_0's auc: 0.750275\n",
      "[300]\tvalid_0's auc: 0.751576\n",
      "Early stopping, best iteration is:\n",
      "[282]\tvalid_0's auc: 0.752146\n",
      "[0.7449013172628429, 0.7521460507338793]\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's auc: 0.733825\n",
      "[200]\tvalid_0's auc: 0.737679\n",
      "[300]\tvalid_0's auc: 0.74077\n",
      "[400]\tvalid_0's auc: 0.740445\n",
      "Early stopping, best iteration is:\n",
      "[329]\tvalid_0's auc: 0.741163\n",
      "[0.7449013172628429, 0.7521460507338793, 0.7411627523108847]\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's auc: 0.738744\n",
      "[200]\tvalid_0's auc: 0.744392\n",
      "[300]\tvalid_0's auc: 0.744867\n",
      "[400]\tvalid_0's auc: 0.746305\n",
      "[500]\tvalid_0's auc: 0.746714\n",
      "[600]\tvalid_0's auc: 0.745917\n",
      "Early stopping, best iteration is:\n",
      "[503]\tvalid_0's auc: 0.746734\n",
      "[0.7449013172628429, 0.7521460507338793, 0.7411627523108847, 0.7467338709677419]\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's auc: 0.725342\n",
      "[200]\tvalid_0's auc: 0.730996\n",
      "[300]\tvalid_0's auc: 0.731352\n",
      "[400]\tvalid_0's auc: 0.732927\n",
      "[500]\tvalid_0's auc: 0.732918\n",
      "Early stopping, best iteration is:\n",
      "[428]\tvalid_0's auc: 0.73349\n",
      "[0.7449013172628429, 0.7521460507338793, 0.7411627523108847, 0.7467338709677419, 0.7334901433691756]\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's auc: 0.728512\n",
      "[200]\tvalid_0's auc: 0.733277\n",
      "[300]\tvalid_0's auc: 0.73496\n",
      "[400]\tvalid_0's auc: 0.735963\n",
      "[500]\tvalid_0's auc: 0.736871\n",
      "Early stopping, best iteration is:\n",
      "[462]\tvalid_0's auc: 0.73731\n",
      "[0.7449013172628429, 0.7521460507338793, 0.7411627523108847, 0.7467338709677419, 0.7334901433691756, 0.7373097057159027]\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's auc: 0.729487\n",
      "[200]\tvalid_0's auc: 0.735656\n",
      "[300]\tvalid_0's auc: 0.737761\n",
      "[400]\tvalid_0's auc: 0.738771\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalid_0's auc: 0.738907\n",
      "[0.7449013172628429, 0.7521460507338793, 0.7411627523108847, 0.7467338709677419, 0.7334901433691756, 0.7373097057159027, 0.7389070458404075]\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's auc: 0.727471\n",
      "[200]\tvalid_0's auc: 0.732483\n",
      "[300]\tvalid_0's auc: 0.73359\n",
      "[400]\tvalid_0's auc: 0.734864\n",
      "[500]\tvalid_0's auc: 0.734879\n",
      "Early stopping, best iteration is:\n",
      "[410]\tvalid_0's auc: 0.735385\n",
      "[0.7449013172628429, 0.7521460507338793, 0.7411627523108847, 0.7467338709677419, 0.7334901433691756, 0.7373097057159027, 0.7389070458404075, 0.7353848330503678]\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's auc: 0.718843\n",
      "[200]\tvalid_0's auc: 0.723929\n",
      "[300]\tvalid_0's auc: 0.72494\n",
      "Early stopping, best iteration is:\n",
      "[275]\tvalid_0's auc: 0.725224\n",
      "[0.7449013172628429, 0.7521460507338793, 0.7411627523108847, 0.7467338709677419, 0.7334901433691756, 0.7373097057159027, 0.7389070458404075, 0.7353848330503678, 0.7252242501414827]\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's auc: 0.745622\n",
      "[200]\tvalid_0's auc: 0.74814\n",
      "[300]\tvalid_0's auc: 0.748269\n",
      "[400]\tvalid_0's auc: 0.748731\n",
      "Early stopping, best iteration is:\n",
      "[350]\tvalid_0's auc: 0.749101\n",
      "[0.7449013172628429, 0.7521460507338793, 0.7411627523108847, 0.7467338709677419, 0.7334901433691756, 0.7373097057159027, 0.7389070458404075, 0.7353848330503678, 0.7252242501414827, 0.7491013488021128]\n",
      "OOF-MEAN-AUC:0.740436, OOF-STD-AUC:0.007679\n",
      "train auc: 0.7403072942844433\n",
      "Wall time: 17min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('开始模型训练...')\n",
    "\n",
    "lgb_preds, lgb_oof, lgb_score,feature_importance_df = lgb_model(train=train, target=target, test=test,k=10)\n",
    "auc_score = roc_auc_score(target.values, lgb_oof)\n",
    "print(\"train auc:\",auc_score)\n",
    "\n",
    "sub_df = test[['user']].copy()\n",
    "sub_df['prob'] = lgb_preds\n",
    "sub_df.to_csv('sub_lgb{%.5f}.csv'%(lgb_score,), index=False)\n",
    "# 0.741833"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>label</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>provider</th>\n",
       "      <th>level</th>\n",
       "      <th>verified</th>\n",
       "      <th>using_time</th>\n",
       "      <th>regist_type</th>\n",
       "      <th>card_a_cnt</th>\n",
       "      <th>...</th>\n",
       "      <th>fast_text_ip_3_9_mean</th>\n",
       "      <th>fast_text_ip_3_9_std</th>\n",
       "      <th>province_label_kfold_mean</th>\n",
       "      <th>city_label_kfold_mean</th>\n",
       "      <th>city_level_label_kfold_mean</th>\n",
       "      <th>city_balance1_avg_label_kfold_mean</th>\n",
       "      <th>city_balance2_avg_label_kfold_mean</th>\n",
       "      <th>city_product1_amount_label_kfold_mean</th>\n",
       "      <th>city_balance_label_kfold_mean</th>\n",
       "      <th>city_card_a_cnt_label_kfold_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47782</th>\n",
       "      <td>TestA_21427</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>24853</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>24713</td>\n",
       "      <td>1</td>\n",
       "      <td>24719</td>\n",
       "      <td>...</td>\n",
       "      <td>0.325241</td>\n",
       "      <td>0.244119</td>\n",
       "      <td>0.227933</td>\n",
       "      <td>0.242705</td>\n",
       "      <td>0.248918</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>0.238431</td>\n",
       "      <td>0.254276</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.231746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47783</th>\n",
       "      <td>TestA_08694</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>25011</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>24743</td>\n",
       "      <td>7</td>\n",
       "      <td>24712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.559621</td>\n",
       "      <td>0.208273</td>\n",
       "      <td>0.357078</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>0.311475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47784</th>\n",
       "      <td>TestA_13449</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>24877</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>24744</td>\n",
       "      <td>7</td>\n",
       "      <td>24719</td>\n",
       "      <td>...</td>\n",
       "      <td>0.106659</td>\n",
       "      <td>1.075293</td>\n",
       "      <td>0.212571</td>\n",
       "      <td>0.242321</td>\n",
       "      <td>0.217778</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.139241</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.171875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47785</th>\n",
       "      <td>TestA_10937</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>24925</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>24715</td>\n",
       "      <td>1</td>\n",
       "      <td>24712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.150957</td>\n",
       "      <td>1.017913</td>\n",
       "      <td>0.221616</td>\n",
       "      <td>0.176101</td>\n",
       "      <td>0.178571</td>\n",
       "      <td>0.212121</td>\n",
       "      <td>0.122951</td>\n",
       "      <td>0.243902</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.202532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47786</th>\n",
       "      <td>TestA_14194</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>24877</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>24706</td>\n",
       "      <td>3</td>\n",
       "      <td>24712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.899547</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.227933</td>\n",
       "      <td>0.168000</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.254545</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.238806</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.176471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72092</th>\n",
       "      <td>TestA_19183</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>24859</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>24741</td>\n",
       "      <td>7</td>\n",
       "      <td>24719</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.212571</td>\n",
       "      <td>0.190769</td>\n",
       "      <td>0.305085</td>\n",
       "      <td>0.246536</td>\n",
       "      <td>0.276596</td>\n",
       "      <td>0.240838</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.157895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72093</th>\n",
       "      <td>TestA_12697</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>24889</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>24714</td>\n",
       "      <td>3</td>\n",
       "      <td>24725</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.425036</td>\n",
       "      <td>1.511289</td>\n",
       "      <td>0.212571</td>\n",
       "      <td>0.190769</td>\n",
       "      <td>0.166038</td>\n",
       "      <td>0.262295</td>\n",
       "      <td>0.276596</td>\n",
       "      <td>0.150943</td>\n",
       "      <td>0.207547</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72094</th>\n",
       "      <td>TestA_07983</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>24950</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>24709</td>\n",
       "      <td>3</td>\n",
       "      <td>24719</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.054895</td>\n",
       "      <td>2.252176</td>\n",
       "      <td>0.234980</td>\n",
       "      <td>0.241830</td>\n",
       "      <td>0.219697</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>0.220930</td>\n",
       "      <td>0.255102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.294118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72095</th>\n",
       "      <td>TestA_01730</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>24925</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>24740</td>\n",
       "      <td>7</td>\n",
       "      <td>24731</td>\n",
       "      <td>...</td>\n",
       "      <td>0.742430</td>\n",
       "      <td>0.074145</td>\n",
       "      <td>0.221616</td>\n",
       "      <td>0.176101</td>\n",
       "      <td>0.178571</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.122951</td>\n",
       "      <td>0.115942</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72096</th>\n",
       "      <td>TestA_00199</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>24871</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>24721</td>\n",
       "      <td>3</td>\n",
       "      <td>24712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.627973</td>\n",
       "      <td>0.259810</td>\n",
       "      <td>0.204749</td>\n",
       "      <td>0.187220</td>\n",
       "      <td>0.232955</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.180685</td>\n",
       "      <td>0.206723</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.200765</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24315 rows × 888 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              user  label  sex    age  provider  level  verified  using_time  \\\n",
       "47782  TestA_21427    NaN    0  24853         0      1         0       24713   \n",
       "47783  TestA_08694    NaN    1  25011         0      1         0       24743   \n",
       "47784  TestA_13449    NaN    1  24877         0      2         0       24744   \n",
       "47785  TestA_10937    NaN    0  24925         0      2         0       24715   \n",
       "47786  TestA_14194    NaN    1  24877         2      1         0       24706   \n",
       "...            ...    ...  ...    ...       ...    ...       ...         ...   \n",
       "72092  TestA_19183    NaN    0  24859         0      1         0       24741   \n",
       "72093  TestA_12697    NaN    1  24889         0      2         0       24714   \n",
       "72094  TestA_07983    NaN    0  24950         0      2         0       24709   \n",
       "72095  TestA_01730    NaN    0  24925         0      2         0       24740   \n",
       "72096  TestA_00199    NaN    0  24871         1      1         0       24721   \n",
       "\n",
       "       regist_type  card_a_cnt  ...  fast_text_ip_3_9_mean  \\\n",
       "47782            1       24719  ...               0.325241   \n",
       "47783            7       24712  ...               0.559621   \n",
       "47784            7       24719  ...               0.106659   \n",
       "47785            1       24712  ...               0.150957   \n",
       "47786            3       24712  ...               0.899547   \n",
       "...            ...         ...  ...                    ...   \n",
       "72092            7       24719  ...                    NaN   \n",
       "72093            3       24725  ...              -0.425036   \n",
       "72094            3       24719  ...              -1.054895   \n",
       "72095            7       24731  ...               0.742430   \n",
       "72096            3       24712  ...               0.627973   \n",
       "\n",
       "       fast_text_ip_3_9_std  province_label_kfold_mean  city_label_kfold_mean  \\\n",
       "47782              0.244119                   0.227933               0.242705   \n",
       "47783              0.208273                   0.357078               0.318182   \n",
       "47784              1.075293                   0.212571               0.242321   \n",
       "47785              1.017913                   0.221616               0.176101   \n",
       "47786                   NaN                   0.227933               0.168000   \n",
       "...                     ...                        ...                    ...   \n",
       "72092                   NaN                   0.212571               0.190769   \n",
       "72093              1.511289                   0.212571               0.190769   \n",
       "72094              2.252176                   0.234980               0.241830   \n",
       "72095              0.074145                   0.221616               0.176101   \n",
       "72096              0.259810                   0.204749               0.187220   \n",
       "\n",
       "       city_level_label_kfold_mean  city_balance1_avg_label_kfold_mean  \\\n",
       "47782                     0.248918                            0.258065   \n",
       "47783                     0.416667                            0.346154   \n",
       "47784                     0.217778                            0.111111   \n",
       "47785                     0.178571                            0.212121   \n",
       "47786                     0.172414                            0.254545   \n",
       "...                            ...                                 ...   \n",
       "72092                     0.305085                            0.246536   \n",
       "72093                     0.166038                            0.262295   \n",
       "72094                     0.219697                            0.275862   \n",
       "72095                     0.178571                            0.250000   \n",
       "72096                     0.232955                            0.238095   \n",
       "\n",
       "       city_balance2_avg_label_kfold_mean  \\\n",
       "47782                            0.238431   \n",
       "47783                            0.304348   \n",
       "47784                            0.181818   \n",
       "47785                            0.122951   \n",
       "47786                            0.227273   \n",
       "...                                   ...   \n",
       "72092                            0.276596   \n",
       "72093                            0.276596   \n",
       "72094                            0.220930   \n",
       "72095                            0.122951   \n",
       "72096                            0.180685   \n",
       "\n",
       "       city_product1_amount_label_kfold_mean  city_balance_label_kfold_mean  \\\n",
       "47782                               0.254276                       0.272727   \n",
       "47783                               0.350000                       0.391304   \n",
       "47784                               0.139241                       0.285714   \n",
       "47785                               0.243902                       0.333333   \n",
       "47786                               0.238806                       0.172414   \n",
       "...                                      ...                            ...   \n",
       "72092                               0.240838                       0.071429   \n",
       "72093                               0.150943                       0.207547   \n",
       "72094                               0.255102                       0.000000   \n",
       "72095                               0.115942                       0.181818   \n",
       "72096                               0.206723                       0.107143   \n",
       "\n",
       "       city_card_a_cnt_label_kfold_mean  \n",
       "47782                          0.231746  \n",
       "47783                          0.311475  \n",
       "47784                          0.171875  \n",
       "47785                          0.202532  \n",
       "47786                          0.176471  \n",
       "...                                 ...  \n",
       "72092                          0.157895  \n",
       "72093                          0.083333  \n",
       "72094                          0.294118  \n",
       "72095                          0.000000  \n",
       "72096                          0.200765  \n",
       "\n",
       "[24315 rows x 888 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py37]",
   "language": "python",
   "name": "conda-env-py37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
