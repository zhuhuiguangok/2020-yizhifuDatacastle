{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "Final-V3-{0.701}.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "iI3eXzjHoJWm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "70fb5f10-4278-446d-c11a-f53af2c529fc"
      },
      "source": [
        "# coding: utf-8\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from tqdm import tqdm\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import f1_score,roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import gc\n",
        "from gensim.models import Word2Vec, FastText\n",
        "import os\n",
        "import itertools\n",
        "import multiprocessing\n",
        "warnings.filterwarnings('ignore')\n",
        "print(multiprocessing.cpu_count())\n",
        "os.chdir(r\"/content/drive/My Drive/Colab Notebooks/Datacastle/code\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fos1FgiPtDv0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1769e34a-4f7c-4613-e5d4-df897d113bd0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i12ySnOnoJWr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_dataset(DATA_PATH):\n",
        "    train_label = pd.read_csv(DATA_PATH+'train_label.csv')\n",
        "    train_base = pd.read_csv(DATA_PATH+'train_base.csv')\n",
        "    test_base = pd.read_csv(DATA_PATH+'test_a_base.csv')\n",
        "\n",
        "    train_op = pd.read_csv(DATA_PATH+'train_op.csv')\n",
        "    train_trans = pd.read_csv(DATA_PATH+'train_trans.csv')\n",
        "    test_op = pd.read_csv(DATA_PATH+'test_a_op.csv')\n",
        "    test_trans = pd.read_csv(DATA_PATH+'test_a_trans.csv')\n",
        "\n",
        "    return train_label, train_base, test_base, train_op, train_trans, test_op, test_trans\n",
        "\n",
        "\n",
        "def transform_time(x):\n",
        "    day = int(x.split(' ')[0])\n",
        "    hour = int(x.split(' ')[2].split('.')[0].split(':')[0])\n",
        "    minute = int(x.split(' ')[2].split('.')[0].split(':')[1])\n",
        "    second = int(x.split(' ')[2].split('.')[0].split(':')[2])\n",
        "    return 86400*day+3600*hour+60*minute+second\n",
        "\n",
        "\n",
        "def data_preprocess(DATA_PATH):\n",
        "    train_label, train_base, test_base, train_op, train_trans, test_op, test_trans = load_dataset(DATA_PATH=DATA_PATH)\n",
        "    # 拼接数据\n",
        "    train_df = train_base.copy()\n",
        "    test_df = test_base.copy()\n",
        "    train_df = train_label.merge(train_df, on=['user'], how='left')\n",
        "    del train_base, test_base\n",
        "\n",
        "    op_df = pd.concat([train_op, test_op], axis=0, ignore_index=True)\n",
        "    trans_df = pd.concat([train_trans, test_trans], axis=0, ignore_index=True)\n",
        "    data = pd.concat([train_df, test_df], axis=0, ignore_index=True)\n",
        "    del train_op, test_op, train_df, test_df\n",
        "    # 时间维度的处理\n",
        "    op_df['days_diff'] = op_df['tm_diff'].apply(lambda x: int(x.split(' ')[0]))\n",
        "    trans_df['days_diff'] = trans_df['tm_diff'].apply(lambda x: int(x.split(' ')[0]))\n",
        "    op_df['timestamp'] = op_df['tm_diff'].apply(lambda x: transform_time(x))\n",
        "    trans_df['timestamp'] = trans_df['tm_diff'].apply(lambda x: transform_time(x))\n",
        "    op_df['hour'] = op_df['tm_diff'].apply(lambda x: int(x.split(' ')[2].split('.')[0].split(':')[0]))\n",
        "    trans_df['hour'] = trans_df['tm_diff'].apply(lambda x: int(x.split(' ')[2].split('.')[0].split(':')[0]))\n",
        "    trans_df['week'] = trans_df['days_diff'].apply(lambda x: x % 7)\n",
        "    # 排序\n",
        "    trans_df = trans_df.sort_values(by=['user', 'timestamp'])\n",
        "    op_df = op_df.sort_values(by=['user', 'timestamp'])\n",
        "    trans_df.reset_index(inplace=True, drop=True)\n",
        "    op_df.reset_index(inplace=True, drop=True)\n",
        "\n",
        "    gc.collect()\n",
        "    return data, op_df, trans_df\n",
        "\n",
        "#用户其他特征的统计量\n",
        "def gen_user_status_features(df,value):\n",
        "    group_df = df.groupby(['user'])[value].agg(\n",
        "        a='mean',\n",
        "        b= 'std',\n",
        "        c='max',\n",
        "        d='min',\n",
        "        e='sum',\n",
        "        f='median',\n",
        "        g='count',\n",
        "        h='median',\n",
        "        i='skew',\n",
        "        ).reset_index()\n",
        "    group_df['j']=group_df['b'] / group_df['a']\n",
        "    group_df.rename(columns={\n",
        "                    'a':'user_{}_mea'.format(value),\n",
        "                    'b':'user_{}_std'.format(value),\n",
        "                    'c':'user_{}_max'.format(value),\n",
        "                    'd':'user_{}_min'.format(value),\n",
        "                    'e':'user_{}_sum'.format(value),\n",
        "                    'f':'user_{}_med'.format(value),\n",
        "                    'g':'user_{}_cnt'.format(value),\n",
        "                    'h':'user_{}_median'.format(value),\n",
        "                    'i':'user_{}_skew'.format(value),\n",
        "                    'j':'user_{}_CV'.format(value)\n",
        "                    },\n",
        "                   inplace=True\n",
        "                   )\n",
        "    return group_df\n",
        "#用户所在城市的其他衍生特征\n",
        "def gen_city_status_features(df,value):\n",
        "    group_df = df.groupby(['city'])[value].agg(\n",
        "        a='mean',\n",
        "        b= 'std',\n",
        "        c='max',\n",
        "        d='min',\n",
        "        e='sum',\n",
        "        f='median',\n",
        "        g='count',\n",
        "        h='median',\n",
        "        i='skew',\n",
        "        ).reset_index()\n",
        "    group_df['j']=group_df['b'] / group_df['a']\n",
        "    group_df.rename(columns={\n",
        "                    'a':'city_{}_mea'.format(value),\n",
        "                    'b':'city_{}_std'.format(value),\n",
        "                    'c':'city_{}_max'.format(value),\n",
        "                    'd':'city_{}_min'.format(value),\n",
        "                    'e':'city_{}_sum'.format(value),\n",
        "                    'f':'city_{}_med'.format(value),\n",
        "                    'g':'city_{}_cnt'.format(value),\n",
        "                    'h':'city_{}_median'.format(value),\n",
        "                    'i':'city_{}_skew'.format(value),\n",
        "                    'j':'city_{}_CV'.format(value)\n",
        "                    },\n",
        "                   inplace=True\n",
        "                   )\n",
        "    return group_df\n",
        "\n",
        "\n",
        "#trans#用户交易消费的特征\n",
        "def gen_user_amount_group_features(df,col,val):\n",
        "    df = df[ df[col]==val ]\n",
        "    l = ['mean','std','max','min','sum','median','count','skew','var']\n",
        "    group_df = df.groupby('user')['amount'].agg(l).reset_index()\n",
        "    group_df[ 'user_'+col+'_'+val+'_amount_jicha'] = group_df['max']-group_df['min']\n",
        "    rename_col = {}\n",
        "    for i in l:\n",
        "        rename_col[i] = 'user_'+col+'_'+val+'_amount_'+i\n",
        "    group_df = group_df.rename(columns=rename_col)\n",
        "    return group_df\n",
        "\n",
        "#trans#用户交易消费的特征\n",
        "def gen_user_amount_features(df):\n",
        "    group_df = df.groupby(['user']).agg(\n",
        "         user_amount_mean=('amount','mean'),\n",
        "         user_amount_std=('amount','std'),\n",
        "         user_amount_max=('amount','max'),\n",
        "         user_amount_min=('amount','min'),\n",
        "         user_amount_sum=('amount','sum'),\n",
        "         user_amount_med=('amount','median'),\n",
        "         user_amount_cnt=('amount','count'),\n",
        "         user_amount_skew=('amount','skew'),\n",
        "         user_amount_var=('amount','var'),\n",
        "        ).reset_index()\n",
        "    group_df['user_amount_jicha'] = group_df['user_amount_max'] - group_df['user_amount_min']\n",
        "    return group_df\n",
        "\n",
        "def gen_user_days_diff_group_features(df,col,val):\n",
        "    df = df[ df[col]==val ]\n",
        "    l = ['mean','std','max','min','sum','median','count','skew','var']\n",
        "    group_df = df.groupby('user')['days_diff'].agg(l).reset_index()\n",
        "    group_df[ 'user_'+col+'_'+val+'_days_diff_jicha'] = group_df['max']-group_df['min']\n",
        "    rename_col = {}\n",
        "    for i in l:\n",
        "        rename_col[i] = 'user_'+col+'_'+val+'_days_diff_'+i\n",
        "    group_df = group_df.rename(columns=rename_col)\n",
        "    return group_df\n",
        "\n",
        "#trans 用户在某平台或者用某ip的消费额特征\n",
        "def gen_user_group_amount_features(df, value):\n",
        "    group_df = df.pivot_table(index='user',\n",
        "                              columns=value,\n",
        "                              values='amount',\n",
        "                              dropna=False,\n",
        "                              aggfunc=['count','sum','median',\"skew\",'var','mean','std','max','min']).fillna(0)\n",
        "    group_df.columns = ['user_{}_{}_amount_{}'.format(value, f[1], f[0]) for f in group_df.columns]\n",
        "    group_df.reset_index(inplace=True)\n",
        "\n",
        "    return group_df\n",
        "\n",
        "\n",
        "#用户在某个时间段内的消费额特征 大于天数的窗口\n",
        "def gen_user_window_amount_features(df, window):\n",
        "    group_df = df[df['days_diff']>window].groupby('user').agg(\n",
        "        mean=('amount','mean'),\n",
        "        std=('amount','std'),\n",
        "        max=('amount','max'),\n",
        "        min=('amount','min'),\n",
        "        median=('amount','median'),\n",
        "        count=('amount','count'),\n",
        "        skew=('amount','skew'),\n",
        "        var=('amount','var'),\n",
        "        ).reset_index().rename(columns={\"mean\":'user_amount_mean_{}d'.format(window),\n",
        "                        \"std\":'user_amount_std_{}d'.format(window),\n",
        "                        \"max\":'user_amount_max_{}d'.format(window),\n",
        "                        \"min\":'user_amount_min_{}d'.format(window),\n",
        "                        \"sum\":'user_amount_sum_{}d'.format(window),\n",
        "                        \"median\":'user_amount_median_{}d'.format(window),\n",
        "                        \"count\":'user_amount_count_{}d'.format(window),\n",
        "                        \"skew\":'user_amount_skew_{}d'.format(window),\n",
        "                        \"var\":'user_amount_var_{}d'.format(window),\n",
        "                        })\n",
        "    return group_df\n",
        "\n",
        "\n",
        "#用户在某个时间段内的消费额特征 大于小时的窗口\n",
        "def gen_user_hourwindow_amount_features(df, window):\n",
        "    group_df = df[df['hour']>=window].groupby('user').agg(\n",
        "        mean=('amount','mean'),\n",
        "        count=('amount','count'),\n",
        "        ).reset_index().rename(columns={\"mean\":'user_amount_mean_{}h'.format(window),\n",
        "                        \"count\":'user_amount_count_{}h'.format(window),\n",
        "                        })\n",
        "    return group_df\n",
        "\n",
        "\n",
        "#trans用户  ['days_diff', 'platform', 'tunnel_in', 'tunnel_out', 'type1', 'type2', 'ip', 'ip_3']交易用到各字段的类数\n",
        "def gen_user_nunique_features(df, value, prefix):\n",
        "    group_df = df.groupby(['user'])[value].agg(\n",
        "        ['nunique','count']\n",
        "    ).reset_index().rename(columns={\"nunique\":'user_{}_{}_nuniq'.format(prefix, value),\n",
        "                                   'count':'user_{}_{}_cnt'.format(prefix, value)})\n",
        "    return group_df\n",
        "\n",
        "\n",
        "#trans用户无IP 这个特征贼奇怪 怎么会交易没有ip呢\n",
        "def gen_user_null_features(df, value, prefix):\n",
        "    df['is_null'] = 0\n",
        "    df.loc[df[value].isnull(), 'is_null'] = 1\n",
        "\n",
        "    group_df = df.groupby(['user'])['is_null'].agg(sum='sum',\n",
        "                            mean='mean').reset_index().rename(columns={\"sum\":'user_{}_{}_null_cnt'.format(prefix, value),\n",
        "                                                        \"mean\":'user_{}_{}_null_ratio'.format(prefix, value)})\n",
        "    return group_df\n",
        "\n",
        "\n",
        "# op op_mode op_tyep op_device net_type channel\n",
        "def gen_user_tfidf_features(df, value):\n",
        "    #填充缺失值\n",
        "    df[value].replace(' ', np.nan, inplace=True)\n",
        "    df[value] = df[value].astype(str)\n",
        "    df[value].fillna('-1', inplace=True)\n",
        "\n",
        "    #\n",
        "    group_df = df.groupby(['user']).apply(lambda x: x[value].tolist()).reset_index()\n",
        "    group_df.columns = ['user', 'list']\n",
        "    group_df['list'] = group_df['list'].apply(lambda x: ','.join(x))\n",
        "    enc_vec = TfidfVectorizer()\n",
        "    tfidf_vec = enc_vec.fit_transform(group_df['list'])\n",
        "    svd_enc = TruncatedSVD(n_components=10, n_iter=20, random_state=2020)\n",
        "    vec_svd = svd_enc.fit_transform(tfidf_vec)\n",
        "    vec_svd = pd.DataFrame(vec_svd)\n",
        "    vec_svd.columns = ['svd_tfidf_{}_{}'.format(value, i) for i in range(10)]\n",
        "    group_df = pd.concat([group_df, vec_svd], axis=1)\n",
        "    del group_df['list']\n",
        "    return group_df\n",
        "\n",
        "\n",
        "# op op_mode op_tyep op_device net_type channel\n",
        "def gen_user_countvec_features(df, value):\n",
        "    #填充缺失值\n",
        "    df[value].replace(' ', np.nan, inplace=True)\n",
        "    df[value] = df[value].astype(str)\n",
        "    df[value].fillna('-1', inplace=True)\n",
        "    #\n",
        "    group_df = df.groupby(['user']).apply(lambda x: x[value].tolist()).reset_index()\n",
        "    group_df.columns = ['user', 'list']\n",
        "    group_df['list'] = group_df['list'].apply(lambda x: ','.join(x))\n",
        "    enc_vec = CountVectorizer()\n",
        "    tfidf_vec = enc_vec.fit_transform(group_df['list'])\n",
        "    svd_enc = TruncatedSVD(n_components=10, n_iter=20, random_state=2020)\n",
        "    vec_svd = svd_enc.fit_transform(tfidf_vec)\n",
        "    vec_svd = pd.DataFrame(vec_svd)\n",
        "    vec_svd.columns = ['svd_countvec_{}_{}'.format(value, i) for i in range(10)]\n",
        "    group_df = pd.concat([group_df, vec_svd], axis=1)\n",
        "    del group_df['list']\n",
        "    return group_df\n",
        "\n",
        "\n",
        "#target_encode_cols = ['province', 'city', 'city_level', 'city_balance_avg']\n",
        "#target_encode\n",
        "def kfold_stats_feature(train, test, feats, k):\n",
        "    folds = StratifiedKFold(n_splits=k, shuffle=True, random_state=2020)  # 这里最好和后面模型的K折交叉验证保持一致\n",
        "\n",
        "    train['fold'] = None\n",
        "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(train, train['label'])):\n",
        "        train.loc[val_idx, 'fold'] = fold_\n",
        "\n",
        "    kfold_features = []\n",
        "    for feat in feats:\n",
        "        nums_columns = ['label']\n",
        "        #只有一个还用for?\n",
        "        for f in nums_columns:\n",
        "            colname = feat + '_' + f + '_kfold_mean'\n",
        "            kfold_features.append(colname)\n",
        "            train[colname] = None\n",
        "            for fold_, (trn_idx, val_idx) in enumerate(folds.split(train, train['label'])):\n",
        "                tmp_trn = train.iloc[trn_idx]\n",
        "                order_label = tmp_trn.groupby([feat])[f].mean()\n",
        "\n",
        "                tmp = train.loc[train.fold == fold_, [feat]]\n",
        "                train.loc[train.fold == fold_, colname] = tmp[feat].map(order_label)#其实就是用kfold方式对该特征的label标签mean化，然后建个新列\n",
        "                \n",
        "                # fillna 平均值填充\n",
        "                global_mean = train[f].mean()\n",
        "                train.loc[train.fold == fold_, colname] = train.loc[train.fold == fold_, colname].fillna(global_mean)\n",
        "            train[colname] = train[colname].astype(float)\n",
        "\n",
        "        for f in nums_columns:\n",
        "            colname = feat + '_' + f + '_kfold_mean'\n",
        "            test[colname] = None\n",
        "            order_label = train.groupby([feat])[f].mean()\n",
        "            test[colname] = test[feat].map(order_label)\n",
        "            # fillna\n",
        "            global_mean = train[f].mean()\n",
        "            test[colname] = test[colname].fillna(global_mean)\n",
        "            test[colname] = test[colname].astype(float)\n",
        "    del train['fold']\n",
        "    return train, test\n",
        "\n",
        "\n",
        "def w2v_feat(df, feat, mode):\n",
        "    data_frame=df.copy()\n",
        "    # 转化为str\n",
        "    for i in feat:\n",
        "        if data_frame[i].dtype != 'object':\n",
        "            data_frame[i] = data_frame[i].astype(str)\n",
        "    data_frame.fillna('nan', inplace=True) # 甜宠\n",
        "\n",
        "    print(f'Start {mode} word2vec ...')\n",
        "    model = Word2Vec(data_frame[feat].values.tolist(), size=10, window=2, min_count=1,\n",
        "                     workers=multiprocessing.cpu_count(), iter=10)\n",
        "    stat_list = ['min', 'max', 'mean', 'std']\n",
        "    new_all = pd.DataFrame()\n",
        "    for m, t in enumerate(feat):\n",
        "        print(f'Start gen feat of {t} ...')\n",
        "        tmp = []\n",
        "        for i in data_frame[t].unique():\n",
        "            tmp_v = [i]\n",
        "            tmp_v.extend(model[i])\n",
        "            tmp.append(tmp_v)\n",
        "        tmp_df = pd.DataFrame(tmp)\n",
        "        w2c_list = [f'w2c_{t}_{n}' for n in range(10)]\n",
        "        tmp_df.columns = [t] + w2c_list\n",
        "        tmp_df = data_frame[['user', t]].merge(tmp_df, on=t)\n",
        "        tmp_df = tmp_df.drop_duplicates().groupby('user').agg(stat_list).reset_index()\n",
        "        tmp_df.columns = ['user'] + [f'{p}_{q}' for p in w2c_list for q in stat_list]\n",
        "        if m == 0:\n",
        "            new_all = pd.concat([new_all, tmp_df], axis=1)\n",
        "        else:\n",
        "            new_all = pd.merge(new_all, tmp_df, how='left', on='user')\n",
        "    return new_all\n",
        "\n",
        "\n",
        "def fasttext(df, feat, mode):\n",
        "    data_frame=df.copy()\n",
        "    # 转化为str\n",
        "    for i in feat:\n",
        "        if data_frame[i].dtype != 'object':\n",
        "            data_frame[i] = data_frame[i].astype(str)\n",
        "    data_frame.fillna('nan', inplace=True) # 甜宠\n",
        "\n",
        "    print(f'Start {mode} FastText ...')\n",
        "    model = FastText(data_frame[feat].values.tolist(),  size=10, window=3, min_count=1, \n",
        "                     workers=multiprocessing.cpu_count(), iter=10,min_n = 3 , max_n = 6,word_ngrams = 0,seed=1)\n",
        "    stat_list = ['min', 'max', 'mean', 'std']\n",
        "    new_all = pd.DataFrame()\n",
        "    for m, t in enumerate(feat):\n",
        "        print(f'Start gen feat of {t} ...')\n",
        "        tmp = []\n",
        "        for i in data_frame[t].unique():\n",
        "            tmp_v = [i]\n",
        "            tmp_v.extend(model[i])\n",
        "            tmp.append(tmp_v)\n",
        "        tmp_df = pd.DataFrame(tmp)\n",
        "        w2c_list = [f'fast_text_{t}_{n}' for n in range(10)]\n",
        "        tmp_df.columns = [t] + w2c_list\n",
        "        tmp_df = data_frame[['user', t]].merge(tmp_df, on=t)\n",
        "        tmp_df = tmp_df.drop_duplicates().groupby('user').agg(stat_list).reset_index()\n",
        "        tmp_df.columns = ['user'] + [f'{p}_{q}' for p in w2c_list for q in stat_list]\n",
        "        if m == 0:\n",
        "            new_all = pd.concat([new_all, tmp_df], axis=1)\n",
        "        else:\n",
        "            new_all = pd.merge(new_all, tmp_df, how='left', on='user')\n",
        "    return new_all\n",
        "\n",
        "def gen_features(df, op, trans):\n",
        "\n",
        "    # base\n",
        "    print(\"base\")\n",
        "    df['product7_fail_ratio'] = df['product7_fail_cnt'] / df['product7_cnt']\n",
        "    df['city_count'] = df.groupby(['city'])['user'].transform('count')\n",
        "    df['province_count'] = df.groupby(['province'])['user'].transform('count')\n",
        "    df['op_cnt_avg'] = (df[\"op1_cnt\"]+df[\"op2_cnt\"])/df[\"using_time\"]\n",
        "    df['ip_cnt_avg'] = df[\"ip_cnt\"]/df[\"using_time\"]\n",
        "    df['card_a_cnt_avg'] = df[\"card_a_cnt\"]/df[\"using_time\"]\n",
        "    df['card_b_cnt_avg'] = df[\"card_b_cnt\"]/df[\"using_time\"]\n",
        "    df['card_c_cnt_avg'] = df[\"card_c_cnt\"]/df[\"using_time\"]\n",
        "    df['card_d_cnt_avg'] = df[\"card_d_cnt\"]/df[\"using_time\"]\n",
        "    df['login_cnt_didive'] = df[\"login_cnt_period1\"]/df[\"login_cnt_period2\"]\n",
        "    df['ip_per_day_cnt'] = df[\"ip_cnt\"]/df[\"login_days_cnt\"]\n",
        "    df['acc_count_per_time'] = df[\"acc_count\"]/df[\"using_time\"]\n",
        "    df['agreement_total_per_using_time'] = df[\"agreement_total\"]/df[\"using_time\"]\n",
        "    df['login_cnt'] = df[\"using_time\"]*df[\"login_cnt_avg\"]\n",
        "    df['agreement_total_per_account_cnt'] = df[\"agreement_total\"]/df[\"acc_count\"]\n",
        "    df['ip_cnt_per_acc_count'] = df[\"ip_cnt\"]/df[\"acc_count\"]\n",
        "    df['service1_amt_per_using_time'] = df[\"service1_amt\"]/df[\"using_time\"]\n",
        "    df[\"service_cnt_per_time\"]=(df[\"service1_cnt\"]+df[\"service1_cnt\"])/df[\"using_time\"]\n",
        "    \n",
        "    df['login_cnt_period_var']=df[['login_cnt_period1','login_cnt_period2']].std(axis=1)\n",
        "    df['login_cnt_period_CV']=df['login_cnt_period_var'] / df[['login_cnt_period1','login_cnt_period2']].mean(axis=1)\n",
        "    df['card_var']=df[['card_a_cnt','card_b_cnt','card_c_cnt','card_d_cnt']].std(axis=1)\n",
        "    df['card_CV']=df['card_var'] / df[['card_a_cnt','card_b_cnt','card_c_cnt','card_d_cnt']].mean(axis=1)\n",
        "    df['op_cnt_var']=df[['op1_cnt','op2_cnt']].std(axis=1)\n",
        "    df['op_cnt_CV']=df['op_cnt_var'] / df[['op1_cnt','op2_cnt']].mean(axis=1)\n",
        "    df['login_var']=df[['login_cnt_period1','login_cnt_period2','login_cnt_avg']].std(axis=1)\n",
        "    df['login_CV']=df['login_var'] / df[['login_cnt_period1','login_cnt_period2','login_cnt_avg']].mean(axis=1)\n",
        "    df['ip_cnt_per_login_cnt_avg'] = df['ip_cnt']/df[['login_cnt_period1','login_cnt_period2','login_cnt_avg']].mean(axis=1)\n",
        "    df['op_cnt_per_login_cnt_avg'] = (df[\"op1_cnt\"]+df[\"op2_cnt\"])/df['login_cnt_avg']\n",
        "\n",
        "    # trans\n",
        "    df = df.merge(gen_user_amount_features(trans), on=['user'], how='left')\n",
        "    for col in tqdm(['days_diff','platform', 'tunnel_in', 'tunnel_out', 'type1', 'type2', 'ip', 'ip_3']):\n",
        "        df = df.merge(gen_user_nunique_features(df=trans, value=col, prefix='trans'), on=['user'], how='left')\n",
        "    df['user_amount_per_days'] = df['user_amount_sum'] / df['user_trans_days_diff_nuniq']\n",
        "    df['user_amount_per_cnt'] = df['user_amount_sum'] / df['user_amount_cnt']\n",
        "    # df['user_amount_per_tunnel_in'] = df['user_amount_sum'] / df['user_trans_tunnel_in_nuniq']\n",
        "    # df['user_amount_per_cnt'] = df['user_amount_sum'] / df['user_trans_tunnel_out_nuniq']\n",
        "    # df['user_amount_per_platform'] = df['user_amount_sum'] / df['user_trans_platform_nuniq']\n",
        "\n",
        "    temp=trans[trans['ip'].isnull()].groupby(['user'])['amount'].agg(user_null_ip_amount_sum='sum').reset_index()\n",
        "    df=df.merge(temp,on=['user'],how='left')\n",
        "    df['user_null_ip_amount_sum'].fillna(0,inplace=True)\n",
        "    df['user_null_ip_amount_percent'] = df['user_null_ip_amount_sum'] / df['user_amount_sum']\n",
        "    df['user_ip_amount_percent'] = 1-df['user_null_ip_amount_percent'] \n",
        "\n",
        "    # df = df.merge(gen_user_status_features(trans,'amount'), on=['user'], how='left')\n",
        "    # df = df.merge(gen_user_status_features(trans,'days_diff'), on=['user'], how='left')\n",
        "    # df = df.merge(gen_user_status_features(trans,'hour'), on=['user'], how='left')\n",
        "    # df=df.merge(df.groupby(['city'])['user_amount_sum'].agg(city_amount_mean='mean',city_amount_sum='sum',city_amount_std='std').reset_index(),\n",
        "    #          on=['city'],how='left')\n",
        "  \n",
        "    # print(\"trans nunique && null features\")\n",
        "    # for col in tqdm(['days_diff', 'platform', 'tunnel_in', 'tunnel_out', 'type1', 'type2','ip', 'ip_3']):\n",
        "    #     df = df.merge(gen_user_nunique_features(df=trans, value=col, prefix='trans'), on=['user'], how='left')\n",
        "    #     df = df.merge(gen_user_null_features(df=trans, value=col, prefix='trans'), on=['user'], how='left')\n",
        "    # df['user_amount_per_days'] = df['user_amount_sum'].div(df['user_trans_days_diff_nuniq']) \n",
        "    # df['user_amount_per_cnt'] = df['user_amount_sum'] / df['user_amount_cnt']\n",
        "    # df['user_amount_per_tunnel_in'] = df['user_amount_sum'] / df['user_trans_tunnel_in_nuniq']\n",
        "    # df['user_amount_per_cnt'] = df['user_amount_sum'] / df['user_trans_tunnel_out_nuniq']\n",
        "    # df['user_amount_per_platform'] = df['user_amount_sum'] / df['user_trans_platform_nuniq']\n",
        "    # df=df.merge(gen_city_status_features(df,'user_trans_ip_null_cnt'),on=['city'],how='left')\n",
        "    # df=df.merge(gen_city_status_features(df,'user_trans_ip_count'),on=['city'],how='left')\n",
        "    # df['city_ipcnt_vs_nullipcnt']=df['city_user_trans_ip_count_sum'] / df['city_user_trans_ip_null_cnt_sum']\n",
        "    \n",
        "    \n",
        "    # df=df.merge(gen_city_status_features(df,'user_null_ip_amount_sum'),on=['city'],how='left')\n",
        "    # df['city_null_ip_amount_percent'] = df['city_user_null_ip_amount_sum_sum'] / df['city_amount_sum']\n",
        "    \n",
        "\n",
        "    print(\"trans group_amount_features\") \n",
        "    # df = df.merge(gen_user_group_amount_features(df=trans, value='platform'), on=['user'], how='left')\n",
        "    # df = df.merge(gen_user_group_amount_features(df=trans, value='type1'), on=['user'], how='left')\n",
        "    # df = df.merge(gen_user_group_amount_features(df=trans, value='type2'), on=['user'], how='left')\n",
        "    # df = df.merge(gen_user_group_amount_features(df=trans, value='tunnel_in'), on=['user'], how='left')\n",
        "    # df = df.merge(gen_user_group_amount_features(df=trans, value='tunnel_out'), on=['user'], how='left')\n",
        "    print(\"trans window\") \n",
        "    df = df.merge(gen_user_window_amount_features(df=trans, window=27), on=['user'], how='left')\n",
        "    df = df.merge(gen_user_window_amount_features(df=trans, window=23), on=['user'], how='left')\n",
        "    df = df.merge(gen_user_window_amount_features(df=trans, window=20), on=['user'], how='left')\n",
        "    df = df.merge(gen_user_window_amount_features(df=trans, window=15), on=['user'], how='left')\n",
        "    df = df.merge(gen_user_hourwindow_amount_features(df=trans, window=20), on=['user'], how='left')\n",
        "    df = df.merge(gen_user_hourwindow_amount_features(df=trans, window=15), on=['user'], how='left')\n",
        "    df = df.merge(gen_user_hourwindow_amount_features(df=trans, window=10), on=['user'], how='left')\n",
        "    print(\"trans null_features\")\n",
        "    df = df.merge(gen_user_null_features(df=trans, value='type2', prefix='trans'), on=['user'], how='left')\n",
        "    df = df.merge(gen_user_null_features(df=trans, value='ip_3', prefix='trans'), on=['user'], how='left')\n",
        "    df = df.merge(gen_user_null_features(df=trans, value='tunnel_in', prefix='trans'), on=['user'], how='left')\n",
        "    df = df.merge(gen_user_null_features(df=trans, value='tunnel_out', prefix='trans'), on=['user'], how='left')\n",
        "\n",
        "    print(\"type1\")\n",
        "    for i in [\"674e8d5860bc033d\",'443b0fd0860c21b6','45a1168437c708ff','f67d4b5a05a1352a','443b0fd0860c21b6',\n",
        "              '3146295fbf43c0cb','8adb3dcfea9dcf5e','33e9d4cef01499e1','19d44f1a51919482','0a3cf8dac7dca9d1']:\n",
        "        df = df.merge(gen_user_days_diff_group_features(trans,'type1',i), on=['user'], how='left')\n",
        "        df = df.merge(gen_user_amount_group_features(trans,'type1',i), on=['user'], how='left')\n",
        "    \n",
        "    print(\"type2\")\n",
        "    for i in ['11a213398ee0c623','2ee592ab06090eb5','b5a8be737a50b171','2ee592ab06090eb5']:\n",
        "        df = df.merge(gen_user_days_diff_group_features(trans,'type1',i), on=['user'], how='left')\n",
        "        df = df.merge(gen_user_amount_group_features(trans,'type1',i), on=['user'], how='left')\n",
        "        \n",
        "    print(\"tunnel_in\")\n",
        "    for i in ['b2e7fa260df4998d']:\n",
        "        df = df.merge(gen_user_days_diff_group_features(trans,'type1',i), on=['user'], how='left')\n",
        "        df = df.merge(gen_user_amount_group_features(trans,'type1',i), on=['user'], how='left')\n",
        "        \n",
        "    print(\"tunnel_out\")\n",
        "    for i in ['6ee790756007e69a','4c8524fb01d8b204']:\n",
        "        df = df.merge(gen_user_days_diff_group_features(trans,'type1',i), on=['user'], how='left')\n",
        "        df = df.merge(gen_user_amount_group_features(trans,'type1',i), on=['user'], how='left')\n",
        "    \n",
        "    df = df.merge(gen_user_tfidf_features(df=trans, value='amount'), on=['user'], how='left')\n",
        "    df = df.merge(gen_user_countvec_features(df=trans, value='amount'), on=['user'], how='left')\n",
        "    \n",
        "    df=df.merge(fasttext(trans,['amount','type1',\"type2\",],\"train\"),on=['user'],how='left')\n",
        "\n",
        "    print(\"op\")\n",
        "    # op\n",
        "    print(\"op null features\")\n",
        "    df = df.merge(gen_user_null_features(df=op, value=\"net_type\", prefix='op'), on=['user'], how='left')\n",
        "    df = df.merge(gen_user_null_features(df=op, value=\"op_device\", prefix='op'), on=['user'], how='left')\n",
        "    df = df.merge(gen_user_null_features(df=op, value=\"ip\", prefix='op'), on=['user'], how='left') \n",
        "\n",
        "    df=df.merge(op.groupby(\"user\").agg(user_op_days_diff_mean=('days_diff','mean')),on=['user'], how='left')\n",
        "    df=df.merge(op.groupby(\"user\").agg(user_op_hour_mean=('hour','mean')),on=['user'], how='left')\n",
        "\n",
        "    print(\"op nunique features\")\n",
        "    for col in tqdm(['days_diff','hour', 'op_mode', 'op_type', 'op_device',\"ip\",'channel',\"ip_3\"]):\n",
        "        df = df.merge(gen_user_nunique_features(df=op, value=col, prefix='op'), on=['user'], how='left')\n",
        "\n",
        "    print(\"op w2v tfidf countvec encoder features\")\n",
        "    df = df.merge(gen_user_tfidf_features(df=op, value='op_mode'), on=['user'], how='left')\n",
        "    df = df.merge(gen_user_tfidf_features(df=op, value='op_type'), on=['user'], how='left')\n",
        "\n",
        "    df = df.merge(gen_user_countvec_features(df=op, value='op_mode'), on=['user'], how='left')\n",
        "    df = df.merge(gen_user_countvec_features(df=op, value='op_type'), on=['user'], how='left')\n",
        "\n",
        "    df=df.merge(fasttext(op,['op_mode','op_type',\"op_device\",\"channel\",'ip_3',\"net_type\",\"ip\"],\"train\"),on=['user'],how='left')\n",
        "    \n",
        "    for col in tqdm([f for f in df.select_dtypes('object').columns if f not in ['user']]):\n",
        "        le = LabelEncoder()\n",
        "        df[col] = df[col].apply(lambda x:str(x))\n",
        "        df[col].fillna('-1', inplace=True)\n",
        "        df[col] = le.fit_transform(df[col])\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def lgb_model(train, target, test, k):\n",
        "    feats = [f for f in train.columns if f not in ['user', 'label']]\n",
        "    print('Current num of features:', len(feats))\n",
        "    folds = StratifiedKFold(n_splits=k, shuffle=True, random_state=2020)\n",
        "    oof_probs = np.zeros(train.shape[0])\n",
        "    output_preds = 0\n",
        "    offline_score = []\n",
        "    feature_importance_df = pd.DataFrame()\n",
        "    parameters = {\n",
        "        'learning_rate': 0.03,\n",
        "        'boosting_type': 'gbdt',\n",
        "        'objective': 'binary',\n",
        "        'metric': 'auc',\n",
        "        'num_leaves': 50,\n",
        "        'feature_fraction': 0.8,\n",
        "        'bagging_fraction': 0.8,\n",
        "        'min_data_in_leaf': 20,\n",
        "        'reg_alpha':10,\n",
        "        'reg_lambda':8,\n",
        "        'verbose': -1,\n",
        "        'nthread': 8,\n",
        "        'colsample_bytree':0.77,\n",
        "        'min_child_weight':4,\n",
        "        'min_child_samples':10,\n",
        "        'min_split_gain':0,\n",
        "        'lambda_l1': 0.8,\n",
        "    }\n",
        "\n",
        "    for i, (train_index, test_index) in enumerate(folds.split(train, target)):\n",
        "        train_y, test_y = target[train_index], target[test_index]\n",
        "        train_X, test_X = train[feats].iloc[train_index, :], train[feats].iloc[test_index, :]\n",
        "\n",
        "        dtrain = lgb.Dataset(train_X,\n",
        "                             label=train_y)\n",
        "        dval = lgb.Dataset(test_X,\n",
        "                           label=test_y)\n",
        "        lgb_model = lgb.train(\n",
        "                parameters,\n",
        "                dtrain,\n",
        "                num_boost_round=5000,\n",
        "                valid_sets=[dval],\n",
        "                early_stopping_rounds=100,\n",
        "                verbose_eval=100,\n",
        "        )\n",
        "        oof_probs[test_index] = lgb_model.predict(test_X[feats], num_iteration=lgb_model.best_iteration)\n",
        "        offline_score.append(lgb_model.best_score['valid_0']['auc'])\n",
        "        output_preds += lgb_model.predict(test[feats], num_iteration=lgb_model.best_iteration)/folds.n_splits\n",
        "        print(offline_score)\n",
        "        # feature importance\n",
        "        fold_importance_df = pd.DataFrame()\n",
        "        print(fold_importance_df.shape)\n",
        "        fold_importance_df[\"feature\"] = feats\n",
        "        fold_importance_df[\"importance\"] = lgb_model.feature_importance(importance_type='gain')\n",
        "        fold_importance_df[\"fold\"] = i + 1\n",
        "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
        "    print('OOF-MEAN-AUC:%.6f, OOF-STD-AUC:%.6f' % (np.mean(offline_score), np.std(offline_score)))\n",
        "    print('feature importance:')\n",
        "    print(feature_importance_df.groupby(['feature'])['importance'].mean().sort_values(ascending=False).head(50))\n",
        "    feature_importance_df.groupby(['feature'])['importance'].mean().to_csv(\"../submission/feature_importance.csv\")\n",
        "    return output_preds, oof_probs, np.mean(offline_score),feature_importance_df\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYhsd0tOoJWv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "outputId": "ecedbf4f-4c15-458e-e7a7-8408f96ad671"
      },
      "source": [
        "%%time\n",
        "DATA_PATH = '../data/'\n",
        "print('读取数据...')\n",
        "data, op_df, trans_df = data_preprocess(DATA_PATH=DATA_PATH)\n",
        "\n",
        "print('开始特征工程...')\n",
        "data = gen_features(data, op_df, trans_df)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "读取数据...\n",
            "开始特征工程...\n",
            "base\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 8/8 [00:03<00:00,  2.44it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "trans group_amount_features\n",
            "trans window\n",
            "trans null_features\n",
            "type1\n",
            "type2\n",
            "tunnel_in\n",
            "tunnel_out\n",
            "Start train FastText ...\n",
            "Start gen feat of amount ...\n",
            "Start gen feat of type1 ...\n",
            "Start gen feat of type2 ...\n",
            "op\n",
            "op null features\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/8 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "op nunique features\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 8/8 [00:14<00:00,  1.82s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "op w2v tfidf countvec encoder features\n",
            "Start train FastText ...\n",
            "Start gen feat of op_mode ...\n",
            "Start gen feat of op_type ...\n",
            "Start gen feat of op_device ...\n",
            "Start gen feat of channel ...\n",
            "Start gen feat of ip_3 ...\n",
            "Start gen feat of net_type ...\n",
            "Start gen feat of ip ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 25/25 [00:01<00:00, 23.38it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 45min 47s, sys: 21.7 s, total: 46min 9s\n",
            "Wall time: 28min 28s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9psMSh1MoJW6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6564cae4-23a4-49a7-ab2c-9efd52d74c3a"
      },
      "source": [
        "\n",
        "data['city_level'] = data['city'].map(str) + '_' + data['level'].map(str)\n",
        "data['city_product1_amount'] = data['city'].map(str) + '_' + data['product1_amount'].map(str)\n",
        "data['city_balance1_avg'] = data['city'].map(str) + '_' + data['balance1_avg'].map(str)\n",
        "data['city_balance2_avg'] = data['city'].map(str) + '_' + data['balance2_avg'].map(str)\n",
        "data['city_balance'] = data['city'].map(str) + '_' + data['balance'].map(str)\n",
        "data['city_card_a_cnt'] = data['city'].map(str) + '_' + data['card_a_cnt'].map(str)\n",
        "\n",
        "\n",
        "train = data[~data['label'].isnull()].copy()\n",
        "target = train['label']\n",
        "test = data[data['label'].isnull()].copy()\n",
        "\n",
        "print(\"target encoder...\")\n",
        "  \n",
        "target_encode_cols = ['province','city',\"city_level\",'city_balance1_avg',\n",
        "                      'city_balance2_avg',\"city_product1_amount\",'city_balance','city_card_a_cnt']\n",
        "\n",
        "train, test = kfold_stats_feature(train, test, target_encode_cols, 10)\n",
        "train.drop(['province', \"city\",\"service3_level\",\"city_level\",'city_balance1_avg','city_balance2_avg',\n",
        "              \"city_product1_amount\",'city_balance','city_card_a_cnt'], axis=1, inplace=True)\n",
        "test.drop(['province',\"city\",\"service3_level\",\"city_level\",'city_balance1_avg','city_balance2_avg',\n",
        "              \"city_product1_amount\",'city_balance','city_card_a_cnt'], axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "target encoder...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8iDmht-oJXB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "44f34d1a-a7fe-4c1e-a105-43e902c6984e"
      },
      "source": [
        "%%time\n",
        "print('开始模型训练...')\n",
        "\n",
        "lgb_preds, lgb_oof, lgb_score,feature_importance_df = lgb_model(train=train, target=target, test=test,k=10)\n",
        "auc_score = roc_auc_score(target.values, lgb_oof)\n",
        "print(\"train auc:\",auc_score)\n",
        "\n",
        "sub_df = test[['user']].copy()\n",
        "sub_df['prob'] = lgb_preds\n",
        "sub_df.to_csv('../submission/sub_lgb{%.5f}.csv'%(lgb_score,), index=False)\n",
        "# 0.741833 No_group_amount\n",
        "# 0.740743 no_cv 0.70***4\n",
        "# 0.740557 (no_day 23 27) 0.70053\n",
        "#0.740945 all  0.69\n",
        "#val_auc0.740481 on_auc0.700902"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "开始模型训练...\n",
            "Current num of features: 979\n",
            "Training until validation scores don't improve for 100 rounds.\n",
            "[100]\tvalid_0's auc: 0.744047\n",
            "[200]\tvalid_0's auc: 0.747383\n",
            "[300]\tvalid_0's auc: 0.746663\n",
            "Early stopping, best iteration is:\n",
            "[202]\tvalid_0's auc: 0.747457\n",
            "[0.7474572003909496]\n",
            "(0, 0)\n",
            "Training until validation scores don't improve for 100 rounds.\n",
            "[100]\tvalid_0's auc: 0.745948\n",
            "[200]\tvalid_0's auc: 0.750451\n",
            "[300]\tvalid_0's auc: 0.750375\n",
            "Early stopping, best iteration is:\n",
            "[215]\tvalid_0's auc: 0.750968\n",
            "[0.7474572003909496, 0.7509675910624714]\n",
            "(0, 0)\n",
            "Training until validation scores don't improve for 100 rounds.\n",
            "[100]\tvalid_0's auc: 0.734166\n",
            "[200]\tvalid_0's auc: 0.737768\n",
            "[300]\tvalid_0's auc: 0.739372\n",
            "[400]\tvalid_0's auc: 0.739839\n",
            "[500]\tvalid_0's auc: 0.739464\n",
            "Early stopping, best iteration is:\n",
            "[416]\tvalid_0's auc: 0.740202\n",
            "[0.7474572003909496, 0.7509675910624714, 0.7402016129032258]\n",
            "(0, 0)\n",
            "Training until validation scores don't improve for 100 rounds.\n",
            "[100]\tvalid_0's auc: 0.743048\n",
            "[200]\tvalid_0's auc: 0.746144\n",
            "[300]\tvalid_0's auc: 0.7468\n",
            "Early stopping, best iteration is:\n",
            "[251]\tvalid_0's auc: 0.747152\n",
            "[0.7474572003909496, 0.7509675910624714, 0.7402016129032258, 0.7471517166572345]\n",
            "(0, 0)\n",
            "Training until validation scores don't improve for 100 rounds.\n",
            "[100]\tvalid_0's auc: 0.727725\n",
            "[200]\tvalid_0's auc: 0.733417\n",
            "[300]\tvalid_0's auc: 0.734536\n",
            "[400]\tvalid_0's auc: 0.733938\n",
            "[500]\tvalid_0's auc: 0.735241\n",
            "[600]\tvalid_0's auc: 0.734237\n",
            "Early stopping, best iteration is:\n",
            "[508]\tvalid_0's auc: 0.735267\n",
            "[0.7474572003909496, 0.7509675910624714, 0.7402016129032258, 0.7471517166572345, 0.7352671665723448]\n",
            "(0, 0)\n",
            "Training until validation scores don't improve for 100 rounds.\n",
            "[100]\tvalid_0's auc: 0.728961\n",
            "[200]\tvalid_0's auc: 0.733052\n",
            "[300]\tvalid_0's auc: 0.734396\n",
            "[400]\tvalid_0's auc: 0.735345\n",
            "[500]\tvalid_0's auc: 0.736187\n",
            "[600]\tvalid_0's auc: 0.735745\n",
            "Early stopping, best iteration is:\n",
            "[505]\tvalid_0's auc: 0.736407\n",
            "[0.7474572003909496, 0.7509675910624714, 0.7402016129032258, 0.7471517166572345, 0.7352671665723448, 0.7364068100358423]\n",
            "(0, 0)\n",
            "Training until validation scores don't improve for 100 rounds.\n",
            "[100]\tvalid_0's auc: 0.729097\n",
            "[200]\tvalid_0's auc: 0.733816\n",
            "[300]\tvalid_0's auc: 0.733928\n",
            "Early stopping, best iteration is:\n",
            "[247]\tvalid_0's auc: 0.734286\n",
            "[0.7474572003909496, 0.7509675910624714, 0.7402016129032258, 0.7471517166572345, 0.7352671665723448, 0.7364068100358423, 0.7342859837766459]\n",
            "(0, 0)\n",
            "Training until validation scores don't improve for 100 rounds.\n",
            "[100]\tvalid_0's auc: 0.727879\n",
            "[200]\tvalid_0's auc: 0.734362\n",
            "[300]\tvalid_0's auc: 0.736589\n",
            "[400]\tvalid_0's auc: 0.738372\n",
            "[500]\tvalid_0's auc: 0.740178\n",
            "[600]\tvalid_0's auc: 0.740873\n",
            "[700]\tvalid_0's auc: 0.740193\n",
            "Early stopping, best iteration is:\n",
            "[652]\tvalid_0's auc: 0.741087\n",
            "[0.7474572003909496, 0.7509675910624714, 0.7402016129032258, 0.7471517166572345, 0.7352671665723448, 0.7364068100358423, 0.7342859837766459, 0.7410870590454631]\n",
            "(0, 0)\n",
            "Training until validation scores don't improve for 100 rounds.\n",
            "[100]\tvalid_0's auc: 0.719314\n",
            "[200]\tvalid_0's auc: 0.723729\n",
            "[300]\tvalid_0's auc: 0.723878\n",
            "Early stopping, best iteration is:\n",
            "[211]\tvalid_0's auc: 0.724087\n",
            "[0.7474572003909496, 0.7509675910624714, 0.7402016129032258, 0.7471517166572345, 0.7352671665723448, 0.7364068100358423, 0.7342859837766459, 0.7410870590454631, 0.7240872005282022]\n",
            "(0, 0)\n",
            "Training until validation scores don't improve for 100 rounds.\n",
            "[100]\tvalid_0's auc: 0.746089\n",
            "[200]\tvalid_0's auc: 0.74783\n",
            "Early stopping, best iteration is:\n",
            "[189]\tvalid_0's auc: 0.747895\n",
            "[0.7474572003909496, 0.7509675910624714, 0.7402016129032258, 0.7471517166572345, 0.7352671665723448, 0.7364068100358423, 0.7342859837766459, 0.7410870590454631, 0.7240872005282022, 0.7478952084512356]\n",
            "(0, 0)\n",
            "OOF-MEAN-AUC:0.740481, OOF-STD-AUC:0.007807\n",
            "feature importance:\n",
            "feature\n",
            "svd_countvec_amount_3                         22709.317594\n",
            "fast_text_channel_3_max                        7708.894043\n",
            "product7_fail_ratio                            7595.774363\n",
            "user_type1_45a1168437c708ff_days_diff_min      4350.042335\n",
            "user_trans_ip_cnt                              4282.643722\n",
            "product7_fail_cnt                              3299.473218\n",
            "fast_text_channel_7_max                        3117.651412\n",
            "user_type1_19d44f1a51919482_amount_mean        2827.036647\n",
            "province_label_kfold_mean                      2807.637226\n",
            "city_product1_amount_label_kfold_mean          2673.524570\n",
            "service3                                       2506.238526\n",
            "svd_countvec_amount_0                          2267.625267\n",
            "age                                            2240.764340\n",
            "login_cnt_didive                               2028.869683\n",
            "product7_cnt                                   1841.195428\n",
            "svd_countvec_amount_2                          1807.060728\n",
            "city_card_a_cnt_label_kfold_mean               1795.724265\n",
            "city_label_kfold_mean                          1745.900283\n",
            "svd_countvec_op_type_3                         1456.847711\n",
            "user_type1_45a1168437c708ff_amount_sum         1423.344074\n",
            "fast_text_amount_8_min                         1356.869993\n",
            "login_cnt_period_CV                            1355.561656\n",
            "user_amount_count_15d                          1329.654253\n",
            "svd_countvec_amount_5                          1298.317250\n",
            "user_type1_45a1168437c708ff_days_diff_mean     1292.298183\n",
            "svd_tfidf_op_type_3                            1256.581361\n",
            "user_amount_mean_10h                           1189.007678\n",
            "svd_countvec_amount_1                          1186.674774\n",
            "city_balance_label_kfold_mean                  1148.319473\n",
            "fast_text_amount_5_mean                        1139.080602\n",
            "svd_countvec_op_type_7                         1133.990033\n",
            "agreement3                                     1116.532461\n",
            "user_type1_45a1168437c708ff_days_diff_skew     1100.049419\n",
            "fast_text_amount_0_mean                        1097.017735\n",
            "login_cnt_period_var                           1053.039260\n",
            "svd_countvec_op_mode_3                         1049.649401\n",
            "fast_text_type2_5_mean                         1017.995803\n",
            "city_balance2_avg_label_kfold_mean              998.040634\n",
            "city_balance1_avg_label_kfold_mean              991.842795\n",
            "svd_countvec_op_mode_2                          980.274136\n",
            "user_amount_mean_15h                            979.926488\n",
            "user_type1_45a1168437c708ff_amount_skew         959.297486\n",
            "user_op_days_diff_mean                          955.564775\n",
            "op_cnt_avg                                      933.720078\n",
            "user_trans_ip_3_cnt                             921.687201\n",
            "ip_cnt_avg                                      897.479920\n",
            "svd_tfidf_amount_3                              894.415292\n",
            "user_op_op_device_null_ratio                    839.263549\n",
            "svd_countvec_amount_6                           830.624044\n",
            "acc_count_per_time                              815.927736\n",
            "Name: importance, dtype: float64\n",
            "train auc: 0.7401907944370946\n",
            "CPU times: user 1h 1min 36s, sys: 1min 20s, total: 1h 2min 57s\n",
            "Wall time: 33min 53s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJmgCtb_rJJY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}