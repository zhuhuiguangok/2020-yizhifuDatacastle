{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "Final-V3-{0.701}.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "iI3eXzjHoJWm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "22be23c9-2e19-4cb5-e580-c704ce0b49d0"
      },
      "source": [
        "# coding: utf-8\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from tqdm import tqdm\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import f1_score,roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import gc\n",
        "from gensim.models import Word2Vec, FastText\n",
        "import os\n",
        "import itertools\n",
        "import multiprocessing\n",
        "warnings.filterwarnings('ignore')\n",
        "print(multiprocessing.cpu_count())\n",
        "os.chdir(r\"/content/drive/My Drive/Colab Notebooks/Datacastle/code\")\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i12ySnOnoJWr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_dataset(DATA_PATH):\n",
        "    train_label = pd.read_csv(DATA_PATH+'train_label.csv')\n",
        "    train_base = pd.read_csv(DATA_PATH+'train_base.csv')\n",
        "    test_base = pd.read_csv(DATA_PATH+'test_a_base.csv')\n",
        "\n",
        "    train_op = pd.read_csv(DATA_PATH+'train_op.csv')\n",
        "    train_trans = pd.read_csv(DATA_PATH+'train_trans.csv')\n",
        "    test_op = pd.read_csv(DATA_PATH+'test_a_op.csv')\n",
        "    test_trans = pd.read_csv(DATA_PATH+'test_a_trans.csv')\n",
        "\n",
        "    return train_label, train_base, test_base, train_op, train_trans, test_op, test_trans\n",
        "\n",
        "\n",
        "def transform_time(x):\n",
        "    day = int(x.split(' ')[0])\n",
        "    hour = int(x.split(' ')[2].split('.')[0].split(':')[0])\n",
        "    minute = int(x.split(' ')[2].split('.')[0].split(':')[1])\n",
        "    second = int(x.split(' ')[2].split('.')[0].split(':')[2])\n",
        "    return 86400*day+3600*hour+60*minute+second\n",
        "\n",
        "\n",
        "def data_preprocess(DATA_PATH):\n",
        "    train_label, train_base, test_base, train_op, train_trans, test_op, test_trans = load_dataset(DATA_PATH=DATA_PATH)\n",
        "    # 拼接数据\n",
        "    train_df = train_base.copy()\n",
        "    test_df = test_base.copy()\n",
        "    train_df = train_label.merge(train_df, on=['user'], how='left')\n",
        "    del train_base, test_base\n",
        "\n",
        "    op_df = pd.concat([train_op, test_op], axis=0, ignore_index=True)\n",
        "    trans_df = pd.concat([train_trans, test_trans], axis=0, ignore_index=True)\n",
        "    data = pd.concat([train_df, test_df], axis=0, ignore_index=True)\n",
        "    del train_op, test_op, train_df, test_df\n",
        "    # 时间维度的处理\n",
        "    op_df['days_diff'] = op_df['tm_diff'].apply(lambda x: int(x.split(' ')[0]))\n",
        "    trans_df['days_diff'] = trans_df['tm_diff'].apply(lambda x: int(x.split(' ')[0]))\n",
        "    op_df['timestamp'] = op_df['tm_diff'].apply(lambda x: transform_time(x))\n",
        "    trans_df['timestamp'] = trans_df['tm_diff'].apply(lambda x: transform_time(x))\n",
        "    op_df['hour'] = op_df['tm_diff'].apply(lambda x: int(x.split(' ')[2].split('.')[0].split(':')[0]))\n",
        "    trans_df['hour'] = trans_df['tm_diff'].apply(lambda x: int(x.split(' ')[2].split('.')[0].split(':')[0]))\n",
        "    trans_df['week'] = trans_df['days_diff'].apply(lambda x: x % 7)\n",
        "    # 排序\n",
        "    trans_df = trans_df.sort_values(by=['user', 'timestamp'])\n",
        "    op_df = op_df.sort_values(by=['user', 'timestamp'])\n",
        "    trans_df.reset_index(inplace=True, drop=True)\n",
        "    op_df.reset_index(inplace=True, drop=True)\n",
        "\n",
        "    gc.collect()\n",
        "    return data, op_df, trans_df\n",
        "\n",
        "#用户其他特征的统计量\n",
        "def gen_user_status_features(df,value):\n",
        "    group_df = df.groupby(['user'])[value].agg(\n",
        "        a='mean',\n",
        "        b= 'std',\n",
        "        c='max',\n",
        "        d='min',\n",
        "        e='sum',\n",
        "        f='median',\n",
        "        g='count',\n",
        "        h='median',\n",
        "        i='skew',\n",
        "        ).reset_index()\n",
        "    group_df['j']=group_df['b'] / group_df['a']\n",
        "    group_df.rename(columns={\n",
        "                    'a':'user_{}_mea'.format(value),\n",
        "                    'b':'user_{}_std'.format(value),\n",
        "                    'c':'user_{}_max'.format(value),\n",
        "                    'd':'user_{}_min'.format(value),\n",
        "                    'e':'user_{}_sum'.format(value),\n",
        "                    'f':'user_{}_med'.format(value),\n",
        "                    'g':'user_{}_cnt'.format(value),\n",
        "                    'h':'user_{}_median'.format(value),\n",
        "                    'i':'user_{}_skew'.format(value),\n",
        "                    'j':'user_{}_CV'.format(value)\n",
        "                    },\n",
        "                   inplace=True\n",
        "                   )\n",
        "    return group_df\n",
        "#用户所在城市的其他衍生特征\n",
        "def gen_city_status_features(df,value):\n",
        "    group_df = df.groupby(['city'])[value].agg(\n",
        "        a='mean',\n",
        "        b= 'std',\n",
        "        c='max',\n",
        "        d='min',\n",
        "        e='sum',\n",
        "        f='median',\n",
        "        g='count',\n",
        "        h='median',\n",
        "        i='skew',\n",
        "        ).reset_index()\n",
        "    group_df['j']=group_df['b'] / group_df['a']\n",
        "    group_df.rename(columns={\n",
        "                    'a':'city_{}_mea'.format(value),\n",
        "                    'b':'city_{}_std'.format(value),\n",
        "                    'c':'city_{}_max'.format(value),\n",
        "                    'd':'city_{}_min'.format(value),\n",
        "                    'e':'city_{}_sum'.format(value),\n",
        "                    'f':'city_{}_med'.format(value),\n",
        "                    'g':'city_{}_cnt'.format(value),\n",
        "                    'h':'city_{}_median'.format(value),\n",
        "                    'i':'city_{}_skew'.format(value),\n",
        "                    'j':'city_{}_CV'.format(value)\n",
        "                    },\n",
        "                   inplace=True\n",
        "                   )\n",
        "    return group_df\n",
        "\n",
        "\n",
        "#trans#用户交易消费的特征\n",
        "def gen_user_amount_group_features(df,col,val):\n",
        "    df = df[ df[col]==val ]\n",
        "    l = ['mean','std','max','min','sum','median','count','skew','var']\n",
        "    group_df = df.groupby('user')['amount'].agg(l).reset_index()\n",
        "    group_df[ 'user_'+col+'_'+val+'_amount_jicha'] = group_df['max']-group_df['min']\n",
        "    rename_col = {}\n",
        "    for i in l:\n",
        "        rename_col[i] = 'user_'+col+'_'+val+'_amount_'+i\n",
        "    group_df = group_df.rename(columns=rename_col)\n",
        "    return group_df\n",
        "\n",
        "#trans#用户交易消费的特征\n",
        "def gen_user_amount_features(df):\n",
        "    group_df = df.groupby(['user']).agg(\n",
        "         user_amount_mean=('amount','mean'),\n",
        "         user_amount_std=('amount','std'),\n",
        "         user_amount_max=('amount','max'),\n",
        "         user_amount_min=('amount','min'),\n",
        "         user_amount_med=('amount','median'),\n",
        "         user_amount_cnt=('amount','count'),\n",
        "         user_amount_skew=('amount','skew'),\n",
        "         user_amount_var=('amount','var'),\n",
        "        ).reset_index()\n",
        "    group_df['user_amount_jicha'] = group_df['user_amount_max'] - group_df['user_amount_min']\n",
        "    return group_df\n",
        "\n",
        "def gen_user_days_diff_group_features(df,col,val):\n",
        "    df = df[ df[col]==val ]\n",
        "    l = ['mean','std','max','min','sum','median','count','skew','var']\n",
        "    group_df = df.groupby('user')['days_diff'].agg(l).reset_index()\n",
        "    group_df[ 'user_'+col+'_'+val+'_days_diff_jicha'] = group_df['max']-group_df['min']\n",
        "    rename_col = {}\n",
        "    for i in l:\n",
        "        rename_col[i] = 'user_'+col+'_'+val+'_days_diff_'+i\n",
        "    group_df = group_df.rename(columns=rename_col)\n",
        "    return group_df\n",
        "\n",
        "#trans 用户在某平台或者用某ip的消费额特征\n",
        "def gen_user_group_amount_features(df, value):\n",
        "    group_df = df.pivot_table(index='user',\n",
        "                              columns=value,\n",
        "                              values='amount',\n",
        "                              dropna=False,\n",
        "                              aggfunc=['count','sum','median',\"skew\",'var','mean','std','max','min']).fillna(0)\n",
        "    group_df.columns = ['user_{}_{}_amount_{}'.format(value, f[1], f[0]) for f in group_df.columns]\n",
        "    group_df.reset_index(inplace=True)\n",
        "\n",
        "    return group_df\n",
        "\n",
        "\n",
        "#用户在某个时间段内的消费额特征 大于天数的窗口\n",
        "def gen_user_window_amount_features(df, window):\n",
        "    group_df = df[df['days_diff']>window].groupby('user').agg(\n",
        "        mean=('amount','mean'),\n",
        "        std=('amount','std'),\n",
        "        max=('amount','max'),\n",
        "        min=('amount','min'),\n",
        "        median=('amount','median'),\n",
        "        count=('amount','count'),\n",
        "        skew=('amount','skew'),\n",
        "        var=('amount','var'),\n",
        "        ).reset_index().rename(columns={\"mean\":'user_amount_mean_{}d'.format(window),\n",
        "                        \"std\":'user_amount_std_{}d'.format(window),\n",
        "                        \"max\":'user_amount_max_{}d'.format(window),\n",
        "                        \"min\":'user_amount_min_{}d'.format(window),\n",
        "                        \"sum\":'user_amount_sum_{}d'.format(window),\n",
        "                        \"median\":'user_amount_median_{}d'.format(window),\n",
        "                        \"count\":'user_amount_count_{}d'.format(window),\n",
        "                        \"skew\":'user_amount_skew_{}d'.format(window),\n",
        "                        \"var\":'user_amount_var_{}d'.format(window),\n",
        "                        })\n",
        "    return group_df\n",
        "\n",
        "\n",
        "#用户在某个时间段内的消费额特征 大于小时的窗口\n",
        "def gen_user_hourwindow_amount_features(df, window):\n",
        "    group_df = df[df['hour']>=window].groupby('user').agg(\n",
        "        mean=('amount','mean'),\n",
        "        count=('amount','count'),\n",
        "        ).reset_index().rename(columns={\"mean\":'user_amount_mean_{}h'.format(window),\n",
        "                        \"count\":'user_amount_count_{}h'.format(window),\n",
        "                        })\n",
        "    return group_df\n",
        "\n",
        "\n",
        "#trans用户  ['days_diff', 'platform', 'tunnel_in', 'tunnel_out', 'type1', 'type2', 'ip', 'ip_3']交易用到各字段的类数\n",
        "def gen_user_nunique_features(df, value, prefix):\n",
        "    group_df = df.groupby(['user'])[value].agg(\n",
        "        ['nunique','count']\n",
        "    ).reset_index().rename(columns={\"nunique\":'user_{}_{}_nuniq'.format(prefix, value),\n",
        "                                   'count':'user_{}_{}_cnt'.format(prefix, value)})\n",
        "    return group_df\n",
        "\n",
        "\n",
        "#trans用户无IP 这个特征贼奇怪 怎么会交易没有ip呢\n",
        "def gen_user_null_features(df, value, prefix):\n",
        "    df['is_null'] = 0\n",
        "    df.loc[df[value].isnull(), 'is_null'] = 1\n",
        "\n",
        "    group_df = df.groupby(['user'])['is_null'].agg(sum='sum',\n",
        "                            mean='mean').reset_index().rename(columns={\"sum\":'user_{}_{}_null_cnt'.format(prefix, value),\n",
        "                                                        \"mean\":'user_{}_{}_null_ratio'.format(prefix, value)})\n",
        "    return group_df\n",
        "\n",
        "\n",
        "# op op_mode op_tyep op_device net_type channel\n",
        "def gen_user_tfidf_features(df, value):\n",
        "    #填充缺失值\n",
        "    df[value].replace(' ', np.nan, inplace=True)\n",
        "    df[value] = df[value].astype(str)\n",
        "    df[value].fillna('-1', inplace=True)\n",
        "\n",
        "    #\n",
        "    group_df = df.groupby(['user']).apply(lambda x: x[value].tolist()).reset_index()\n",
        "    group_df.columns = ['user', 'list']\n",
        "    group_df['list'] = group_df['list'].apply(lambda x: ','.join(x))\n",
        "    enc_vec = TfidfVectorizer()\n",
        "    tfidf_vec = enc_vec.fit_transform(group_df['list'])\n",
        "    svd_enc = TruncatedSVD(n_components=10, n_iter=20, random_state=2020)\n",
        "    vec_svd = svd_enc.fit_transform(tfidf_vec)\n",
        "    vec_svd = pd.DataFrame(vec_svd)\n",
        "    vec_svd.columns = ['svd_tfidf_{}_{}'.format(value, i) for i in range(10)]\n",
        "    group_df = pd.concat([group_df, vec_svd], axis=1)\n",
        "    del group_df['list']\n",
        "    return group_df\n",
        "\n",
        "\n",
        "# op op_mode op_tyep op_device net_type channel\n",
        "def gen_user_countvec_features(df, value):\n",
        "    #填充缺失值\n",
        "    df[value].replace(' ', np.nan, inplace=True)\n",
        "    df[value] = df[value].astype(str)\n",
        "    df[value].fillna('-1', inplace=True)\n",
        "    #\n",
        "    group_df = df.groupby(['user']).apply(lambda x: x[value].tolist()).reset_index()\n",
        "    group_df.columns = ['user', 'list']\n",
        "    group_df['list'] = group_df['list'].apply(lambda x: ','.join(x))\n",
        "    enc_vec = CountVectorizer()\n",
        "    tfidf_vec = enc_vec.fit_transform(group_df['list'])\n",
        "    svd_enc = TruncatedSVD(n_components=10, n_iter=20, random_state=2020)\n",
        "    vec_svd = svd_enc.fit_transform(tfidf_vec)\n",
        "    vec_svd = pd.DataFrame(vec_svd)\n",
        "    vec_svd.columns = ['svd_countvec_{}_{}'.format(value, i) for i in range(10)]\n",
        "    group_df = pd.concat([group_df, vec_svd], axis=1)\n",
        "    del group_df['list']\n",
        "    return group_df\n",
        "\n",
        "\n",
        "#target_encode_cols = ['province', 'city', 'city_level', 'city_balance_avg']\n",
        "#target_encode\n",
        "def kfold_stats_feature(train, test, feats, k):\n",
        "    folds = StratifiedKFold(n_splits=k, shuffle=True, random_state=2020)  # 这里最好和后面模型的K折交叉验证保持一致\n",
        "\n",
        "    train['fold'] = None\n",
        "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(train, train['label'])):\n",
        "        train.loc[val_idx, 'fold'] = fold_\n",
        "\n",
        "    kfold_features = []\n",
        "    for feat in feats:\n",
        "        nums_columns = ['label']\n",
        "        #只有一个还用for?\n",
        "        for f in nums_columns:\n",
        "            colname = feat + '_' + f + '_kfold_mean'\n",
        "            kfold_features.append(colname)\n",
        "            train[colname] = None\n",
        "            for fold_, (trn_idx, val_idx) in enumerate(folds.split(train, train['label'])):\n",
        "                tmp_trn = train.iloc[trn_idx]\n",
        "                order_label = tmp_trn.groupby([feat])[f].mean()\n",
        "\n",
        "                tmp = train.loc[train.fold == fold_, [feat]]\n",
        "                train.loc[train.fold == fold_, colname] = tmp[feat].map(order_label)#其实就是用kfold方式对该特征的label标签mean化，然后建个新列\n",
        "                \n",
        "                # fillna 平均值填充\n",
        "                global_mean = train[f].mean()\n",
        "                train.loc[train.fold == fold_, colname] = train.loc[train.fold == fold_, colname].fillna(global_mean)\n",
        "            train[colname] = train[colname].astype(float)\n",
        "\n",
        "        for f in nums_columns:\n",
        "            colname = feat + '_' + f + '_kfold_mean'\n",
        "            test[colname] = None\n",
        "            order_label = train.groupby([feat])[f].mean()\n",
        "            test[colname] = test[feat].map(order_label)\n",
        "            # fillna\n",
        "            global_mean = train[f].mean()\n",
        "            test[colname] = test[colname].fillna(global_mean)\n",
        "            test[colname] = test[colname].astype(float)\n",
        "    del train['fold']\n",
        "    return train, test\n",
        "\n",
        "\n",
        "def w2v_feat(df, feat, mode):\n",
        "    data_frame=df.copy()\n",
        "    # 转化为str\n",
        "    for i in feat:\n",
        "        if data_frame[i].dtype != 'object':\n",
        "            data_frame[i] = data_frame[i].astype(str)\n",
        "    data_frame.fillna('nan', inplace=True) # 甜宠\n",
        "\n",
        "    print(f'Start {mode} word2vec ...')\n",
        "    model = Word2Vec(data_frame[feat].values.tolist(), size=10, window=2, min_count=1,\n",
        "                     workers=multiprocessing.cpu_count(), iter=10)\n",
        "    stat_list = ['min', 'max', 'mean', 'std']\n",
        "    new_all = pd.DataFrame()\n",
        "    for m, t in enumerate(feat):\n",
        "        print(f'Start gen feat of {t} ...')\n",
        "        tmp = []\n",
        "        for i in data_frame[t].unique():\n",
        "            tmp_v = [i]\n",
        "            tmp_v.extend(model[i])\n",
        "            tmp.append(tmp_v)\n",
        "        tmp_df = pd.DataFrame(tmp)\n",
        "        w2c_list = [f'w2c_{t}_{n}' for n in range(10)]\n",
        "        tmp_df.columns = [t] + w2c_list\n",
        "        tmp_df = data_frame[['user', t]].merge(tmp_df, on=t)\n",
        "        tmp_df = tmp_df.drop_duplicates().groupby('user').agg(stat_list).reset_index()\n",
        "        tmp_df.columns = ['user'] + [f'{p}_{q}' for p in w2c_list for q in stat_list]\n",
        "        if m == 0:\n",
        "            new_all = pd.concat([new_all, tmp_df], axis=1)\n",
        "        else:\n",
        "            new_all = pd.merge(new_all, tmp_df, how='left', on='user')\n",
        "    return new_all\n",
        "\n",
        "\n",
        "def fasttext(df, feat, mode):\n",
        "    data_frame=df.copy()\n",
        "    # 转化为str\n",
        "    for i in feat:\n",
        "        if data_frame[i].dtype != 'object':\n",
        "            data_frame[i] = data_frame[i].astype(str)\n",
        "    data_frame.fillna('nan', inplace=True) # 甜宠\n",
        "\n",
        "    print(f'Start {mode} FastText ...')\n",
        "    model = FastText(data_frame[feat].values.tolist(),  size=10, window=3, min_count=1, \n",
        "                     workers=multiprocessing.cpu_count(), iter=10,min_n = 3 , max_n = 6,word_ngrams = 0,seed=1)\n",
        "    stat_list = ['min', 'max', 'mean', 'std']\n",
        "    new_all = pd.DataFrame()\n",
        "    for m, t in enumerate(feat):\n",
        "        print(f'Start gen feat of {t} ...')\n",
        "        tmp = []\n",
        "        for i in data_frame[t].unique():\n",
        "            tmp_v = [i]\n",
        "            tmp_v.extend(model[i])\n",
        "            tmp.append(tmp_v)\n",
        "        tmp_df = pd.DataFrame(tmp)\n",
        "        w2c_list = [f'fast_text_{t}_{n}' for n in range(10)]\n",
        "        tmp_df.columns = [t] + w2c_list\n",
        "        tmp_df = data_frame[['user', t]].merge(tmp_df, on=t)\n",
        "        tmp_df = tmp_df.drop_duplicates().groupby('user').agg(stat_list).reset_index()\n",
        "        tmp_df.columns = ['user'] + [f'{p}_{q}' for p in w2c_list for q in stat_list]\n",
        "        if m == 0:\n",
        "            new_all = pd.concat([new_all, tmp_df], axis=1)\n",
        "        else:\n",
        "            new_all = pd.merge(new_all, tmp_df, how='left', on='user')\n",
        "    return new_all\n",
        "\n",
        "def gen_features(df, op, trans):\n",
        "\n",
        "    # base\n",
        "    print(\"base\")\n",
        "    df['product7_fail_ratio'] = df['product7_fail_cnt'] / df['product7_cnt']\n",
        "    df['city_count'] = df.groupby(['city'])['user'].transform('count')\n",
        "    df['province_count'] = df.groupby(['province'])['user'].transform('count')\n",
        "    df['op_cnt_avg'] = (df[\"op1_cnt\"]+df[\"op2_cnt\"])/df[\"using_time\"]\n",
        "    df['ip_cnt_avg'] = df[\"ip_cnt\"]/df[\"using_time\"]\n",
        "    df['card_a_cnt_avg'] = df[\"card_a_cnt\"]/df[\"using_time\"]\n",
        "    df['card_b_cnt_avg'] = df[\"card_b_cnt\"]/df[\"using_time\"]\n",
        "    df['card_c_cnt_avg'] = df[\"card_c_cnt\"]/df[\"using_time\"]\n",
        "    df['card_d_cnt_avg'] = df[\"card_d_cnt\"]/df[\"using_time\"]\n",
        "    df['login_cnt_didive'] = df[\"login_cnt_period1\"]/df[\"login_cnt_period2\"]\n",
        "    df['ip_per_day_cnt'] = df[\"ip_cnt\"]/df[\"login_days_cnt\"]\n",
        "    df['acc_count_per_time'] = df[\"acc_count\"]/df[\"using_time\"]\n",
        "    df['agreement_total_per_using_time'] = df[\"agreement_total\"]/df[\"using_time\"]\n",
        "    df['login_cnt'] = df[\"using_time\"]*df[\"login_cnt_avg\"]\n",
        "    df['agreement_total_per_account_cnt'] = df[\"agreement_total\"]/df[\"acc_count\"]\n",
        "    df['ip_cnt_per_acc_count'] = df[\"ip_cnt\"]/df[\"acc_count\"]\n",
        "    df['service1_amt_per_using_time'] = df[\"service1_amt\"]/df[\"using_time\"]\n",
        "    df[\"service_cnt_per_time\"]=(df[\"service1_cnt\"]+df[\"service1_cnt\"])/df[\"using_time\"]\n",
        "    \n",
        "    df['login_cnt_period_var']=df[['login_cnt_period1','login_cnt_period2']].std(axis=1)\n",
        "    df['login_cnt_period_CV']=df['login_cnt_period_var'] / df[['login_cnt_period1','login_cnt_period2']].mean(axis=1)\n",
        "    df['card_var']=df[['card_a_cnt','card_b_cnt','card_c_cnt','card_d_cnt']].std(axis=1)\n",
        "    df['card_CV']=df['card_var'] / df[['card_a_cnt','card_b_cnt','card_c_cnt','card_d_cnt']].mean(axis=1)\n",
        "    df['op_cnt_var']=df[['op1_cnt','op2_cnt']].std(axis=1)\n",
        "    df['op_cnt_CV']=df['op_cnt_var'] / df[['op1_cnt','op2_cnt']].mean(axis=1)\n",
        "    df['login_var']=df[['login_cnt_period1','login_cnt_period2','login_cnt_avg']].std(axis=1)\n",
        "    df['login_CV']=df['login_var'] / df[['login_cnt_period1','login_cnt_period2','login_cnt_avg']].mean(axis=1)\n",
        "    df['ip_cnt_per_login_cnt_avg'] = df['ip_cnt']/df[['login_cnt_period1','login_cnt_period2','login_cnt_avg']].mean(axis=1)\n",
        "    df['op_cnt_per_login_cnt_avg'] = (df[\"op1_cnt\"]+df[\"op2_cnt\"])/df['login_cnt_avg']\n",
        "\n",
        "    # trans\n",
        "    print(\"trans nunique & amount feature & user_status_features \")\n",
        "    df = df.merge(gen_user_status_features(trans,'amount'), on=['user'], how='left')\n",
        "    df = df.merge(gen_user_status_features(trans,'days_diff'), on=['user'], how='left')\n",
        "    df = df.merge(gen_user_status_features(trans,'hour'), on=['user'], how='left')\n",
        "\n",
        "    for col in tqdm(['days_diff','platform', 'tunnel_in', 'tunnel_out', 'type1', 'type2', 'ip', 'ip_3']):\n",
        "        df = df.merge(gen_user_nunique_features(df=trans, value=col, prefix='trans'), on=['user'], how='left')\n",
        "    df['user_amount_per_days'] = df['user_amount_sum'] / df['user_trans_days_diff_nuniq']\n",
        "    df['user_amount_per_cnt'] = df['user_amount_sum'] / df['user_amount_cnt']\n",
        "    df = df.merge(gen_user_amount_features(trans), on=['user'], how='left')\n",
        "\n",
        "    temp=trans[trans['ip'].isnull()].groupby(['user'])['amount'].agg(user_null_ip_amount_sum='sum').reset_index()\n",
        "    df=df.merge(temp,on=['user'],how='left')\n",
        "    df['user_null_ip_amount_sum'].fillna(0,inplace=True)\n",
        "    df['user_null_ip_amount_percent'] = df['user_null_ip_amount_sum'] / df['user_amount_sum']\n",
        "    df['user_ip_amount_percent'] = 1-df['user_null_ip_amount_percent'] \n",
        "    df['user_amount_per_tunnel_in'] = df['user_amount_sum'] / df['user_trans_tunnel_in_nuniq']\n",
        "    df['user_amount_per_platform'] = df['user_amount_sum'] / df['user_trans_platform_nuniq']\n",
        "\n",
        "    # df=df.merge(df.groupby(['city'])['user_amount_sum'].agg(city_amount_mean='mean',city_amount_sum='sum',city_amount_std='std').reset_index(),on=['city'],how='left')\n",
        "    # df=df.merge(gen_city_status_features(df,'user_trans_ip_null_cnt'),on=['city'],how='left')\n",
        "    # df=df.merge(gen_city_status_features(df,'user_trans_ip_count'),on=['city'],how='left')\n",
        "    # df['city_ipcnt_vs_nullipcnt']=df['city_user_trans_ip_count_sum'] / df['city_user_trans_ip_null_cnt_sum']\n",
        "    # df=df.merge(gen_city_status_features(df,'user_null_ip_amount_sum'),on=['city'],how='left')\n",
        "    # df['city_null_ip_amount_percent'] = df['city_user_null_ip_amount_sum_sum'] / df['city_amount_sum'] \n",
        "\n",
        "    print(\"trans window\") \n",
        "    df = df.merge(gen_user_window_amount_features(df=trans, window=27), on=['user'], how='left')\n",
        "    df = df.merge(gen_user_window_amount_features(df=trans, window=23), on=['user'], how='left')\n",
        "    # df = df.merge(gen_user_window_amount_features(df=trans, window=19), on=['user'], how='left')\n",
        "    df = df.merge(gen_user_window_amount_features(df=trans, window=15), on=['user'], how='left')\n",
        "    df = df.merge(gen_user_hourwindow_amount_features(df=trans, window=20), on=['user'], how='left')\n",
        "    df = df.merge(gen_user_hourwindow_amount_features(df=trans, window=15), on=['user'], how='left')\n",
        "    df = df.merge(gen_user_hourwindow_amount_features(df=trans, window=10), on=['user'], how='left')\n",
        "    print(\"trans null_features\")\n",
        "    df = df.merge(gen_user_null_features(df=trans, value='type2', prefix='trans'), on=['user'], how='left')\n",
        "    df = df.merge(gen_user_null_features(df=trans, value='ip_3', prefix='trans'), on=['user'], how='left')\n",
        "    df = df.merge(gen_user_null_features(df=trans, value='tunnel_in', prefix='trans'), on=['user'], how='left')\n",
        "    df = df.merge(gen_user_null_features(df=trans, value='tunnel_out', prefix='trans'), on=['user'], how='left')\n",
        "\n",
        "    print(\"trans group_amount_features\")\n",
        "    print(\"type1\")\n",
        "    for i in [\"674e8d5860bc033d\",'443b0fd0860c21b6','45a1168437c708ff','f67d4b5a05a1352a','443b0fd0860c21b6',\n",
        "              '3146295fbf43c0cb','8adb3dcfea9dcf5e','33e9d4cef01499e1','19d44f1a51919482','0a3cf8dac7dca9d1']:\n",
        "        df = df.merge(gen_user_days_diff_group_features(trans,'type1',i), on=['user'], how='left')\n",
        "        df = df.merge(gen_user_amount_group_features(trans,'type1',i), on=['user'], how='left')\n",
        "    \n",
        "    print(\"type2\")\n",
        "    for i in ['11a213398ee0c623','2ee592ab06090eb5','b5a8be737a50b171','2ee592ab06090eb5']:\n",
        "        df = df.merge(gen_user_days_diff_group_features(trans,'type1',i), on=['user'], how='left')\n",
        "        df = df.merge(gen_user_amount_group_features(trans,'type1',i), on=['user'], how='left')\n",
        "        \n",
        "    print(\"tunnel_in\")\n",
        "    for i in ['b2e7fa260df4998d']:\n",
        "        df = df.merge(gen_user_days_diff_group_features(trans,'type1',i), on=['user'], how='left')\n",
        "        df = df.merge(gen_user_amount_group_features(trans,'type1',i), on=['user'], how='left')\n",
        "        \n",
        "    print(\"tunnel_out\")\n",
        "    for i in ['6ee790756007e69a','4c8524fb01d8b204']:\n",
        "        df = df.merge(gen_user_days_diff_group_features(trans,'type1',i), on=['user'], how='left')\n",
        "        df = df.merge(gen_user_amount_group_features(trans,'type1',i), on=['user'], how='left')\n",
        "    \n",
        "    df = df.merge(gen_user_tfidf_features(df=trans, value='amount'), on=['user'], how='left')\n",
        "    df = df.merge(gen_user_countvec_features(df=trans, value='amount'), on=['user'], how='left')\n",
        "    \n",
        "    df=df.merge(fasttext(trans,['amount',\"type1\",\"type2\"],\"trans\"),on=['user'],how='left')\n",
        "\n",
        "    print(\"op\")\n",
        "    # op\n",
        "    print(\"op null features\")\n",
        "    df = df.merge(gen_user_null_features(df=op, value=\"net_type\", prefix='op'), on=['user'], how='left')\n",
        "    df = df.merge(gen_user_null_features(df=op, value=\"op_device\", prefix='op'), on=['user'], how='left')\n",
        "    df = df.merge(gen_user_null_features(df=op, value=\"ip\", prefix='op'), on=['user'], how='left') \n",
        "\n",
        "    df=df.merge(op.groupby(\"user\").agg(user_op_days_diff_mean=('days_diff','mean')),on=['user'], how='left')\n",
        "    df=df.merge(op.groupby(\"user\").agg(user_op_hour_mean=('hour','mean')),on=['user'], how='left')\n",
        "\n",
        "    df=df.merge(op.groupby(\"user\").agg(user_op_tm_diff_count=('tm_diff','count')),on=['user'], how='left')\n",
        "\n",
        "    print(\"op nunique features\")\n",
        "    for col in tqdm(['days_diff','hour', 'op_mode', 'op_type', 'op_device',\"ip\",'channel',\"ip_3\",]):\n",
        "        df = df.merge(gen_user_nunique_features(df=op, value=col, prefix='op'), on=['user'], how='left')\n",
        "\n",
        "    print(\"op tfidf countvec encoder features\")\n",
        "    df = df.merge(gen_user_tfidf_features(df=op, value='op_mode'), on=['user'], how='left')\n",
        "    df = df.merge(gen_user_tfidf_features(df=op, value='op_type'), on=['user'], how='left')\n",
        "\n",
        "    df = df.merge(gen_user_countvec_features(df=op, value='op_mode'), on=['user'], how='left')\n",
        "    df = df.merge(gen_user_countvec_features(df=op, value='op_type'), on=['user'], how='left')\n",
        "\n",
        "    print(\"op fasttext encoder features\")\n",
        "    df=df.merge(fasttext(op,[\"op_mode\",\"op_type\",\"op_device\",\"channel\",'ip_3',\"net_type\",\"ip\"],\"op\"),on=['user'],how='left')\n",
        "\n",
        "    print(\"LabelEncoder\")\n",
        "    for col in tqdm([f for f in df.select_dtypes('object').columns if f not in ['user']]):\n",
        "        le = LabelEncoder()\n",
        "        df[col] = df[col].apply(lambda x:str(x))\n",
        "        df[col].fillna('-1', inplace=True)\n",
        "        df[col] = le.fit_transform(df[col])\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def lgb_model(train, target, test, k):\n",
        "    feats = [f for f in train.columns if f not in ['user', 'label']]\n",
        "    print('Current num of features:', len(feats))\n",
        "    folds = StratifiedKFold(n_splits=k, shuffle=True, random_state=2020)\n",
        "    oof_probs = np.zeros(train.shape[0])\n",
        "    output_preds = 0\n",
        "    offline_score = []\n",
        "    feature_importance_df = pd.DataFrame()\n",
        "    parameters = {\n",
        "        'learning_rate': 0.03,\n",
        "        'boosting_type': 'gbdt',\n",
        "        'objective': 'binary',\n",
        "        'metric': 'auc',\n",
        "        'num_leaves': 50,\n",
        "        'feature_fraction': 0.8,\n",
        "        'bagging_fraction': 0.8,\n",
        "        'min_data_in_leaf': 20,\n",
        "        'reg_alpha':10,\n",
        "        'reg_lambda':8,\n",
        "        'verbose': -1,\n",
        "        'nthread': 8,\n",
        "        'colsample_bytree':0.77,\n",
        "        'min_child_weight':4,\n",
        "        'min_child_samples':10,\n",
        "        'min_split_gain':0,\n",
        "        'lambda_l1': 0.8,\n",
        "    }\n",
        "\n",
        "    for i, (train_index, test_index) in enumerate(folds.split(train, target)):\n",
        "        train_y, test_y = target[train_index], target[test_index]\n",
        "        train_X, test_X = train[feats].iloc[train_index, :], train[feats].iloc[test_index, :]\n",
        "\n",
        "        dtrain = lgb.Dataset(train_X,\n",
        "                             label=train_y)\n",
        "        dval = lgb.Dataset(test_X,\n",
        "                           label=test_y)\n",
        "        lgb_model = lgb.train(\n",
        "                parameters,\n",
        "                dtrain,\n",
        "                num_boost_round=5000,\n",
        "                valid_sets=[dval],\n",
        "                early_stopping_rounds=100,\n",
        "                verbose_eval=100,\n",
        "        )\n",
        "        oof_probs[test_index] = lgb_model.predict(test_X[feats], num_iteration=lgb_model.best_iteration)\n",
        "        offline_score.append(lgb_model.best_score['valid_0']['auc'])\n",
        "        output_preds += lgb_model.predict(test[feats], num_iteration=lgb_model.best_iteration)/folds.n_splits\n",
        "        print(offline_score)\n",
        "        # feature importance\n",
        "        fold_importance_df = pd.DataFrame()\n",
        "        print(fold_importance_df.shape)\n",
        "        fold_importance_df[\"feature\"] = feats\n",
        "        fold_importance_df[\"importance\"] = lgb_model.feature_importance(importance_type='gain')\n",
        "        fold_importance_df[\"fold\"] = i + 1\n",
        "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
        "    print('OOF-MEAN-AUC:%.6f, OOF-STD-AUC:%.6f' % (np.mean(offline_score), np.std(offline_score)))\n",
        "    print('feature importance:')\n",
        "    print(feature_importance_df.groupby(['feature'])['importance'].mean().sort_values(ascending=False).head(60))\n",
        "    feature_importance_df.groupby(['feature'])['importance'].mean().to_csv(\"../submission/feature_importance.csv\")\n",
        "    return output_preds, oof_probs, np.mean(offline_score),feature_importance_df\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYhsd0tOoJWv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588
        },
        "outputId": "a726a56f-f6f6-457f-d598-33897455b662"
      },
      "source": [
        "%%time\n",
        "DATA_PATH = '../data/'\n",
        "print('读取数据...')\n",
        "data, op_df, trans_df = data_preprocess(DATA_PATH=DATA_PATH)\n",
        "\n",
        "print('开始特征工程...')\n",
        "data = gen_features(data, op_df, trans_df)\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "读取数据...\n",
            "开始特征工程...\n",
            "base\n",
            "trans nunique & amount feature & user_status_features \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 8/8 [00:02<00:00,  2.73it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "trans window\n",
            "trans null_features\n",
            "trans group_amount_features\n",
            "type1\n",
            "type2\n",
            "tunnel_in\n",
            "tunnel_out\n",
            "Start trans FastText ...\n",
            "Start gen feat of amount ...\n",
            "Start gen feat of type1 ...\n",
            "Start gen feat of type2 ...\n",
            "op\n",
            "op null features\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/8 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "op nunique features\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 8/8 [00:12<00:00,  1.60s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "op tfidf countvec encoder features\n",
            "op fasttext encoder features\n",
            "Start op FastText ...\n",
            "Start gen feat of op_mode ...\n",
            "Start gen feat of op_type ...\n",
            "Start gen feat of op_device ...\n",
            "Start gen feat of channel ...\n",
            "Start gen feat of ip_3 ...\n",
            "Start gen feat of net_type ...\n",
            "Start gen feat of ip ...\n",
            "LabelEncoder\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 25/25 [00:01<00:00, 24.67it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 45min 8s, sys: 26.1 s, total: 45min 34s\n",
            "Wall time: 28min 15s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9psMSh1MoJW6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4c7d75e1-95a1-455b-b33f-f56eb567c667"
      },
      "source": [
        "\n",
        "data['city_level'] = data['city'].map(str) + '_' + data['level'].map(str)\n",
        "data['city_product1_amount'] = data['city'].map(str) + '_' + data['product1_amount'].map(str)\n",
        "data['city_balance1_avg'] = data['city'].map(str) + '_' + data['balance1_avg'].map(str)\n",
        "data['city_balance2_avg'] = data['city'].map(str) + '_' + data['balance2_avg'].map(str)\n",
        "data['city_balance'] = data['city'].map(str) + '_' + data['balance'].map(str)\n",
        "data['city_card_a_cnt'] = data['city'].map(str) + '_' + data['card_a_cnt'].map(str)\n",
        "\n",
        "\n",
        "train = data[~data['label'].isnull()].copy()\n",
        "target = train['label']\n",
        "test = data[data['label'].isnull()].copy()\n",
        "\n",
        "print(\"target encoder...\")\n",
        "  \n",
        "target_encode_cols = ['province','city',\"city_level\",'city_balance1_avg',\n",
        "                      'city_balance2_avg',\"city_product1_amount\",'city_balance','city_card_a_cnt']\n",
        "\n",
        "train, test = kfold_stats_feature(train, test, target_encode_cols, 10)\n",
        "train.drop(['province', \"city\",\"service3_level\",\"city_level\",'city_balance1_avg','city_balance2_avg',\n",
        "              \"city_product1_amount\",'city_balance','city_card_a_cnt'], axis=1, inplace=True)\n",
        "test.drop(['province',\"city\",\"service3_level\",\"city_level\",'city_balance1_avg','city_balance2_avg',\n",
        "              \"city_product1_amount\",'city_balance','city_card_a_cnt'], axis=1, inplace=True)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "target encoder...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8iDmht-oJXB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9aba165f-9449-40a4-92c0-1a3678efc578"
      },
      "source": [
        "%%time\n",
        "print('开始模型训练...')\n",
        "\n",
        "lgb_preds, lgb_oof, lgb_score,feature_importance_df = lgb_model(train=train, target=target, test=test,k=10)\n",
        "auc_score = roc_auc_score(target.values, lgb_oof)\n",
        "print(\"train auc:\",auc_score)\n",
        "\n",
        "sub_df = test[['user']].copy()\n",
        "sub_df['prob'] = lgb_preds\n",
        "sub_df.to_csv('../submission/sub_lgb{%.5f}.csv'%(lgb_score,), index=False)\n",
        "# 0.740557 (no_day 23 27) 0.70053\n",
        "#val_auc0.740481 on_auc0.700902\n",
        "#0.740219 0.70***6"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "开始模型训练...\n",
            "Current num of features: 1003\n",
            "Training until validation scores don't improve for 100 rounds.\n",
            "[100]\tvalid_0's auc: 0.74395\n",
            "[200]\tvalid_0's auc: 0.74777\n",
            "[300]\tvalid_0's auc: 0.748324\n",
            "Early stopping, best iteration is:\n",
            "[299]\tvalid_0's auc: 0.748353\n",
            "[0.7483525374247579]\n",
            "(0, 0)\n",
            "Training until validation scores don't improve for 100 rounds.\n",
            "[100]\tvalid_0's auc: 0.747535\n",
            "[200]\tvalid_0's auc: 0.749378\n",
            "[300]\tvalid_0's auc: 0.751296\n",
            "[400]\tvalid_0's auc: 0.751794\n",
            "[500]\tvalid_0's auc: 0.751316\n",
            "Early stopping, best iteration is:\n",
            "[404]\tvalid_0's auc: 0.752082\n",
            "[0.7483525374247579, 0.7520821654426307]\n",
            "(0, 0)\n",
            "Training until validation scores don't improve for 100 rounds.\n",
            "[100]\tvalid_0's auc: 0.735997\n",
            "[200]\tvalid_0's auc: 0.740904\n",
            "[300]\tvalid_0's auc: 0.741653\n",
            "[400]\tvalid_0's auc: 0.742894\n",
            "[500]\tvalid_0's auc: 0.742944\n",
            "Early stopping, best iteration is:\n",
            "[486]\tvalid_0's auc: 0.743111\n",
            "[0.7483525374247579, 0.7520821654426307, 0.7431114412375024]\n",
            "(0, 0)\n",
            "Training until validation scores don't improve for 100 rounds.\n",
            "[100]\tvalid_0's auc: 0.742914\n",
            "[200]\tvalid_0's auc: 0.747012\n",
            "[300]\tvalid_0's auc: 0.748482\n",
            "[400]\tvalid_0's auc: 0.749499\n",
            "Early stopping, best iteration is:\n",
            "[365]\tvalid_0's auc: 0.749901\n",
            "[0.7483525374247579, 0.7520821654426307, 0.7431114412375024, 0.7499007262780607]\n",
            "(0, 0)\n",
            "Training until validation scores don't improve for 100 rounds.\n",
            "[100]\tvalid_0's auc: 0.728537\n",
            "[200]\tvalid_0's auc: 0.732497\n",
            "[300]\tvalid_0's auc: 0.734953\n",
            "[400]\tvalid_0's auc: 0.736899\n",
            "[500]\tvalid_0's auc: 0.738576\n",
            "[600]\tvalid_0's auc: 0.738938\n",
            "[700]\tvalid_0's auc: 0.738629\n",
            "Early stopping, best iteration is:\n",
            "[629]\tvalid_0's auc: 0.739354\n",
            "[0.7483525374247579, 0.7520821654426307, 0.7431114412375024, 0.7499007262780607, 0.739354367100547]\n",
            "(0, 0)\n",
            "Training until validation scores don't improve for 100 rounds.\n",
            "[100]\tvalid_0's auc: 0.730519\n",
            "[200]\tvalid_0's auc: 0.734013\n",
            "[300]\tvalid_0's auc: 0.735669\n",
            "[400]\tvalid_0's auc: 0.736182\n",
            "[500]\tvalid_0's auc: 0.736083\n",
            "Early stopping, best iteration is:\n",
            "[416]\tvalid_0's auc: 0.736458\n",
            "[0.7483525374247579, 0.7520821654426307, 0.7431114412375024, 0.7499007262780607, 0.739354367100547, 0.7364577438219204]\n",
            "(0, 0)\n",
            "Training until validation scores don't improve for 100 rounds.\n",
            "[100]\tvalid_0's auc: 0.730665\n",
            "[200]\tvalid_0's auc: 0.735438\n",
            "[300]\tvalid_0's auc: 0.736433\n",
            "[400]\tvalid_0's auc: 0.736167\n",
            "Early stopping, best iteration is:\n",
            "[346]\tvalid_0's auc: 0.737006\n",
            "[0.7483525374247579, 0.7520821654426307, 0.7431114412375024, 0.7499007262780607, 0.739354367100547, 0.7364577438219204, 0.7370062252405206]\n",
            "(0, 0)\n",
            "Training until validation scores don't improve for 100 rounds.\n",
            "[100]\tvalid_0's auc: 0.7307\n",
            "[200]\tvalid_0's auc: 0.735953\n",
            "[300]\tvalid_0's auc: 0.737394\n",
            "[400]\tvalid_0's auc: 0.738033\n",
            "Early stopping, best iteration is:\n",
            "[375]\tvalid_0's auc: 0.738261\n",
            "[0.7483525374247579, 0.7520821654426307, 0.7431114412375024, 0.7499007262780607, 0.739354367100547, 0.7364577438219204, 0.7370062252405206, 0.7382609413318242]\n",
            "(0, 0)\n",
            "Training until validation scores don't improve for 100 rounds.\n",
            "[100]\tvalid_0's auc: 0.718599\n",
            "[200]\tvalid_0's auc: 0.723037\n",
            "[300]\tvalid_0's auc: 0.724062\n",
            "[400]\tvalid_0's auc: 0.725554\n",
            "[500]\tvalid_0's auc: 0.724513\n",
            "Early stopping, best iteration is:\n",
            "[404]\tvalid_0's auc: 0.725752\n",
            "[0.7483525374247579, 0.7520821654426307, 0.7431114412375024, 0.7499007262780607, 0.739354367100547, 0.7364577438219204, 0.7370062252405206, 0.7382609413318242, 0.7257522165629127]\n",
            "(0, 0)\n",
            "Training until validation scores don't improve for 100 rounds.\n",
            "[100]\tvalid_0's auc: 0.743118\n",
            "[200]\tvalid_0's auc: 0.747396\n",
            "[300]\tvalid_0's auc: 0.748794\n",
            "Early stopping, best iteration is:\n",
            "[287]\tvalid_0's auc: 0.748999\n",
            "[0.7483525374247579, 0.7520821654426307, 0.7431114412375024, 0.7499007262780607, 0.739354367100547, 0.7364577438219204, 0.7370062252405206, 0.7382609413318242, 0.7257522165629127, 0.7489987738162611]\n",
            "(0, 0)\n",
            "OOF-MEAN-AUC:0.741928, OOF-STD-AUC:0.007715\n",
            "feature importance:\n",
            "feature\n",
            "svd_countvec_amount_3                         22906.662258\n",
            "product7_fail_ratio                            7640.834229\n",
            "fast_text_channel_7_max                        4875.715011\n",
            "fast_text_channel_6_min                        4383.691846\n",
            "user_type1_45a1168437c708ff_days_diff_min      4269.140187\n",
            "user_trans_ip_cnt                              3819.928896\n",
            "product7_fail_cnt                              3175.038002\n",
            "user_days_diff_sum                             3052.711159\n",
            "svd_countvec_amount_0                          3003.023097\n",
            "province_label_kfold_mean                      2992.674406\n",
            "city_product1_amount_label_kfold_mean          2984.419935\n",
            "user_type1_19d44f1a51919482_amount_mean        2556.935372\n",
            "service3                                       2382.245126\n",
            "age                                            2366.875420\n",
            "fast_text_channel_7_std                        2121.365509\n",
            "login_cnt_didive                               2059.854660\n",
            "city_card_a_cnt_label_kfold_mean               1943.080174\n",
            "product7_cnt                                   1832.812635\n",
            "city_label_kfold_mean                          1815.362695\n",
            "svd_countvec_amount_2                          1682.295418\n",
            "svd_countvec_op_type_3                         1541.372638\n",
            "user_type1_45a1168437c708ff_amount_sum         1413.899076\n",
            "svd_countvec_amount_5                          1289.204687\n",
            "city_balance_label_kfold_mean                  1272.625446\n",
            "svd_tfidf_op_type_3                            1258.135731\n",
            "user_amount_mean_10h                           1249.643256\n",
            "agreement3                                     1237.016155\n",
            "fast_text_amount_5_mean                        1230.432275\n",
            "user_type1_45a1168437c708ff_days_diff_mean     1212.664500\n",
            "login_cnt_period_var                           1180.667676\n",
            "login_cnt_period_CV                            1176.584024\n",
            "fast_text_amount_0_mean                        1142.168597\n",
            "svd_countvec_op_type_7                         1138.995798\n",
            "fast_text_op_mode_7_max                        1116.890505\n",
            "svd_countvec_op_mode_3                         1109.000495\n",
            "user_op_days_diff_mean                         1086.428613\n",
            "city_balance2_avg_label_kfold_mean             1075.141526\n",
            "svd_countvec_op_mode_2                         1063.827601\n",
            "fast_text_op_device_0_max                      1058.492752\n",
            "svd_tfidf_amount_3                             1055.570949\n",
            "city_balance1_avg_label_kfold_mean             1041.652817\n",
            "user_type1_45a1168437c708ff_amount_skew         999.665839\n",
            "op_cnt_avg                                      993.518078\n",
            "acc_count_per_time                              991.279289\n",
            "city_count                                      959.283660\n",
            "svd_tfidf_op_mode_2                             947.094488\n",
            "fast_text_amount_6_std                          947.079698\n",
            "fast_text_op_device_6_min                       944.622294\n",
            "ip_per_day_cnt                                  942.175615\n",
            "ip_cnt_avg                                      931.066237\n",
            "fast_text_amount_1_max                          922.676695\n",
            "user_type1_45a1168437c708ff_days_diff_skew      914.646023\n",
            "login_cnt_period2                               898.440163\n",
            "user_amount_mean_15h                            889.612810\n",
            "city_level_label_kfold_mean                     862.192393\n",
            "user_trans_ip_3_cnt                             814.368287\n",
            "card_a_cnt_avg                                  812.006072\n",
            "sex                                             811.647679\n",
            "login_cnt                                       796.239888\n",
            "svd_countvec_amount_6                           794.942271\n",
            "Name: importance, dtype: float64\n",
            "train auc: 0.7418005380572613\n",
            "CPU times: user 1h 5min 55s, sys: 1min 20s, total: 1h 7min 16s\n",
            "Wall time: 35min 45s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJmgCtb_rJJY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "50abe6cd-53f0-4dfc-caf8-134417cc93d3"
      },
      "source": [
        "\n",
        "#coding=utf-8\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "'''单变量特征选取'''\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "'''去除方差小的特征'''\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "'''循环特征选取'''\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_selection import RFE\n",
        "'''RFE_CV'''\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "\n",
        "\n",
        "class FeatureSelection(object):\n",
        "    def __init__(self,train, label, test, feature_list,feature_num):\n",
        "        self.feature_name = feature_list     # feature name #\n",
        "        self.feature_num = feature_num\n",
        "        self.train, self.label, self.test = train.fillna(-1), label, test.fillna(-1)   # features #\n",
        "        #归一化方便进行特征筛选\n",
        "        standard=MinMaxScaler()\n",
        "        self.train=standard.fit_transform(self.train[self.feature_name])\n",
        "    def variance_threshold(self):\n",
        "        sel = VarianceThreshold()\n",
        "        sel.fit_transform(self.train)\n",
        "        feature_var = list(sel.variances_)    # feature variance #\n",
        "        features = dict(zip(self.feature_name, feature_var))\n",
        "        features = list(dict(sorted(features.items(), key=lambda d: d[1])).keys())[-self.feature_num:]\n",
        "        print(features)   # 100 cols #\n",
        "        return set(features)   # return set type #\n",
        "\n",
        "    def select_k_best(self):\n",
        "        ch2 = SelectKBest(chi2, k=self.feature_num)\n",
        "        ch2.fit(self.train, self.label)\n",
        "        feature_var = list(ch2.scores_)  # feature scores #\n",
        "        features = dict(zip(self.feature_name, feature_var))\n",
        "        features = list(dict(sorted(features.items(), key=lambda d: d[1])).keys())[-self.feature_num:]\n",
        "        print(features)     # 100 cols #\n",
        "        return set(features)    # return set type #\n",
        "\n",
        "    def svc_select(self):\n",
        "        svc = SVC(kernel='rbf', C=1, random_state=2020,verbose=1)    # linear #\n",
        "        rfe = RFE(estimator=svc, n_features_to_select=self.feature_num, step=1,verbose=1)\n",
        "        rfe.fit(self.train, self.label.ravel())\n",
        "        print(rfe.ranking_)\n",
        "        return rfe.ranking_\n",
        "\n",
        "    def tree_select(self):\n",
        "        clf = ExtraTreesClassifier(n_estimators=300, max_depth=7, n_jobs=2,verbose=1)\n",
        "        clf.fit(self.train, self.label)\n",
        "        feature_var = list(clf.feature_importances_)  # feature scores #\n",
        "        features = dict(zip(self.feature_name, feature_var))\n",
        "        features = list(dict(sorted(features.items(), key=lambda d: d[1])).keys())[-self.feature_num:]\n",
        "        print(features)     # 100 cols #\n",
        "        return set(features)  # return set type #\n",
        "\n",
        "    def return_feature_set(self, variance_threshold=False, select_k_best=False, svc_select=False, tree_select=False):\n",
        "        names = set([])\n",
        "        if variance_threshold is True:\n",
        "            name_one = self.variance_threshold()\n",
        "            names = names.union(name_one)\n",
        "        if select_k_best is True:\n",
        "            name_two = self.select_k_best()\n",
        "            names = names.intersection(name_two)\n",
        "        if svc_select is True:\n",
        "            name_three = self.svc_select()\n",
        "            names = names.intersection(name_three)\n",
        "        if tree_select is True:\n",
        "            name_four = self.tree_select()\n",
        "            names = names.intersection(name_four)\n",
        "\n",
        "        # print(len(names))\n",
        "        print(names)\n",
        "        return list(names)\n",
        "\n",
        "feats = [f for f in train.columns if f not in ['user', 'label']]\n",
        "selection = FeatureSelection(train,target,test,feats,900)\n",
        "selection_feature=selection.return_feature_set(variance_threshold=True, select_k_best=True, svc_select=False, tree_select=True)\n",
        "len(selection_feature)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-4042dfe11be7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0mfeats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'user'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m \u001b[0mselection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFeatureSelection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m900\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0mselection_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mselection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_feature_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariance_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselect_k_best\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msvc_select\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree_select\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselection_feature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-4042dfe11be7>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, train, label, test, feature_list, feature_num)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m#归一化方便进行特征筛选\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mstandard\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstandard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvariance_threshold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0msel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVarianceThreshold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;31m# Reset internal state before fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    371\u001b[0m         X = check_array(X,\n\u001b[1;32m    372\u001b[0m                         \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m                         force_all_finite=\"allow-nan\")\n\u001b[0m\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0mdata_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnanmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m             _assert_all_finite(array,\n\u001b[0;32m--> 578\u001b[0;31m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m     58\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                     (type_err,\n\u001b[0;32m---> 60\u001b[0;31m                      msg_dtype if msg_dtype is not None else X.dtype)\n\u001b[0m\u001b[1;32m     61\u001b[0m             )\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input contains infinity or a value too large for dtype('float64')."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zfsG-luN0xx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 7,
      "outputs": []
    }
  ]
}