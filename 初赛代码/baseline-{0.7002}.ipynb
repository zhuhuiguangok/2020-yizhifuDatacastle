{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score,roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import FastText\n",
    "import os\n",
    "import multiprocessing\n",
    "warnings.filterwarnings('ignore')\n",
    "# os.chdir(r\"/content/drive/My Drive/Colab Notebooks/Datacastle/code\")\n",
    "print(multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(DATA_PATH):\n",
    "    train_label = pd.read_csv(DATA_PATH+'train_label.csv')\n",
    "    train_base = pd.read_csv(DATA_PATH+'train_base.csv')\n",
    "    test_base = pd.read_csv(DATA_PATH+'test_a_base.csv')\n",
    "\n",
    "    train_op = pd.read_csv(DATA_PATH+'train_op.csv')\n",
    "    train_trans = pd.read_csv(DATA_PATH+'train_trans.csv')\n",
    "    test_op = pd.read_csv(DATA_PATH+'test_a_op.csv')\n",
    "    test_trans = pd.read_csv(DATA_PATH+'test_a_trans.csv')\n",
    "\n",
    "    return train_label, train_base, test_base, train_op, train_trans, test_op, test_trans\n",
    "\n",
    "\n",
    "def transform_time(x):\n",
    "    day = int(x.split(' ')[0])\n",
    "    hour = int(x.split(' ')[2].split('.')[0].split(':')[0])\n",
    "    minute = int(x.split(' ')[2].split('.')[0].split(':')[1])\n",
    "    second = int(x.split(' ')[2].split('.')[0].split(':')[2])\n",
    "    return 86400*day+3600*hour+60*minute+second\n",
    "\n",
    "\n",
    "def data_preprocess(DATA_PATH):\n",
    "    train_label, train_base, test_base, train_op, train_trans, test_op, test_trans = load_dataset(DATA_PATH=DATA_PATH)\n",
    "    # 拼接数据\n",
    "    train_df = train_base.copy()\n",
    "    test_df = test_base.copy()\n",
    "    train_df = train_label.merge(train_df, on=['user'], how='left')\n",
    "    del train_base, test_base\n",
    "\n",
    "    op_df = pd.concat([train_op, test_op], axis=0, ignore_index=True)\n",
    "    trans_df = pd.concat([train_trans, test_trans], axis=0, ignore_index=True)\n",
    "    data = pd.concat([train_df, test_df], axis=0, ignore_index=True)\n",
    "    del train_op, test_op, train_df, test_df\n",
    "    # 时间维度的处理\n",
    "    op_df['days_diff'] = op_df['tm_diff'].apply(lambda x: int(x.split(' ')[0]))\n",
    "    trans_df['days_diff'] = trans_df['tm_diff'].apply(lambda x: int(x.split(' ')[0]))\n",
    "    op_df['timestamp'] = op_df['tm_diff'].apply(lambda x: transform_time(x))\n",
    "    trans_df['timestamp'] = trans_df['tm_diff'].apply(lambda x: transform_time(x))\n",
    "    op_df['hour'] = op_df['tm_diff'].apply(lambda x: int(x.split(' ')[2].split('.')[0].split(':')[0]))\n",
    "    trans_df['hour'] = trans_df['tm_diff'].apply(lambda x: int(x.split(' ')[2].split('.')[0].split(':')[0]))\n",
    "    trans_df['week'] = trans_df['days_diff'].apply(lambda x: x % 7)\n",
    "    # 排序\n",
    "    trans_df = trans_df.sort_values(by=['user', 'timestamp'])\n",
    "    op_df = op_df.sort_values(by=['user', 'timestamp'])\n",
    "    trans_df.reset_index(inplace=True, drop=True)\n",
    "    op_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    gc.collect()\n",
    "    return data, op_df, trans_df\n",
    "\n",
    "\n",
    "#用户单特征的统计特征\n",
    "def gen_user_single_features(df,col):\n",
    "    group_df = df.groupby(['user']).agg(\n",
    "         mean=(col,'mean'),\n",
    "         std=(col,'std'),\n",
    "         max=(col,'max'),\n",
    "         min=(col,'min'),\n",
    "         sum=(col,'sum'),\n",
    "         median=(col,'median'),\n",
    "         count=(col,'count'),\n",
    "         skew=(col,'skew'),\n",
    "         var=(col,'var'),\n",
    "        ).reset_index().rename(columns={\"mean\":'user_{}_mean'.format(col),\n",
    "                        \"std\":'user_{}_std'.format(col),\n",
    "                        \"max\":'user_{}_max'.format(col),\n",
    "                        \"min\":'user_{}_min'.format(window),\n",
    "                        \"sum\":'user_{}_sum'.format(col),\n",
    "                        \"median\":'user_{}_median'.format(col),\n",
    "                        \"count\":'user_{}_count'.format(col),\n",
    "                        \"skew\":'user_{}_skew'.format(col),\n",
    "                        \"var\":'user_{}_var'.format(col),\n",
    "                        })\n",
    "    return group_df\n",
    "\n",
    "\n",
    "#trans#用户交易消费的特征\n",
    "def gen_user_amount_features(df):\n",
    "    group_df = df.groupby(['user']).agg(\n",
    "         user_amount_mean=('amount','mean'),\n",
    "         user_amount_std=('amount','std'),\n",
    "         user_amount_max=('amount','max'),\n",
    "         user_amount_min=('amount','min'),\n",
    "         user_amount_sum=('amount','sum'),\n",
    "         user_amount_med=('amount','median'),\n",
    "         user_amount_cnt=('amount','count'),\n",
    "         user_amount_skew=('amount','skew'),\n",
    "         user_amount_var=('amount','var'),\n",
    "        ).reset_index()\n",
    "    return group_df\n",
    "\n",
    "\n",
    "#trans 用户在某平台或者用某ip的消费额特征\n",
    "def gen_user_group_amount_features(df, value):\n",
    "    group_df = df.pivot_table(index='user',\n",
    "                              columns=value,\n",
    "                              values='amount',\n",
    "                              dropna=False,\n",
    "                              aggfunc=['count','sum','median',\"skew\",'var','mean','std','max','min']).fillna(0)\n",
    "    group_df.columns = ['user_{}_{}_amount_{}'.format(value, f[1], f[0]) for f in group_df.columns]\n",
    "    group_df.reset_index(inplace=True)\n",
    "\n",
    "    return group_df\n",
    "\n",
    "\n",
    "#用户在某个时间段内的消费额特征 大于天数的窗口\n",
    "def gen_user_window_amount_features(df, window):\n",
    "    group_df = df[df['days_diff']>window].groupby('user').agg(\n",
    "        mean=('amount','mean'),\n",
    "        std=('amount','std'),\n",
    "        max=('amount','max'),\n",
    "        min=('amount','min'),\n",
    "        median=('amount','median'),\n",
    "        count=('amount','count'),\n",
    "        skew=('amount','skew'),\n",
    "        var=('amount','var'),\n",
    "        ).reset_index().rename(columns={\"mean\":'user_amount_mean_{}d'.format(window),\n",
    "                        \"std\":'user_amount_std_{}d'.format(window),\n",
    "                        \"max\":'user_amount_max_{}d'.format(window),\n",
    "                        \"min\":'user_amount_min_{}d'.format(window),\n",
    "                        \"sum\":'user_amount_sum_{}d'.format(window),\n",
    "                        \"median\":'user_amount_median_{}d'.format(window),\n",
    "                        \"count\":'user_amount_count_{}d'.format(window),\n",
    "                        \"skew\":'user_amount_skew_{}d'.format(window),\n",
    "                        \"var\":'user_amount_var_{}d'.format(window),\n",
    "                        })\n",
    "    return group_df\n",
    "\n",
    "\n",
    "#用户在某个时间段内的消费额特征 大于小时的窗口\n",
    "def gen_user_hourwindow_amount_features(df, window):\n",
    "    group_df = df[df['hour']>=window].groupby('user').agg(\n",
    "        mean=('amount','mean'),\n",
    "        count=('amount','count'),\n",
    "        ).reset_index().rename(columns={\"mean\":'user_amount_mean_{}h'.format(window),\n",
    "                        \"count\":'user_amount_count_{}h'.format(window),\n",
    "                        })\n",
    "    return group_df\n",
    "\n",
    "\n",
    "#trans用户  ['days_diff', 'platform', 'tunnel_in', 'tunnel_out', 'type1', 'type2', 'ip', 'ip_3']交易用到各字段的类数\n",
    "def gen_user_nunique_features(df, value, prefix):\n",
    "    group_df = df.groupby(['user'])[value].agg(\n",
    "        ['nunique','count']\n",
    "    ).reset_index().rename(columns={\"nunique\":'user_{}_{}_nuniq'.format(prefix, value),\n",
    "                                   'count':'user_{}_{}_cnt'.format(prefix, value)})\n",
    "    return group_df\n",
    "\n",
    "\n",
    "#trans用户无IP 这个特征贼奇怪 怎么会交易没有ip呢\n",
    "def gen_user_null_features(df, value, prefix):\n",
    "    df['is_null'] = 0\n",
    "    df.loc[df[value].isnull(), 'is_null'] = 1\n",
    "\n",
    "    group_df = df.groupby(['user'])['is_null'].agg(sum='sum',\n",
    "                            mean='mean').reset_index().rename(columns={\"sum\":'user_{}_{}_null_cnt'.format(prefix, value),\n",
    "                                                        \"mean\":'user_{}_{}_null_ratio'.format(prefix, value)})\n",
    "    return group_df\n",
    "\n",
    "\n",
    "# op op_mode op_tyep op_device net_type channel\n",
    "def gen_user_tfidf_features(df, value):\n",
    "    #填充缺失值\n",
    "    df[value].replace(' ', np.nan, inplace=True)\n",
    "    df[value] = df[value].astype(str)\n",
    "    df[value].fillna('-1', inplace=True)\n",
    "\n",
    "    #\n",
    "    group_df = df.groupby(['user']).apply(lambda x: x[value].tolist()).reset_index()\n",
    "    group_df.columns = ['user', 'list']\n",
    "    group_df['list'] = group_df['list'].apply(lambda x: ','.join(x))\n",
    "    enc_vec = TfidfVectorizer()\n",
    "    tfidf_vec = enc_vec.fit_transform(group_df['list'])\n",
    "    svd_enc = TruncatedSVD(n_components=10, n_iter=20, random_state=2020)\n",
    "    vec_svd = svd_enc.fit_transform(tfidf_vec)\n",
    "    vec_svd = pd.DataFrame(vec_svd)\n",
    "    vec_svd.columns = ['svd_tfidf_{}_{}'.format(value, i) for i in range(10)]\n",
    "    group_df = pd.concat([group_df, vec_svd], axis=1)\n",
    "    del group_df['list']\n",
    "    return group_df\n",
    "\n",
    "\n",
    "# op op_mode op_tyep op_device net_type channel\n",
    "def gen_user_countvec_features(df, value):\n",
    "    #填充缺失值\n",
    "    df[value].replace(' ', np.nan, inplace=True)\n",
    "    df[value] = df[value].astype(str)\n",
    "    df[value].fillna('-1', inplace=True)\n",
    "    #\n",
    "    group_df = df.groupby(['user']).apply(lambda x: x[value].tolist()).reset_index()\n",
    "    group_df.columns = ['user', 'list']\n",
    "    group_df['list'] = group_df['list'].apply(lambda x: ','.join(x))\n",
    "    enc_vec = CountVectorizer()\n",
    "    tfidf_vec = enc_vec.fit_transform(group_df['list'])\n",
    "    svd_enc = TruncatedSVD(n_components=10, n_iter=20, random_state=2020)\n",
    "    vec_svd = svd_enc.fit_transform(tfidf_vec)\n",
    "    vec_svd = pd.DataFrame(vec_svd)\n",
    "    vec_svd.columns = ['svd_countvec_{}_{}'.format(value, i) for i in range(10)]\n",
    "    group_df = pd.concat([group_df, vec_svd], axis=1)\n",
    "    del group_df['list']\n",
    "    return group_df\n",
    "\n",
    "\n",
    "#target_encode_cols = ['province', 'city', 'city_level', 'city_balance_avg']\n",
    "#target_encode\n",
    "def kfold_stats_feature(train, test, feats, k):\n",
    "    folds = StratifiedKFold(n_splits=k, shuffle=True, random_state=2020)  # 这里最好和后面模型的K折交叉验证保持一致\n",
    "\n",
    "    train['fold'] = None\n",
    "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(train, train['label'])):\n",
    "        train.loc[val_idx, 'fold'] = fold_\n",
    "\n",
    "    kfold_features = []\n",
    "    for feat in feats:\n",
    "        nums_columns = ['label']\n",
    "        #只有一个还用for?\n",
    "        for f in nums_columns:\n",
    "            colname = feat + '_' + f + '_kfold_mean'\n",
    "            kfold_features.append(colname)\n",
    "            train[colname] = None\n",
    "            for fold_, (trn_idx, val_idx) in enumerate(folds.split(train, train['label'])):\n",
    "                tmp_trn = train.iloc[trn_idx]\n",
    "                order_label = tmp_trn.groupby([feat])[f].mean()\n",
    "\n",
    "                tmp = train.loc[train.fold == fold_, [feat]]\n",
    "                train.loc[train.fold == fold_, colname] = tmp[feat].map(order_label)#其实就是用kfold方式对该特征的label标签mean化，然后建个新列\n",
    "                \n",
    "                # fillna 平均值填充\n",
    "                global_mean = train[f].mean()\n",
    "                train.loc[train.fold == fold_, colname] = train.loc[train.fold == fold_, colname].fillna(global_mean)\n",
    "            train[colname] = train[colname].astype(float)\n",
    "\n",
    "        for f in nums_columns:\n",
    "            colname = feat + '_' + f + '_kfold_mean'\n",
    "            test[colname] = None\n",
    "            order_label = train.groupby([feat])[f].mean()\n",
    "            test[colname] = test[feat].map(order_label)\n",
    "            # fillna\n",
    "            global_mean = train[f].mean()\n",
    "            test[colname] = test[colname].fillna(global_mean)\n",
    "            test[colname] = test[colname].astype(float)\n",
    "    del train['fold']\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def w2v_feat(df, feat, mode):\n",
    "    data_frame=df.copy()\n",
    "    # 转化为str\n",
    "    for i in feat:\n",
    "        if data_frame[i].dtype != 'object':\n",
    "            data_frame[i] = data_frame[i].astype(str)\n",
    "    data_frame.fillna('nan', inplace=True) # 甜宠\n",
    "\n",
    "    print(f'Start {mode} word2vec ...')\n",
    "    model = Word2Vec(data_frame[feat].values.tolist(), size=10, window=2, min_count=1,\n",
    "                     workers=multiprocessing.cpu_count(), iter=10)\n",
    "    stat_list = ['min', 'max', 'mean', 'std']\n",
    "    new_all = pd.DataFrame()\n",
    "    for m, t in enumerate(feat):\n",
    "        print(f'Start gen feat of {t} ...')\n",
    "        tmp = []\n",
    "        for i in data_frame[t].unique():\n",
    "            tmp_v = [i]\n",
    "            tmp_v.extend(model[i])\n",
    "            tmp.append(tmp_v)\n",
    "        tmp_df = pd.DataFrame(tmp)\n",
    "        w2c_list = [f'w2c_{t}_{n}' for n in range(10)]\n",
    "        tmp_df.columns = [t] + w2c_list\n",
    "        tmp_df = data_frame[['user', t]].merge(tmp_df, on=t)\n",
    "        tmp_df = tmp_df.drop_duplicates().groupby('user').agg(stat_list).reset_index()\n",
    "        tmp_df.columns = ['user'] + [f'{p}_{q}' for p in w2c_list for q in stat_list]\n",
    "        if m == 0:\n",
    "            new_all = pd.concat([new_all, tmp_df], axis=1)\n",
    "        else:\n",
    "            new_all = pd.merge(new_all, tmp_df, how='left', on='user')\n",
    "    return new_all\n",
    "\n",
    "\n",
    "def fasttext(df, feat, mode):\n",
    "    data_frame=df.copy()\n",
    "    # 转化为str\n",
    "    for i in feat:\n",
    "        if data_frame[i].dtype != 'object':\n",
    "            data_frame[i] = data_frame[i].astype(str)\n",
    "    data_frame.fillna('nan', inplace=True) # 甜宠\n",
    "\n",
    "    print(f'Start {mode} FastText ...')\n",
    "    model = FastText(data_frame[feat].values.tolist(),  size=10, window=3, min_count=1, \n",
    "                     workers=multiprocessing.cpu_count(), iter=10,min_n = 3 , max_n = 6,word_ngrams = 0)\n",
    "    stat_list = ['min', 'max', 'mean', 'std']\n",
    "    new_all = pd.DataFrame()\n",
    "    for m, t in enumerate(feat):\n",
    "        print(f'Start gen feat of {t} ...')\n",
    "        tmp = []\n",
    "        for i in data_frame[t].unique():\n",
    "            tmp_v = [i]\n",
    "            tmp_v.extend(model[i])\n",
    "            tmp.append(tmp_v)\n",
    "        tmp_df = pd.DataFrame(tmp)\n",
    "        w2c_list = [f'fast_text_{t}_{n}' for n in range(10)]\n",
    "        tmp_df.columns = [t] + w2c_list\n",
    "        tmp_df = data_frame[['user', t]].merge(tmp_df, on=t)\n",
    "        tmp_df = tmp_df.drop_duplicates().groupby('user').agg(stat_list).reset_index()\n",
    "        tmp_df.columns = ['user'] + [f'{p}_{q}' for p in w2c_list for q in stat_list]\n",
    "        if m == 0:\n",
    "            new_all = pd.concat([new_all, tmp_df], axis=1)\n",
    "        else:\n",
    "            new_all = pd.merge(new_all, tmp_df, how='left', on='user')\n",
    "    return new_all\n",
    "\n",
    "\n",
    "import itertools\n",
    "def gen_features(df, op, trans):\n",
    "\n",
    "    # base\n",
    "    print(\"base\")\n",
    "    df['product7_fail_ratio'] = df['product7_fail_cnt'] / df['product7_cnt']\n",
    "    df['city_count'] = df.groupby(['city'])['user'].transform('count')\n",
    "    df['province_count'] = df.groupby(['province'])['user'].transform('count')\n",
    "    df['op_cnt_avg'] = (df[\"op1_cnt\"]+df[\"op2_cnt\"])/df[\"using_time\"]\n",
    "    df['ip_cnt_avg'] = df[\"ip_cnt\"]/df[\"using_time\"]\n",
    "    df['card_a_cnt_avg'] = df[\"card_a_cnt\"]/df[\"using_time\"]\n",
    "    df['card_b_cnt_avg'] = df[\"card_b_cnt\"]/df[\"using_time\"]\n",
    "    df['card_c_cnt_avg'] = df[\"card_c_cnt\"]/df[\"using_time\"]\n",
    "    df['card_d_cnt_avg'] = df[\"card_d_cnt\"]/df[\"using_time\"]\n",
    "    df['login_cnt_didive'] = df[\"login_cnt_period1\"]/df[\"login_cnt_period2\"]\n",
    "    df['ip_per_day_cnt'] = df[\"ip_cnt\"]/df[\"login_days_cnt\"]\n",
    "    df['acc_count_per_time'] = df[\"acc_count\"]/df[\"using_time\"]\n",
    "    df['agreement_total_per_using_time'] = df[\"agreement_total\"]/df[\"using_time\"]\n",
    "    df['login_cnt'] = df[\"using_time\"]*df[\"login_cnt_avg\"]\n",
    "    df['agreement_total_per_account_cnt'] = df[\"agreement_total\"]/df[\"acc_count\"]\n",
    "    df['ip_cnt_per_acc_count'] = df[\"ip_cnt\"]/df[\"acc_count\"]\n",
    "    df['service1_amt_per_using_time'] = df[\"service1_amt\"]/df[\"using_time\"]\n",
    "    df[\"service_cnt_per_time\"]=(df[\"service1_cnt\"]+df[\"service1_cnt\"])/df[\"using_time\"]\n",
    "    \n",
    "#     df['login_cnt_period_var']=df[['login_cnt_period1','login_cnt_period2']].std(axis=1)\n",
    "#     df['login_cnt_period_CV']=df['login_cnt_period_var'] / df[['login_cnt_period1','login_cnt_period2']].mean(axis=1)\n",
    "#     df['card_var']=df[['card_a_cnt','card_b_cnt','card_c_cnt','card_d_cnt']].std(axis=1)\n",
    "#     df['card_CV']=df['card_var'] / df[['card_a_cnt','card_b_cnt','card_c_cnt','card_d_cnt']].mean(axis=1)\n",
    "#     df['op_cnt_var']=df[['op1_cnt','op2_cnt']].std(axis=1)\n",
    "#     df['op_cnt_CV']=df['op_cnt_var'] / df[['op1_cnt','op2_cnt']].mean(axis=1)\n",
    "#     df['service_var']=df[['service1_cnt','service2_cnt']].std(axis=1)\n",
    "#     df['service_CV']=df['service_var'] / df[['service1_cnt','service2_cnt']].mean(axis=1)\n",
    "#     df['login_var']=df[['login_cnt_period1','login_cnt_period2','login_cnt_avg']].std(axis=1)\n",
    "#     df['login_CV']=df['login_var'] / df[['login_cnt_period1','login_cnt_period2','login_cnt_avg']].mean(axis=1)\n",
    "    \n",
    "\n",
    "    # trans\n",
    "    df = df.merge(gen_user_amount_features(trans), on=['user'], how='left')\n",
    "    for col in tqdm(['days_diff','platform', 'tunnel_in', 'tunnel_out', 'type1', 'type2', 'ip', 'ip_3']):\n",
    "        df = df.merge(gen_user_nunique_features(df=trans, value=col, prefix='trans'), on=['user'], how='left')\n",
    "    df['user_amount_per_days'] = df['user_amount_sum'] / df['user_trans_days_diff_nuniq']\n",
    "    df['user_amount_per_cnt'] = df['user_amount_sum'] / df['user_amount_cnt']\n",
    "    print(\"trans group_amount_features\") \n",
    "    df = df.merge(gen_user_group_amount_features(df=trans, value='platform'), on=['user'], how='left')\n",
    "    df = df.merge(gen_user_group_amount_features(df=trans, value='type1'), on=['user'], how='left')\n",
    "    df = df.merge(gen_user_group_amount_features(df=trans, value='type2'), on=['user'], how='left')\n",
    "    df = df.merge(gen_user_group_amount_features(df=trans, value='tunnel_in'), on=['user'], how='left')\n",
    "    df = df.merge(gen_user_group_amount_features(df=trans, value='tunnel_out'), on=['user'], how='left')\n",
    "    print(\"trans window\") \n",
    "    df = df.merge(gen_user_window_amount_features(df=trans, window=27), on=['user'], how='left')\n",
    "    df = df.merge(gen_user_window_amount_features(df=trans, window=23), on=['user'], how='left')\n",
    "    df = df.merge(gen_user_window_amount_features(df=trans, window=15), on=['user'], how='left')\n",
    "    df = df.merge(gen_user_hourwindow_amount_features(df=trans, window=20), on=['user'], how='left')\n",
    "    df = df.merge(gen_user_hourwindow_amount_features(df=trans, window=15), on=['user'], how='left')\n",
    "    df = df.merge(gen_user_hourwindow_amount_features(df=trans, window=10), on=['user'], how='left')\n",
    "    print(\"trans null_features\")\n",
    "    df = df.merge(gen_user_null_features(df=trans, value='type2', prefix='trans'), on=['user'], how='left')\n",
    "    df = df.merge(gen_user_null_features(df=trans, value='ip_3', prefix='trans'), on=['user'], how='left')\n",
    "    df = df.merge(gen_user_null_features(df=trans, value='tunnel_in', prefix='trans'), on=['user'], how='left')\n",
    "    df = df.merge(gen_user_null_features(df=trans, value='tunnel_out', prefix='trans'), on=['user'], how='left')\n",
    "\n",
    "    print(\"trans type\")\n",
    "    group_df = trans[trans['type1']=='45a1168437c708ff'].groupby(['user']).agg(user_type1_45a1_min_day=('days_diff','min')).reset_index()\n",
    "    df = df.merge(group_df, on=['user'], how='left')\n",
    "    group_df = trans[trans['type1']=='45a1168437c708ff'].groupby(['user']).agg(user_type1_45a1_skew_day=('days_diff','skew')).reset_index()\n",
    "    df = df.merge(group_df, on=['user'], how='left')\n",
    "\n",
    "    group_df = trans[trans['type1']=='fc9b75cf62ba8b8f'].groupby(['user']).agg(user_type1_fc9b_skew_day=('days_diff','skew')).reset_index()\n",
    "    df = df.merge(group_df, on=['user'], how='left')\n",
    "    group_df = trans[trans['type1']=='fc9b75cf62ba8b8f'].groupby(['user']).agg(user_type1_fc9b_mean_day=('days_diff','mean')).reset_index()\n",
    "    df = df.merge(group_df, on=['user'], how='left')\n",
    "\n",
    "    group_df = trans[trans['type2']=='2ee592ab06090eb5'].groupby(['user']).agg(user_type1_2ee5_skew_day=('days_diff','skew')).reset_index()\n",
    "    df = df.merge(group_df, on=['user'], how='left')\n",
    "    group_df = trans[trans['type2']=='2ee592ab06090eb5'].groupby(['user']).agg(user_type1_2ee5_mean_day=('days_diff','mean')).reset_index()\n",
    "    df = df.merge(group_df, on=['user'], how='left')\n",
    "\n",
    "    group_df = trans[trans['type2']=='2bf61669e40ef6b8'].groupby(['user']).agg(user_type1_2bf6_skew_day=('days_diff','skew')).reset_index()\n",
    "    df = df.merge(group_df, on=['user'], how='left')\n",
    "    group_df = trans[trans['type2']=='2bf61669e40ef6b8'].groupby(['user']).agg(user_type1_2bf6_mean_day=('days_diff','mean')).reset_index()\n",
    "    df = df.merge(group_df, on=['user'], how='left')\n",
    "\n",
    "    print(\"op\")\n",
    "    # op\n",
    "    print(\"op null features\")\n",
    "    df = df.merge(gen_user_null_features(df=op, value=\"net_type\", prefix='op'), on=['user'], how='left')\n",
    "    df = df.merge(gen_user_null_features(df=op, value=\"op_device\", prefix='op'), on=['user'], how='left')\n",
    "    df = df.merge(gen_user_null_features(df=op, value=\"ip\", prefix='op'), on=['user'], how='left') \n",
    "\n",
    "    df=df.merge(op.groupby(\"user\").agg(user_op_days_diff_mean=('days_diff','mean')),on=['user'], how='left')\n",
    "    df=df.merge(op.groupby(\"user\").agg(user_op_hour_mean=('hour','mean')),on=['user'], how='left')\n",
    "\n",
    "    print(\"op nunique features\")\n",
    "    for col in tqdm(['days_diff','hour', 'op_mode', 'op_type', 'op_device', 'channel', 'ip',\"ip_3\"]):\n",
    "        df = df.merge(gen_user_nunique_features(df=op, value=col, prefix='op'), on=['user'], how='left')\n",
    "\n",
    "    \n",
    "    df = df.merge(gen_user_tfidf_features(df=trans, value='amount'), on=['user'], how='left')\n",
    "    df = df.merge(gen_user_countvec_features(df=trans, value='amount'), on=['user'], how='left')\n",
    "\n",
    "\n",
    "    print(\"op w2v tfidf countvec encoder features\")\n",
    "    df = df.merge(gen_user_tfidf_features(df=op, value='op_mode'), on=['user'], how='left')\n",
    "    df = df.merge(gen_user_tfidf_features(df=op, value='op_type'), on=['user'], how='left')\n",
    "\n",
    "    df = df.merge(gen_user_countvec_features(df=op, value='op_mode'), on=['user'], how='left')\n",
    "    df = df.merge(gen_user_countvec_features(df=op, value='op_type'), on=['user'], how='left')\n",
    "\n",
    "    df=df.merge(fasttext(op,['op_mode','op_type',\"op_device\",\"channel\",\"net_type\",\"ip\",\"ip_3\"],\"train\"),on=['user'],how='left')\n",
    "    \n",
    "    # LabelEncoder\n",
    "    print(\"LabelEncoder\")\n",
    "    cat_cols = []\n",
    "    for col in tqdm([f for f in df.select_dtypes('object').columns if f not in ['user']]):\n",
    "        le = LabelEncoder()\n",
    "        df[col].fillna('-1', inplace=True)\n",
    "        df[col] = le.fit_transform(df[col])\n",
    "        cat_cols.append(col)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def lgb_model(train, target, test, k):\n",
    "    feats = [f for f in train.columns if f not in ['user', 'label']]\n",
    "    print('Current num of features:', len(feats))\n",
    "    folds = StratifiedKFold(n_splits=k, shuffle=True, random_state=2020)\n",
    "    oof_probs = np.zeros(train.shape[0])\n",
    "    output_preds = 0\n",
    "    offline_score = []\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    parameters = {\n",
    "        'learning_rate': 0.03,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'num_leaves': 50,\n",
    "        'feature_fraction': 0.8,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'min_data_in_leaf': 20,\n",
    "        'reg_alpha':10,\n",
    "        'reg_lambda':8,\n",
    "        'verbose': -1,\n",
    "        'nthread': 8,\n",
    "        'colsample_bytree':0.77,\n",
    "        'min_child_weight':4,\n",
    "        'min_child_samples':10,\n",
    "        'min_split_gain':0,\n",
    "        'lambda_l1': 0.8,\n",
    "    }\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(folds.split(train, target)):\n",
    "        train_y, test_y = target[train_index], target[test_index]\n",
    "        train_X, test_X = train[feats].iloc[train_index, :], train[feats].iloc[test_index, :]\n",
    "\n",
    "        dtrain = lgb.Dataset(train_X,\n",
    "                             label=train_y)\n",
    "        dval = lgb.Dataset(test_X,\n",
    "                           label=test_y)\n",
    "        lgb_model = lgb.train(\n",
    "                parameters,\n",
    "                dtrain,\n",
    "                num_boost_round=5000,\n",
    "                valid_sets=[dval],\n",
    "                early_stopping_rounds=100,\n",
    "                verbose_eval=100,\n",
    "        )\n",
    "        oof_probs[test_index] = lgb_model.predict(test_X[feats], num_iteration=lgb_model.best_iteration)\n",
    "        offline_score.append(lgb_model.best_score['valid_0']['auc'])\n",
    "        output_preds += lgb_model.predict(test[feats], num_iteration=lgb_model.best_iteration)/folds.n_splits\n",
    "        print(offline_score)\n",
    "        # feature importance\n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df[\"feature\"] = feats\n",
    "        fold_importance_df[\"importance\"] = lgb_model.feature_importance(importance_type='gain')\n",
    "        fold_importance_df[\"fold\"] = i + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    print('OOF-MEAN-AUC:%.6f, OOF-STD-AUC:%.6f' % (np.mean(offline_score), np.std(offline_score)))\n",
    "    print('feature importance:')\n",
    "    print(feature_importance_df.groupby(['feature'])['importance'].mean().sort_values(ascending=False).head(50))\n",
    "    feature_importance_df.to_csv(\"feature_importance.csv\")\n",
    "    return output_preds, oof_probs, np.mean(offline_score),feature_importance_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "DATA_PATH = 'dataset/'\n",
    "print('读取数据...')\n",
    "data, op_df, trans_df = data_preprocess(DATA_PATH=DATA_PATH)\n",
    "\n",
    "print('开始特征工程...')\n",
    "data = gen_features(data, op_df, trans_df)\n",
    "data['city_level'] = data['city'].map(str) + '_' + data['level'].map(str)\n",
    "data['city_product1_amount'] = data['city'].map(str) + '_' + data['product1_amount'].map(str)\n",
    "data['city_balance1_avg'] = data['city'].map(str) + '_' + data['balance1_avg'].map(str)\n",
    "data['city_balance2_avg'] = data['city'].map(str) + '_' + data['balance2_avg'].map(str)\n",
    "data['city_balance'] = data['city'].map(str) + '_' + data['balance'].map(str)\n",
    "data['city_card_a_cnt'] = data['city'].map(str) + '_' + data['card_a_cnt'].map(str)\n",
    "\n",
    "\n",
    "train = data[~data['label'].isnull()].copy()\n",
    "target = train['label']\n",
    "test = data[data['label'].isnull()].copy()\n",
    "\n",
    "print(\"target encoder...\")\n",
    "  \n",
    "target_encode_cols = ['province','city',\"city_level\",'city_balance1_avg',\n",
    "                      'city_balance2_avg',\"city_product1_amount\",'city_balance','city_card_a_cnt']\n",
    "\n",
    "train, test = kfold_stats_feature(train, test, target_encode_cols, 10)\n",
    "train.drop(['province', \"city\",\"service3_level\",\"city_level\",'city_balance1_avg','city_balance2_avg',\n",
    "              \"city_product1_amount\",'city_balance','city_card_a_cnt'], axis=1, inplace=True)\n",
    "test.drop(['province',\"city\",\"service3_level\",\"city_level\",'city_balance1_avg','city_balance2_avg',\n",
    "              \"city_product1_amount\",'city_balance','city_card_a_cnt'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('train.csv',index=False)\n",
    "target.to_csv('target.csv',index=False)\n",
    "test.to_csv('test.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print('开始模型训练...')\n",
    "# param = {\n",
    "#         'learning_rate': 0.05,\n",
    "#         'boosting_type': 'gbdt',\n",
    "#         'objective': 'binary',\n",
    "#         'metric': 'auc',\n",
    "#         'num_leaves': 50,\n",
    "#         'feature_fraction': 0.8,\n",
    "#         'bagging_fraction': 0.8,\n",
    "#         'min_data_in_leaf': 20,\n",
    "#         'reg_alpha':10,\n",
    "#         'reg_lambda':8,\n",
    "#         'verbose': -1,\n",
    "#         'nthread': 8,\n",
    "#         'colsample_bytree':0.77,\n",
    "#         'min_child_weight':4,\n",
    "#         'min_child_samples':10,\n",
    "#         'min_split_gain':0,\n",
    "#         'lambda_l1': 0.8,\n",
    "#     }\n",
    "lgb_preds, lgb_oof, lgb_score,feature_importance_df = lgb_model(train=train, target=target, test=test,k=10)\n",
    "auc_score = roc_auc_score(target.values, lgb_oof)\n",
    "print(\"train auc:\",auc_score)\n",
    "\n",
    "sub_df = test[['user']].copy()\n",
    "sub_df['prob'] = lgb_preds\n",
    "sub_df.to_csv('sub_lgb{%.5f}.csv'%(lgb_score,), index=False)\n",
    "# 跑了3次分数都不同\n",
    "# 0.739275\n",
    "# 0.739369\n",
    "# 0.739213\n",
    "\n",
    "# 0.737\n",
    "# 0.738936\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py37]",
   "language": "python",
   "name": "conda-env-py37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
