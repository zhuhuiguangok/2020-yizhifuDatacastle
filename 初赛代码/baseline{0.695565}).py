# -*- coding: utf-8 -*-
"""baseline.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1K7deG-ubhYyN9os9zHpX-sd6aajPTs1t
"""

# coding: utf-8
import pandas as pd
import numpy as np
import warnings
from sklearn.preprocessing import LabelEncoder
import lightgbm as lgb
from sklearn.model_selection import StratifiedKFold
from sklearn.decomposition import TruncatedSVD
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from tqdm import tqdm
import gc
import os
warnings.filterwarnings('ignore')
os.chdir(r"/content/drive/My Drive/Colab Notebooks/Datacastle/code")

def load_dataset(DATA_PATH):
    train_label = pd.read_csv(DATA_PATH+'train_label.csv')
    train_base = pd.read_csv(DATA_PATH+'train_base.csv')
    test_base = pd.read_csv(DATA_PATH+'test_a_base.csv')

    train_op = pd.read_csv(DATA_PATH+'train_op.csv')
    train_trans = pd.read_csv(DATA_PATH+'train_trans.csv')
    test_op = pd.read_csv(DATA_PATH+'test_a_op.csv')
    test_trans = pd.read_csv(DATA_PATH+'test_a_trans.csv')

    return train_label, train_base, test_base, train_op, train_trans, test_op, test_trans

def transform_time(x):
    day = int(x.split(' ')[0])
    hour = int(x.split(' ')[2].split('.')[0].split(':')[0])
    minute = int(x.split(' ')[2].split('.')[0].split(':')[1])
    second = int(x.split(' ')[2].split('.')[0].split(':')[2])
    return 86400*day+3600*hour+60*minute+second

def data_preprocess(DATA_PATH):
    train_label, train_base, test_base, train_op, train_trans, test_op, test_trans = load_dataset(DATA_PATH=DATA_PATH)
    # 拼接数据
    train_df = train_base.copy()
    test_df = test_base.copy()
    train_df = train_label.merge(train_df, on=['user'], how='left')
    del train_base, test_base

    op_df = pd.concat([train_op, test_op], axis=0, ignore_index=True)
    trans_df = pd.concat([train_trans, test_trans], axis=0, ignore_index=True)
    data = pd.concat([train_df, test_df], axis=0, ignore_index=True)
    del train_op, test_op, train_df, test_df
    # 时间维度的处理
    op_df['days_diff'] = op_df['tm_diff'].apply(lambda x: int(x.split(' ')[0]))
    trans_df['days_diff'] = trans_df['tm_diff'].apply(lambda x: int(x.split(' ')[0]))
    op_df['timestamp'] = op_df['tm_diff'].apply(lambda x: transform_time(x))
    trans_df['timestamp'] = trans_df['tm_diff'].apply(lambda x: transform_time(x))
    op_df['hour'] = op_df['tm_diff'].apply(lambda x: int(x.split(' ')[2].split('.')[0].split(':')[0]))
    trans_df['hour'] = trans_df['tm_diff'].apply(lambda x: int(x.split(' ')[2].split('.')[0].split(':')[0]))
    trans_df['week'] = trans_df['days_diff'].apply(lambda x: x % 7)
    # 排序
    trans_df = trans_df.sort_values(by=['user', 'timestamp'])
    op_df = op_df.sort_values(by=['user', 'timestamp'])
    trans_df.reset_index(inplace=True, drop=True)
    op_df.reset_index(inplace=True, drop=True)

    gc.collect()
    return data, op_df, trans_df

#trans#用户交易消费的特征
def gen_user_amount_features(df):
    group_df = df.groupby(['user']).agg(
         user_amount_mean=('amount','mean'),
         user_amount_std=('amount','std'),
         user_amount_max=('amount','max'),
         user_amount_min=('amount','min'),
         user_amount_sum=('amount','sum'),
         user_amount_med=('amount','median'),
         user_amount_cnt=('amount','count'),
         user_amount_skew=('amount','skew'),
         user_amount_var=('amount','var'),
        ).reset_index()
    return group_df

#trans 用户在某平台或者用某ip的消费额特征
def gen_user_group_amount_features(df, value):
    group_df = df.pivot_table(index='user',
                              columns=value,
                              values='amount',
                              dropna=False,
                              aggfunc=['count','sum','median',"skew",'var','mean','std','max','min']).fillna(0)
    group_df.columns = ['user_{}_{}_amount_{}'.format(value, f[1], f[0]) for f in group_df.columns]
    group_df.reset_index(inplace=True)

    return group_df

#用户在某个时间段内的消费额特征 大于天数的窗口
def gen_user_window_amount_features(df, window):
    group_df = df[df['days_diff']>window].groupby('user').agg(
        mean=('amount','mean'),
        std=('amount','std'),
        max=('amount','max'),
        min=('amount','min'),
        median=('amount','median'),
        count=('amount','count'),
        skew=('amount','skew'),
        var=('amount','var'),
        ).reset_index().rename(columns={"mean":'user_amount_mean_{}d'.format(window),
                        "std":'user_amount_std_{}d'.format(window),
                        "max":'user_amount_max_{}d'.format(window),
                        "min":'user_amount_min_{}d'.format(window),
                        "sum":'user_amount_sum_{}d'.format(window),
                        "median":'user_amount_median_{}d'.format(window),
                        "count":'user_amount_count_{}d'.format(window),
                        "skew":'user_amount_skew_{}d'.format(window),
                        "var":'user_amount_var_{}d'.format(window),
                        })
    return group_df

#用户在某个时间段内的消费额特征 大于小时的窗口
def gen_user_hourwindow_amount_features(df, window):
    group_df = df[df['hour']>window].groupby('user').agg(
        mean=('amount','mean'),
        std=('amount','std'),
        max=('amount','max'),
        min=('amount','min'),
        median=('amount','median'),
        count=('amount','count'),
        skew=('amount','skew'),
        var=('amount','var'),
        ).reset_index().rename(columns={"mean":'user_amount_mean_{}h'.format(window),
                        "std":'user_amount_std_{}h'.format(window),
                        "max":'user_amount_max_{}h'.format(window),
                        "min":'user_amount_min_{}h'.format(window),
                        "sum":'user_amount_sum_{}h'.format(window),
                        "median":'user_amount_median_{}h'.format(window),
                        "count":'user_amount_count_{}h'.format(window),
                        "skew":'user_amount_skew_{}h'.format(window),
                        "var":'user_amount_var_{}h'.format(window),
                        })
    return group_df

#trans用户  ['days_diff', 'platform', 'tunnel_in', 'tunnel_out', 'type1', 'type2', 'ip', 'ip_3']交易用到各字段的类数
def gen_user_nunique_features(df, value, prefix):
    group_df = df.groupby(['user']).agg(
        nunique=(value,'nunique')
    ).reset_index().rename(columns={"nunique":'user_{}_{}_nuniq'.format(prefix, value)})
    return group_df

#trans用户无IP 这个特征贼奇怪 怎么会交易没有ip呢
def gen_user_null_features(df, value, prefix):
    df['is_null'] = 0
    df.loc[df[value].isnull(), 'is_null'] = 1

    group_df = df.groupby(['user'])['is_null'].agg(sum=('is_null','sum'),
                            mean=('is_null','mean')).reset_index().rename(columns={"sum":'user_{}_{}_null_cnt'.format(prefix, value),
                                                        "mean":'user_{}_{}_null_ratio'.format(prefix, value)})
    return group_df

# op op_mode op_tyep op_device net_type channel
def gen_user_tfidf_features(df, value):
    #填充缺失值
    df[value].replace(' ', np.nan, inplace=True)
    df[value] = df[value].astype(str)
    df[value].fillna('-1', inplace=True)

    #
    group_df = df.groupby(['user']).apply(lambda x: x[value].tolist()).reset_index()
    group_df.columns = ['user', 'list']
    group_df['list'] = group_df['list'].apply(lambda x: ','.join(x))
    enc_vec = TfidfVectorizer()
    tfidf_vec = enc_vec.fit_transform(group_df['list'])
    svd_enc = TruncatedSVD(n_components=10, n_iter=20, random_state=2020)
    vec_svd = svd_enc.fit_transform(tfidf_vec)
    vec_svd = pd.DataFrame(vec_svd)
    vec_svd.columns = ['svd_tfidf_{}_{}'.format(value, i) for i in range(10)]
    group_df = pd.concat([group_df, vec_svd], axis=1)
    del group_df['list']
    return group_df

# op op_mode op_tyep op_device net_type channel
def gen_user_countvec_features(df, value):
    #填充缺失值
    df[value].replace(' ', np.nan, inplace=True)
    df[value] = df[value].astype(str)
    df[value].fillna('-1', inplace=True)
    #
    group_df = df.groupby(['user']).apply(lambda x: x[value].tolist()).reset_index()
    group_df.columns = ['user', 'list']
    group_df['list'] = group_df['list'].apply(lambda x: ','.join(x))
    enc_vec = CountVectorizer()
    tfidf_vec = enc_vec.fit_transform(group_df['list'])
    svd_enc = TruncatedSVD(n_components=10, n_iter=20, random_state=2020)
    vec_svd = svd_enc.fit_transform(tfidf_vec)
    vec_svd = pd.DataFrame(vec_svd)
    vec_svd.columns = ['svd_countvec_{}_{}'.format(value, i) for i in range(10)]
    group_df = pd.concat([group_df, vec_svd], axis=1)
    del group_df['list']
    return group_df

#target_encode_cols = ['province', 'city', 'city_level', 'city_balance_avg']
#target_encode
def kfold_stats_feature(train, test, feats, k):
    folds = StratifiedKFold(n_splits=k, shuffle=True, random_state=2020)  # 这里最好和后面模型的K折交叉验证保持一致

    train['fold'] = None
    for fold_, (trn_idx, val_idx) in enumerate(folds.split(train, train['label'])):
        train.loc[val_idx, 'fold'] = fold_

    kfold_features = []
    for feat in feats:
        nums_columns = ['label']
        #只有一个还用for?
        for f in nums_columns:
            colname = feat + '_' + f + '_kfold_mean'
            kfold_features.append(colname)
            train[colname] = None
            for fold_, (trn_idx, val_idx) in enumerate(folds.split(train, train['label'])):
                tmp_trn = train.iloc[trn_idx]
                order_label = tmp_trn.groupby([feat])[f].mean()

                tmp = train.loc[train.fold == fold_, [feat]]
                train.loc[train.fold == fold_, colname] = tmp[feat].map(order_label)
                
                # fillna
                global_mean = train[f].mean()
                train.loc[train.fold == fold_, colname] = train.loc[train.fold == fold_, colname].fillna(global_mean)
            train[colname] = train[colname].astype(float)

        for f in nums_columns:
            colname = feat + '_' + f + '_kfold_mean'
            test[colname] = None
            order_label = train.groupby([feat])[f].mean()
            test[colname] = test[feat].map(order_label)
            # fillna
            global_mean = train[f].mean()
            test[colname] = test[colname].fillna(global_mean)
            test[colname] = test[colname].astype(float)
    del train['fold']
    return train, test

def gen_features(df, op, trans):
    #df.drop(['service3_level'], axis=1, inplace=True)

    # base
    df['product7_fail_ratio'] = df['product7_fail_cnt'] / df['product7_cnt']
    df['city_count'] = df.groupby(['city'])['user'].transform('count')
    df['province_count'] = df.groupby(['province'])['user'].transform('count')
    df['op_cnt_avg'] = (df["op1_cnt"]+df["op2_cnt"])/df["using_time"]
    df['ip_cnt_avg'] = df["ip_cnt"]/df["using_time"]
    # trans
    df = df.merge(gen_user_amount_features(trans), on=['user'], how='left')
    for col in tqdm(['days_diff','platform', 'tunnel_in', 'tunnel_out', 'type1', 'type2', 'ip', 'ip_3']):
        df = df.merge(gen_user_nunique_features(df=trans, value=col, prefix='trans'), on=['user'], how='left')
    df['user_amount_per_days'] = df['user_amount_sum'] / df['user_trans_days_diff_nuniq']
    df['user_amount_per_cnt'] = df['user_amount_sum'] / df['user_amount_cnt']
    print("trans group_amount_features") 
    df = df.merge(gen_user_group_amount_features(df=trans, value='platform'), on=['user'], how='left')
    df = df.merge(gen_user_group_amount_features(df=trans, value='type1'), on=['user'], how='left')
    df = df.merge(gen_user_group_amount_features(df=trans, value='type2'), on=['user'], how='left')
    df = df.merge(gen_user_group_amount_features(df=trans, value='tunnel_in'), on=['user'], how='left')
    df = df.merge(gen_user_group_amount_features(df=trans, value='tunnel_out'), on=['user'], how='left')
    print("trans window") 
    df = df.merge(gen_user_window_amount_features(df=trans, window=27), on=['user'], how='left')
    df = df.merge(gen_user_window_amount_features(df=trans, window=23), on=['user'], how='left')
    df = df.merge(gen_user_window_amount_features(df=trans, window=15), on=['user'], how='left')
    #df = df.merge(gen_user_hourwindow_amount_features(df=trans, window=10), on=['user'], how='left')
    print("trans null_features")
    df = df.merge(gen_user_null_features(df=trans, value='ip', prefix='trans'), on=['user'], how='left')
    df = df.merge(gen_user_null_features(df=trans, value='ip_3', prefix='trans'), on=['user'], how='left')
    df = df.merge(gen_user_null_features(df=trans, value='tunnel_in', prefix='trans'), on=['user'], how='left')
    df = df.merge(gen_user_null_features(df=trans, value='tunnel_out', prefix='trans'), on=['user'], how='left')
    print("trans type")
    group_df = trans[trans['type1']=='45a1168437c708ff'].groupby(['user']).agg(user_type1_45a1_min_day=('days_diff','min')).reset_index()
    df = df.merge(group_df, on=['user'], how='left')
    group_df = trans[trans['type1']=='45a1168437c708ff'].groupby(['user']).agg(user_type1_45a1_skew_day=('days_diff','skew')).reset_index()
    df = df.merge(group_df, on=['user'], how='left')

    group_df = trans[trans['type1']=='fc9b75cf62ba8b8f'].groupby(['user']).agg(user_type1_fc9b_skew_day=('days_diff','skew')).reset_index()
    df = df.merge(group_df, on=['user'], how='left')
    group_df = trans[trans['type1']=='fc9b75cf62ba8b8f'].groupby(['user']).agg(user_type1_fc9b_mean_day=('days_diff','mean')).reset_index()
    df = df.merge(group_df, on=['user'], how='left')

    group_df = trans[trans['type2']=='2ee592ab06090eb5'].groupby(['user']).agg(user_type1_2ee5_skew_day=('days_diff','skew')).reset_index()
    df = df.merge(group_df, on=['user'], how='left')
    group_df = trans[trans['type2']=='2ee592ab06090eb5'].groupby(['user']).agg(user_type1_2ee5_mean_day=('days_diff','mean')).reset_index()
    df = df.merge(group_df, on=['user'], how='left')

    group_df = trans[trans['type2']=='2bf61669e40ef6b8'].groupby(['user']).agg(user_type1_2bf6_skew_day=('days_diff','skew')).reset_index()
    df = df.merge(group_df, on=['user'], how='left')
    group_df = trans[trans['type2']=='2bf61669e40ef6b8'].groupby(['user']).agg(user_type1_2bf6_mean_day=('days_diff','mean')).reset_index()
    df = df.merge(group_df, on=['user'], how='left')

    print("op")
    # op
    df = df.merge(gen_user_null_features(df=op, value="net_type", prefix='op'), on=['user'], how='left')
    df=df.merge(op.groupby("user").agg(user_op_days_diff_mean=('days_diff','mean')),on=['user'], how='left')
    df=df.merge(op.groupby("user").agg(user_op_hour_mean=('hour','mean')),on=['user'], how='left')

    df = df.merge(gen_user_tfidf_features(df=op, value='op_mode'), on=['user'], how='left')
    df = df.merge(gen_user_tfidf_features(df=op, value='op_type'), on=['user'], how='left')

    df = df.merge(gen_user_countvec_features(df=op, value='op_mode'), on=['user'], how='left')
    df = df.merge(gen_user_countvec_features(df=op, value='op_type'), on=['user'], how='left')



    # LabelEncoder
    print("LabelEncoder")
    cat_cols = []
    for col in tqdm([f for f in df.select_dtypes('object').columns if f not in ['user']]):
        le = LabelEncoder()
        df[col].fillna('-1', inplace=True)
        df[col] = le.fit_transform(df[col])
        cat_cols.append(col)

    return df

def lgb_model(train, target, test, k):
    feats = [f for f in train.columns if f not in ['user', 'label']]
    print('Current num of features:', len(feats))
    folds = StratifiedKFold(n_splits=k, shuffle=True, random_state=2020)
    oof_probs = np.zeros(train.shape[0])
    output_preds = 0
    offline_score = []
    feature_importance_df = pd.DataFrame()
    parameters = {
        'learning_rate': 0.05,
        'boosting_type': 'gbdt',
        'objective': 'binary',
        'metric': 'auc',
        'num_leaves': 63,
        'feature_fraction': 0.8,
        'bagging_fraction': 0.8,
        'min_data_in_leaf': 20,
        'verbose': -1,
        'nthread': 8,
    }

    for i, (train_index, test_index) in enumerate(folds.split(train, target)):
        train_y, test_y = target[train_index], target[test_index]
        train_X, test_X = train[feats].iloc[train_index, :], train[feats].iloc[test_index, :]

        dtrain = lgb.Dataset(train_X,
                             label=train_y)
        dval = lgb.Dataset(test_X,
                           label=test_y)
        lgb_model = lgb.train(
                parameters,
                dtrain,
                num_boost_round=5000,
                valid_sets=[dval],
                early_stopping_rounds=100,
                verbose_eval=100,
        )
        oof_probs[test_index] = lgb_model.predict(test_X[feats], num_iteration=lgb_model.best_iteration)
        offline_score.append(lgb_model.best_score['valid_0']['auc'])
        output_preds += lgb_model.predict(test[feats], num_iteration=lgb_model.best_iteration)/folds.n_splits
        print(offline_score)
        # feature importance
        fold_importance_df = pd.DataFrame()
        fold_importance_df["feature"] = feats
        fold_importance_df["importance"] = lgb_model.feature_importance(importance_type='gain')
        fold_importance_df["fold"] = i + 1
        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)
    print('OOF-MEAN-AUC:%.6f, OOF-STD-AUC:%.6f' % (np.mean(offline_score), np.std(offline_score)))
    print('feature importance:')
    print(feature_importance_df.groupby(['feature'])['importance'].mean().sort_values(ascending=False).head(50))
    feature_importance_df.to_csv("../data/feature_importance.csv")
    return output_preds, oof_probs, np.mean(offline_score),feature_importance_df

DATA_PATH = '../data/'
print('读取数据...')
data, op_df, trans_df = data_preprocess(DATA_PATH=DATA_PATH)

print('开始特征工程...')
data = gen_features(data, op_df, trans_df)
data['city_level'] = data['city'].map(str) + '_' + data['level'].map(str)
data['city_product1_amount'] = data['city'].map(str) + '_' + data['product1_amount'].map(str)
data['city_balance1_avg'] = data['city'].map(str) + '_' + data['balance1_avg'].map(str)
data['city_balance2_avg'] = data['city'].map(str) + '_' + data['balance2_avg'].map(str)
data['city_balance'] = data['city'].map(str) + '_' + data['balance'].map(str)

print('开始模型训练...')
train = data[~data['label'].isnull()].copy()
target = train['label']
test = data[data['label'].isnull()].copy()

target_encode_cols = ['province','city',"city_level",'city_balance1_avg','city_balance2_avg',
                      "city_product1_amount",'city_balance']
train, test = kfold_stats_feature(train, test, target_encode_cols, 5)
train.drop(['province', "service3_level",'city',"city_level",'city_balance1_avg','city_balance2_avg',
                      "city_product1_amount",'city_balance'], axis=1, inplace=True)
test.drop(['province', "service3_level",'city',"city_level",'city_balance1_avg','city_balance2_avg',
                      "city_product1_amount",'city_balance'], axis=1, inplace=True)

lgb_preds, lgb_oof, lgb_score,feature_importance_df = lgb_model(train=train, target=target, test=test, k=5)
sub_df = test[['user']].copy()
sub_df['prob'] = lgb_preds
sub_df.to_csv('../submission/sub{%.5f}.csv'%(lgb_score,), index=False)

feature_list=feature_importance_df[feature_importance_df["importance"]>0]
feature_list=feature_list[feature_list["fold"]==1].sort_values(by="importance",ascending=False)
print("feature_list length",len(feature_list))
print(feature_list["feature"].tolist())
print(len(test))

features_col=feature_list["feature"].tolist()
x=train[feature_list["feature"].tolist()].values
y=target.values
t=test[feature_list["feature"].tolist()].values
print("y",len(y),"x",len(x),"t",len(t))
y[5:20]

# selection features
print("features selection")
from sklearn.ensemble import RandomForestClassifier
clf =RandomForestClassifier(n_estimators=300,n_jobs=-1)
selector = RFECV(estimator=clf, step=1, cv=StratifiedKFold(3), scoring='roc_auc')
selector.fit(x, y)
 
print(' Optimal number of features: %d' % selector.n_features_)
sel_features = [f for f, s in zip(features_col, selector.support_) if s]
sel_features

import tensorflow as tf
import numpy as np
from tensorflow.keras.layers import Conv1D,Dense,Dropout,Input,MaxPooling1D,LayerNormalization,BatchNormalization,LSTM
def Linear_Lstm(input_shape, num_classes):
  inputs=Input(shape=(input_shape[1],))
  x=Dense(512, activation='relu')(inputs)
  x=LayerNormalization()(x) 
  #x=Dropout(0.3)(x)

  x=Dense(512, activation='relu')(x)
  x=LayerNormalization()(x)
  x=Dropout(0.3)(x)
  x= tf.concat([x, inputs],axis=1)
  x=Dense(256, activation='relu')(x)
  x=LayerNormalization()(x)
  #x=Dropout(0.3)(x)

  x=Dense(128)(x)
  x=LayerNormalization()(x)
  #x=Dropout(0.3)(x)

  outputs=Dense(num_classes, activation='softmax')(x)

  return tf.keras.models.Model(inputs=[inputs],outputs=[outputs])

test_sample=np.zeros((24315,60))
test_model=Linear_Lstm(test_sample.shape,2)
model_test=test_model(test_sample)
model_test.shape





import tensorflow as tf
from sklearn.metrics import roc_auc_score
def auc(y_true, y_pred):
  y_true=tf.argmax(y_true,axis=1)
  y_pred=tf.argmax(y_pred,axis=1)
  return tf.py_function(roc_auc_score, (y_true, y_pred), tf.double)

#?tf.metrics.AUC

from sklearn.model_selection import StratifiedKFold

kfold = StratifiedKFold(10, shuffle=True)
proba_t = np.zeros((t.shape[0], 2))
val_loss = []
val_acc = []
for fold, (xx, yy) in enumerate(kfold.split(x, y)):
    y_ = tf.keras.utils.to_categorical(y, num_classes=2)
    model=Linear_Lstm(x.shape,2)
    model.compile(loss=tf.losses.CategoricalCrossentropy(label_smoothing=0.1),
                  optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
                  metrics=['acc',auc])
    plateau = tf.keras.callbacks.ReduceLROnPlateau(monitor="val_acc",
                                verbose=1,
                                mode='max',
                                factor=0.1,
                                patience=3)
    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_acc',
                                   verbose=1,
                                   mode='max',
                                   patience=12)
    checkpoint = tf.keras.callbacks.ModelCheckpoint(f'../submission/fold{fold}.h5',
                                 monitor='val_acc',
                                 verbose=1,
                                 mode='max',
                                 save_best_only=True)
    model.summary()
    print("fold",fold)
    hist = model.fit(
              x[xx],y_[xx],
              batch_size=256,
              epochs=150,
              verbose=1,
              shuffle=True,
              validation_data=(x[yy], y_[yy]),
              callbacks=[plateau, early_stopping, checkpoint])
     
    val_loss.append(np.min(hist.history['val_loss']))
    val_acc.append(np.max(hist.history['val_acc']))
    model.load_weights(f'../submission/fold{fold}.h5')
    proba_t+= model.predict(t, verbose=1, batch_size=1024) / 10.

txt_acc=np.mean(val_acc)
sub_keras_df = test[['user']].copy()
sub_keras_df['prob'] = np.argmax(proba_t,axis=1)
sub_keras_df.to_csv('../submission/sub_keras{%.5f}.csv'%(txt_acc,), index=False)

