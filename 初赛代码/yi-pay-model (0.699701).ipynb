{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2020-08-19T15:22:48.850324Z",
     "iopub.status.busy": "2020-08-19T15:22:48.849322Z",
     "iopub.status.idle": "2020-08-19T15:22:51.519915Z",
     "shell.execute_reply": "2020-08-19T15:22:51.518773Z"
    },
    "papermill": {
     "duration": 2.698172,
     "end_time": "2020-08-19T15:22:51.520132",
     "exception": false,
     "start_time": "2020-08-19T15:22:48.821960",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "/kaggle/input/financial-risk/test_a_trans.csv\n",
      "/kaggle/input/financial-risk/test_a_base.csv\n",
      "/kaggle/input/financial-risk/feature_importance_df 32_after.csv\n",
      "/kaggle/input/financial-risk/submit_example.csv\n",
      "/kaggle/input/financial-risk/train_label.csv\n",
      "/kaggle/input/financial-risk/feature_importance_df(32ming) (1).csv\n",
      "/kaggle/input/financial-risk/train_trans.csv\n",
      "/kaggle/input/financial-risk/train_base.csv\n",
      "/kaggle/input/financial-risk/.doc\n",
      "/kaggle/input/financial-risk/all_precess_data.csv\n",
      "/kaggle/input/financial-risk/test_a_op.csv\n",
      "/kaggle/input/financial-risk/train_op.csv\n",
      "/kaggle/input/financial-risk/test_a_trans.csv\n",
      "/kaggle/input/financial-risk/test_a_base.csv\n",
      "/kaggle/input/financial-risk/feature_importance_df 32_after.csv\n",
      "/kaggle/input/financial-risk/submit_example.csv\n",
      "/kaggle/input/financial-risk/train_label.csv\n",
      "/kaggle/input/financial-risk/feature_importance_df(32ming) (1).csv\n",
      "/kaggle/input/financial-risk/train_trans.csv\n",
      "/kaggle/input/financial-risk/train_base.csv\n",
      "/kaggle/input/financial-risk/.doc\n",
      "/kaggle/input/financial-risk/all_precess_data.csv\n",
      "/kaggle/input/financial-risk/test_a_op.csv\n",
      "/kaggle/input/financial-risk/train_op.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.decomposition import TruncatedSVD,PCA\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn import preprocessing\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "warnings.filterwarnings('ignore')\n",
    "print(multiprocessing.cpu_count())\n",
    "import gc\n",
    "from category_encoders import *\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "## You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "origin_features = []\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2020-08-19T15:22:51.613294Z",
     "iopub.status.busy": "2020-08-19T15:22:51.573431Z",
     "iopub.status.idle": "2020-08-19T15:22:51.643024Z",
     "shell.execute_reply": "2020-08-19T15:22:51.642159Z"
    },
    "papermill": {
     "duration": 0.108709,
     "end_time": "2020-08-19T15:22:51.643229",
     "exception": false,
     "start_time": "2020-08-19T15:22:51.534520",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14 µs, sys: 3 µs, total: 17 µs\n",
      "Wall time: 25.7 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def gen_user_status_features(df,value):\n",
    "    group_df = df.groupby(['user'])[value].agg(\n",
    "        a='mean',\n",
    "        b= 'std',\n",
    "        c='max',\n",
    "        d='min',\n",
    "        e='sum',\n",
    "        f='median',\n",
    "        g='count',\n",
    "        h='median',\n",
    "        i='skew',\n",
    "        ).reset_index()\n",
    "    group_df['j']=group_df['b'] / group_df['a']\n",
    "    group_df.rename(columns={\n",
    "                    'a':'user_{}_mea'.format(value),\n",
    "                    'b':'user_{}_std'.format(value),\n",
    "                    'c':'user_{}_max'.format(value),\n",
    "                    'd':'user_{}_min'.format(value),\n",
    "                    'e':'user_{}_sum'.format(value),\n",
    "                    'f':'user_{}_med'.format(value),\n",
    "                    'g':'user_{}_cnt'.format(value),\n",
    "                    'h':'user_{}_median'.format(value),\n",
    "                    'i':'user_{}_skew'.format(value),\n",
    "                    'j':'user_{}_CV'.format(value)\n",
    "                    },\n",
    "                   inplace=True\n",
    "                   )\n",
    "    return group_df\n",
    "def gen_city_status_features(df,value):\n",
    "    group_df = df.groupby(['city'])[value].agg(\n",
    "        a='mean',\n",
    "        b= 'std',\n",
    "        c='max',\n",
    "        d='min',\n",
    "        e='sum',\n",
    "        f='median',\n",
    "        g='count',\n",
    "        h='median',\n",
    "        i='skew',\n",
    "        ).reset_index()\n",
    "    group_df['j']=group_df['b'] / group_df['a']\n",
    "    group_df.rename(columns={\n",
    "                    'a':'city_{}_mea'.format(value),\n",
    "                    'b':'city_{}_std'.format(value),\n",
    "                    'c':'city_{}_max'.format(value),\n",
    "                    'd':'city_{}_min'.format(value),\n",
    "                    'e':'city_{}_sum'.format(value),\n",
    "                    'f':'city_{}_med'.format(value),\n",
    "                    'g':'city_{}_cnt'.format(value),\n",
    "                    'h':'city_{}_median'.format(value),\n",
    "                    'i':'city_{}_skew'.format(value),\n",
    "                    'j':'city_{}_CV'.format(value)\n",
    "                    },\n",
    "                   inplace=True\n",
    "                   )\n",
    "    return group_df\n",
    "    \n",
    "def gen_user_group_features(df, col,value):\n",
    "    \n",
    "    group_df = df.pivot_table(index='user',\n",
    "                              columns=col,\n",
    "                              values=value,\n",
    "                              dropna=False,\n",
    "                              aggfunc=['count', 'sum','nunique','mean','std','max','min']).fillna(0)\n",
    "    cols=group_df.columns\n",
    "    group_df.columns = ['user_group_{}_{}_{}_{}'.format(col,value, f[1], f[0]) for f in group_df.columns]\n",
    "    for f in cols:\n",
    "        group_df['user_group_{}_{}_{}_CV'.format(col,value,f[1])] = group_df['user_group_{}_{}_{}_std'.format(col,value,f[1])] / group_df['user_group_{}_{}_{}_mean'.format(col,value,f[1])]\n",
    "    group_df.reset_index(inplace=True)\n",
    "    return group_df\n",
    "\n",
    "def gen_user_group_nunique_features(df, col,value):\n",
    "    \n",
    "    group_df = df.pivot_table(index='user',\n",
    "                              columns=col,\n",
    "                              values=value,\n",
    "                              dropna=False,\n",
    "                              aggfunc=['count','nunique']).fillna(0)\n",
    "    group_df.columns = ['user_{}_{}_{}_{}'.format(col,value, f[1], f[0]) for f in group_df.columns]\n",
    "    group_df.reset_index(inplace=True)\n",
    "    return group_df\n",
    "\n",
    "def gen_user_nunique_features(df, value, prefix):\n",
    "    a=\"user_{}_{}_nuniq\".format(prefix, value)\n",
    "    b=\"user_{}_{}_count\".format(prefix, value)\n",
    "    group_df = df.groupby(['user'])[value].agg(\n",
    "        a='nunique',b='count'\n",
    "    ).reset_index()\n",
    "    \n",
    "    group_df.rename(columns={'a':a,'b':b},inplace=True) \n",
    "    return group_df\n",
    "\n",
    "def gen_user_null_features(df, value, prefix):\n",
    "    df['is_null'] = 0\n",
    "    df.loc[df[value].isnull(), 'is_null'] = 1\n",
    "\n",
    "    group_df = df.groupby(['user'])['is_null'].agg(a='sum',\n",
    "                                                    b='mean').reset_index()\n",
    "    \n",
    "    group_df.rename(columns={'a':'user_{}_{}_null_cnt'.format(prefix, value),\n",
    "                    'b':'user_{}_{}_null_ratio'.format(prefix, value)},\n",
    "                   inplace=True\n",
    "                   )\n",
    "\n",
    "    return group_df\n",
    "\n",
    "    \n",
    "def gen_user_window_amount_features(df, window):\n",
    "    \n",
    "    group_df = df[df['days_diff']>window].groupby('user')['amount'].agg(\n",
    "        a='mean',\n",
    "         b='std',\n",
    "         c='max',\n",
    "         d='min',\n",
    "         e='sum',\n",
    "        f='median',\n",
    "         g='count',\n",
    "        ).reset_index()\n",
    "    \n",
    "    group_df.rename(columns={'a':'user_amount_mean_{}d'.format(window),\n",
    "                    'b':'user_amount_std_{}d'.format(window),\n",
    "                    'c':'user_amount_max_{}d'.format(window),\n",
    "                    'd':'user_amount_min_{}d'.format(window),\n",
    "                    'e':'user_amount_sum_{}d'.format(window),\n",
    "                    'f':'user_amount_med_{}d'.format(window),\n",
    "                    'g':'user_amount_cnt_{}d'.format(window)},\n",
    "                   inplace=True\n",
    "                   )\n",
    "    \n",
    "    return group_df\n",
    "\n",
    "def gen_user_window_hour_amount_features(df, window):\n",
    "    \n",
    "    group_df = df[df['hour']>window].groupby('user')['amount'].agg(\n",
    "        a='mean',\n",
    "         b='std',\n",
    "         c='max',\n",
    "         d='min',\n",
    "         e='sum',\n",
    "         f='median',\n",
    "         g='count',\n",
    "        ).reset_index()\n",
    "    \n",
    "    group_df.rename(columns={'a':'user_amount_mean_{}h'.format(window),\n",
    "                    'b':'user_amount_std_{}h'.format(window),\n",
    "                    'c':'user_amount_max_{}h'.format(window),\n",
    "                    'd':'user_amount_min_{}h'.format(window),\n",
    "                    'e':'user_amount_sum_{}h'.format(window),\n",
    "                    'f':'user_amount_med_{}h'.format(window),\n",
    "                    'g':'user_amount_cnt_{}h'.format(window)},\n",
    "                   inplace=True\n",
    "                   )\n",
    "    \n",
    "    return group_df\n",
    "\n",
    "\n",
    "def gen_user_tfidf_features(df, value):\n",
    "    df[value] = df[value].astype(str)\n",
    "    df[value].fillna('-1', inplace=True)\n",
    "    group_df = df.groupby(['user']).apply(lambda x: x[value].tolist()).reset_index()\n",
    "    group_df.columns = ['user', 'list']\n",
    "    group_df['list'] = group_df['list'].apply(lambda x: ','.join(x))\n",
    "    enc_vec = TfidfVectorizer()\n",
    "    tfidf_vec = enc_vec.fit_transform(group_df['list'])\n",
    "    svd_enc = TruncatedSVD(n_components=5, n_iter=20, random_state=2020)\n",
    "    vec_svd = svd_enc.fit_transform(tfidf_vec)\n",
    "    vec_svd = pd.DataFrame(vec_svd)\n",
    "    vec_svd.columns = ['svd_tfidf_{}_{}'.format(value, i) for i in range(5)]\n",
    "    group_df = pd.concat([group_df, vec_svd], axis=1)\n",
    "    del group_df['list']\n",
    "    return group_df\n",
    "\n",
    "def gen_user_countvec_features(df, value):\n",
    "    df[value] = df[value].astype(str)\n",
    "    df[value].fillna('-1', inplace=True)\n",
    "    group_df = df.groupby(['user']).apply(lambda x: x[value].tolist()).reset_index()\n",
    "    group_df.columns = ['user', 'list']\n",
    "    group_df['list'] = group_df['list'].apply(lambda x: ','.join(x))\n",
    "    enc_vec = CountVectorizer()\n",
    "    tfidf_vec = enc_vec.fit_transform(group_df['list'])\n",
    "    svd_enc = TruncatedSVD(n_components=5, n_iter=20, random_state=2020)\n",
    "    vec_svd = svd_enc.fit_transform(tfidf_vec)\n",
    "    vec_svd = pd.DataFrame(vec_svd)\n",
    "    vec_svd.columns = ['svd_countvec_{}_{}'.format(value, i) for i in range(5)]\n",
    "    group_df = pd.concat([group_df, vec_svd], axis=1)\n",
    "    del group_df['list']\n",
    "    return group_df\n",
    "\n",
    "#定义加载函数\n",
    "def load_dataset(DATA_PATH):\n",
    "    train_label = pd.read_csv(DATA_PATH+'train_label.csv')\n",
    "    test_base = pd.read_csv(DATA_PATH+'test_a_base.csv')\n",
    "    \n",
    "    train_base = pd.read_csv(DATA_PATH+'train_base.csv')\n",
    "    train_op = pd.read_csv(DATA_PATH+'train_op.csv')\n",
    "    train_trans = pd.read_csv(DATA_PATH+'train_trans.csv')\n",
    "    test_op = pd.read_csv(DATA_PATH+'test_a_op.csv')\n",
    "    test_trans = pd.read_csv(DATA_PATH+'test_a_trans.csv')\n",
    "    origin_features = list(train_base.columns)+list(train_trans.columns)+list(train_op.columns)\n",
    "    return train_label, train_base, test_base, train_op, train_trans, test_op, test_trans\n",
    "\n",
    "#tran_trans和train_op文件中的tm_diff时间转换\n",
    "def transform_time(x):\n",
    "    day = int(x.split(' ')[0])\n",
    "    hour = int(x.split(' ')[2].split('.')[0].split(':')[0])\n",
    "    minute = int(x.split(' ')[2].split('.')[0].split(':')[1])\n",
    "    second = int(x.split(' ')[2].split('.')[0].split(':')[2])\n",
    "    return 86400*day+3600*hour+60*minute+second\n",
    "\n",
    "\n",
    "\n",
    "#讲所有的数据进行初步的整合，并且把时间days_diff进行了数字化提取\n",
    "def data_preprocess(DATA_PATH):\n",
    "    train_label, train_base, test_base, train_op, train_trans, test_op, test_trans = load_dataset(DATA_PATH=DATA_PATH)\n",
    "    # 拼接数据\n",
    "    train_df = train_base.copy()\n",
    "    test_df = test_base.copy()\n",
    "        #将train_base数据和train_label整合\n",
    "    train_df = train_label.merge(train_df, on=['user'], how='left')\n",
    "    del train_base, test_base\n",
    "        #将train_op数据和test_op整合--方便将train和test中所有的数据进行统一的处理\n",
    "    op_df = pd.concat([train_op, test_op], axis=0, ignore_index=True)\n",
    "        #将train_trans数据和test_trans整合--方便将train和test中所有的数据进行统一的处理\n",
    "    trans_df = pd.concat([train_trans, test_trans], axis=0, ignore_index=True)\n",
    "        #将train_base数据和test_base整合\n",
    "    data = pd.concat([train_df, test_df], axis=0, ignore_index=True) \n",
    "    del train_op, test_op, train_df, test_df\n",
    "    \n",
    "    \n",
    "    # 时间维度的处理\n",
    "    op_df['days_diff'] = op_df['tm_diff'].apply(lambda x: int(x.split(' ')[0]))\n",
    "    trans_df['days_diff'] = trans_df['tm_diff'].apply(lambda x: int(x.split(' ')[0]))\n",
    "    \n",
    "    op_df['timestamp'] = op_df['tm_diff'].apply(lambda x: transform_time(x))\n",
    "    trans_df['timestamp'] = trans_df['tm_diff'].apply(lambda x: transform_time(x))\n",
    "    op_df['hour'] = op_df['tm_diff'].apply(lambda x: int(x.split(' ')[2].split('.')[0].split(':')[0]))\n",
    "    trans_df['hour'] = trans_df['tm_diff'].apply(lambda x: int(x.split(' ')[2].split('.')[0].split(':')[0]))\n",
    "    trans_df['week'] = trans_df['days_diff'].apply(lambda x: x % 7)\n",
    "    \n",
    "    \n",
    "    # 排序\n",
    "    trans_df = trans_df.sort_values(by=['user', 'timestamp'])\n",
    "    op_df = op_df.sort_values(by=['user', 'timestamp'])\n",
    "    trans_df.reset_index(inplace=True, drop=True)\n",
    "    op_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    gc.collect()\n",
    "    return data, op_df, trans_df\n",
    "\n",
    "\n",
    "def kfold_stats_feature(train, test, feats, k):\n",
    "    folds = StratifiedKFold(n_splits=k, shuffle=True, random_state=2020)  # 这里最好和后面模型的K折交叉验证保持一致\n",
    "\n",
    "    train['fold'] = None\n",
    "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(train, train['label'])):\n",
    "        train.loc[val_idx, 'fold'] = fold_\n",
    "\n",
    "    kfold_features = []\n",
    "    for feat in feats:\n",
    "        nums_columns = ['label']\n",
    "        for f in nums_columns:\n",
    "            colname = feat + '_' + f + '_kfold_mean'\n",
    "            kfold_features.append(colname)\n",
    "            train[colname] = None\n",
    "            for fold_, (trn_idx, val_idx) in enumerate(folds.split(train, train['label'])):\n",
    "                tmp_trn = train.iloc[trn_idx]\n",
    "                order_label = tmp_trn.groupby([feat])[f].mean()\n",
    "                tmp = train.loc[train.fold == fold_, [feat]]\n",
    "                train.loc[train.fold == fold_, colname] = tmp[feat].map(order_label)\n",
    "                # fillna\n",
    "                global_mean = train[f].mean()\n",
    "                train.loc[train.fold == fold_, colname] = train.loc[train.fold == fold_, colname].fillna(global_mean)\n",
    "            train[colname] = train[colname].astype(float)\n",
    "\n",
    "        for f in nums_columns:\n",
    "            colname = feat + '_' + f + '_kfold_mean'\n",
    "            test[colname] = None\n",
    "            order_label = train.groupby([feat])[f].mean()\n",
    "            test[colname] = test[feat].map(order_label)\n",
    "            # fillna\n",
    "            global_mean = train[f].mean()\n",
    "            test[colname] = test[colname].fillna(global_mean)\n",
    "            test[colname] = test[colname].astype(float)\n",
    "    del train['fold']\n",
    "    return train, test\n",
    "def w2v_feat(df, feat, mode):\n",
    "    data_frame=df.copy()\n",
    "    for i in feat:\n",
    "        if data_frame[i].dtype != 'object':\n",
    "            data_frame[i] = data_frame[i].astype(str)\n",
    "    data_frame.fillna('nan', inplace=True)\n",
    "\n",
    "    print(f'Start {mode} word2vec ...')\n",
    "    model = Word2Vec(data_frame[feat].values.tolist(), size=10, window=2, min_count=1,\n",
    "                     workers=multiprocessing.cpu_count(), iter=10)\n",
    "    stat_list = ['min', 'max', 'mean', 'std']\n",
    "    new_all = pd.DataFrame()\n",
    "    for m, t in enumerate(feat):\n",
    "        print(f'Start gen feat of {t} ...')\n",
    "        tmp = []\n",
    "        for i in data_frame[t].unique():\n",
    "            tmp_v = [i]\n",
    "            tmp_v.extend(model[i])\n",
    "            tmp.append(tmp_v)\n",
    "        tmp_df = pd.DataFrame(tmp)\n",
    "        w2c_list = [f'w2c_{t}_{n}' for n in range(10)]\n",
    "        tmp_df.columns = [t] + w2c_list\n",
    "        tmp_df = data_frame[['user', t]].merge(tmp_df, on=t)\n",
    "        tmp_df = tmp_df.drop_duplicates().groupby('user').agg(stat_list).reset_index()\n",
    "        tmp_df.columns = ['user'] + [f'{p}_{q}' for p in w2c_list for q in stat_list]\n",
    "        if m == 0:\n",
    "            new_all = pd.concat([new_all, tmp_df], axis=1)\n",
    "        else:\n",
    "            new_all = pd.merge(new_all, tmp_df, how='left', on='user')\n",
    "    return new_all\n",
    "\n",
    "def gen_features(df, op, trans):\n",
    "    \n",
    "    # base数据处理\n",
    "    print(\"base logistic features\")\n",
    "    df['card_a_cnt_avg'] = df[\"card_a_cnt\"]/df[\"using_time\"]\n",
    "    df['product7_fail_ratio'] = df['product7_fail_cnt'] / df['product7_cnt']\n",
    "    df['city_count'] = df.groupby(['city'])['user'].transform('count')  ## 与原始数据集相同数量的项目\n",
    "    df['city_product7_fail_ratio'] =  df.groupby(['city'])['product7_fail_cnt'].transform('sum')/df.groupby(['city'])['product7_cnt'].transform('sum')\n",
    "    df['province_count'] = df.groupby(['province'])['user'].transform('count')\n",
    "    df['province_product7_fail_ratio'] = df.groupby(['province'])['product7_fail_cnt'].transform('sum')/df.groupby(['province'])['product7_cnt'].transform('sum')\n",
    "    \n",
    "    df['login_cnt_period_var']=df[['login_cnt_period1','login_cnt_period2']].std(axis=1)\n",
    "    df['login_cnt_period_CV']=df['login_cnt_period_var'] / df[['login_cnt_period1','login_cnt_period2']].mean(axis=1)\n",
    "    df['card_var']=df[['card_a_cnt','card_b_cnt','card_c_cnt','card_d_cnt']].std(axis=1)\n",
    "    df['card_CV']=df['card_var'] / df[['card_a_cnt','card_b_cnt','card_c_cnt','card_d_cnt']].mean(axis=1)\n",
    "    df['op_cnt_var']=df[['op1_cnt','op2_cnt']].std(axis=1)\n",
    "    df['op_cnt_CV']=df['op_cnt_var'] / df[['op1_cnt','op2_cnt']].mean(axis=1)\n",
    "    df['service_var']=df[['service1_cnt','service2_cnt']].std(axis=1)\n",
    "    df['service_CV']=df['service_var'] / df[['service1_cnt','service2_cnt']].mean(axis=1)\n",
    "    df['login_var']=df[['login_cnt_period1','login_cnt_period2','login_cnt_avg']].std(axis=1)\n",
    "    df['login_CV']=df['login_var'] / df[['login_cnt_period1','login_cnt_period2','login_cnt_avg']].mean(axis=1)\n",
    "    \n",
    "    \n",
    "    df['ip_cnt_per_login_cnt_avg'] = df['ip_cnt']/df[['login_cnt_period1','login_cnt_period2','login_cnt_avg']].mean(axis=1)\n",
    "    df['ip_cnt_per_login_days_cnt'] = df['ip_cnt']/df['login_days_cnt']\n",
    "    df['acc_count_per_time'] = df[\"acc_count\"]/df[\"using_time\"]\n",
    "    df['ip_cnt_per_using_time'] = df[\"ip_cnt\"]/df[\"using_time\"]\n",
    "    df['agreement_total_per_using_time'] = df[\"agreement_total\"]/df[\"using_time\"]\n",
    "    df['agreement_total_per_account_cnt'] = df[\"agreement_total\"]/df[\"acc_count\"]\n",
    "    df['ip_cnt_per_acc_count'] = df[\"ip_cnt\"]/df[\"acc_count\"]\n",
    "    df['service1_amt_per_using_time'] = df[\"service1_amt\"]/df[\"using_time\"]\n",
    "    df[\"service_cnt_per_time\"]=(df[\"service1_cnt\"]+df[\"service1_cnt\"])/df[\"using_time\"]\n",
    "    df[\"service_cnt_per_time\"]=(df[\"service1_cnt\"]+df[\"service1_cnt\"])/df[\"using_time\"]\n",
    "    \n",
    "    df['op_cnt_per_usign_time'] = (df[\"op1_cnt\"]+df[\"op2_cnt\"])/df[\"using_time\"]\n",
    "    df['op_cnt_per_login_cnt_avg'] = (df[\"op1_cnt\"]+df[\"op2_cnt\"])/df['login_cnt_avg']\n",
    "    df['op_cnt_per_login_cnt_p1'] =  (df[\"op1_cnt\"]+df[\"op2_cnt\"])/df['login_cnt_period1']\n",
    "    df['op_cnt_per_login_cnt_p2'] =  (df[\"op1_cnt\"]+df[\"op2_cnt\"])/df['login_cnt_period2']\n",
    "\n",
    "    # trans\n",
    "    df = df.merge(gen_user_status_features(trans,'amount'), on=['user'], how='left')\n",
    "    df = df.merge(gen_user_status_features(trans,'days_diff'), on=['user'], how='left')\n",
    "    df = df.merge(gen_user_status_features(trans,'hour'), on=['user'], how='left')\n",
    "    df=df.merge(df.groupby(['city'])['user_amount_sum'].agg(city_amount_mean='mean',city_amount_sum='sum',city_amount_std='std').reset_index(),\n",
    "             on=['city'],how='left')\n",
    "  \n",
    "    print(\"trans nunique && null features\")\n",
    "    for col in tqdm(['days_diff', 'platform', 'tunnel_in', 'tunnel_out', 'type1', 'type2','ip', 'ip_3']):\n",
    "        df = df.merge(gen_user_nunique_features(df=trans, value=col, prefix='trans'), on=['user'], how='left')\n",
    "        df = df.merge(gen_user_null_features(df=trans, value=col, prefix='trans'), on=['user'], how='left')\n",
    "    df['user_amount_per_days'] = df['user_amount_sum'].div(df['user_trans_days_diff_nuniq']) \n",
    "    df['user_amount_per_cnt'] = df['user_amount_sum'] / df['user_amount_cnt']\n",
    "    df['user_amount_per_tunnel_in'] = df['user_amount_sum'] / df['user_trans_tunnel_in_nuniq']\n",
    "    df['user_amount_per_cnt'] = df['user_amount_sum'] / df['user_trans_tunnel_out_nuniq']\n",
    "    df['user_amount_per_platform'] = df['user_amount_sum'] / df['user_trans_platform_nuniq']\n",
    "    df=df.merge(gen_city_status_features(df,'user_trans_ip_null_cnt'),on=['city'],how='left')\n",
    "    df=df.merge(gen_city_status_features(df,'user_trans_ip_count'),on=['city'],how='left')\n",
    "    df['city_ipcnt_vs_nullipcnt']=df['city_user_trans_ip_count_sum'] / df['city_user_trans_ip_null_cnt_sum']\n",
    "    \n",
    "    \n",
    "    temp=trans[trans['ip'].isnull()].groupby(['user'])['amount'].agg(user_null_ip_amount_sum='sum').reset_index()\n",
    "    df=df.merge(temp,on=['user'],how='left')\n",
    "    df['user_null_ip_amount_sum'].fillna(0,inplace=True)\n",
    "    df['user_null_ip_amount_percent'] = df['user_null_ip_amount_sum'] / df['user_amount_sum']\n",
    "    df['user_ip_amount_percent'] = 1-df['user_null_ip_amount_percent'] \n",
    "    df=df.merge(gen_city_status_features(df,'user_null_ip_amount_sum'),on=['city'],how='left')\n",
    "    df['city_null_ip_amount_percent'] = df['city_user_null_ip_amount_sum_sum'] / df['city_amount_sum']\n",
    "    \n",
    "    \n",
    "    print(\"trans group amount features\")\n",
    "    trans_type1=trans.query(\"type1==['f67d4b5a05a1352a','19d44f1a51919482','674e8d5860bc033d', '45a1168437c708ff']\")\n",
    "    trans_type2=trans.query(\"type2==['11a213398ee0c623']\")\n",
    "    trans_plat=trans.query(\"platform==['46c69cbbce5f1568','42573d7287a8c9c2']\")\n",
    "    trans_tunnel_in=trans.query(\"tunnel_in==['b2e7fa260df4998d','9c5701005a2d85bd']\")\n",
    "    df = df.merge(gen_user_group_features(df=trans_type2,col='type2',      value='amount'),on=['user'],how='left')\n",
    "    df = df.merge(gen_user_group_features(df=trans_type1,col='type1',      value='amount'),on=['user'],how='left')\n",
    "    df = df.merge(gen_user_group_features(df=trans_tunnel_in,col='tunnel_in',  value='amount'),on=['user'],how='left')\n",
    "    df = df.merge(gen_user_group_features(df=trans,col='tunnel_out',value='amount'), on=['user'], how='left')\n",
    "    df = df.merge(gen_user_group_features(df=trans_plat,col='platform',   value='amount'),on=['user'],how='left')\n",
    "    \n",
    "    print(\"trans group days_diff features\")\n",
    "    df = df.merge(gen_user_group_features(df=trans_type2,col='type2',      value='days_diff'),on=['user'],how='left')\n",
    "    df = df.merge(gen_user_group_features(df=trans_type1,col='type1',      value='days_diff'),on=['user'],how='left')\n",
    "    df = df.merge(gen_user_group_features(df=trans_tunnel_in,col='tunnel_in',  value='days_diff'),on=['user'],how='left')\n",
    "    df = df.merge(gen_user_group_features(df=trans,col='tunnel_out',value='days_diff'), on=['user'], how='left')\n",
    "    df = df.merge(gen_user_group_features(df=trans_plat,col='platform',   value='days_diff'),on=['user'],how='left')\n",
    "    \n",
    "    \n",
    "    print(\"trans group nunique features\")\n",
    "    df = df.merge(gen_user_group_nunique_features(df=trans,col='platform',   value='type1'),on=['user'],how='left')\n",
    "    df = df.merge(gen_user_group_nunique_features(df=trans,col='platform',   value='ip'),on=['user'],how='left')\n",
    "    \n",
    "    print(\"trans days window features\")\n",
    "    for window in tqdm([20,13,15]):\n",
    "        df = df.merge(gen_user_window_amount_features(df=trans, window=window), on=['user'], how='left')\n",
    "        \n",
    "    print(\"trans hour window features\")\n",
    "    for win_hour in tqdm([9,18,19,20]):\n",
    "        df = df.merge(gen_user_window_hour_amount_features(df=trans, window=win_hour), on=['user'], how='left')\n",
    "        \n",
    "    print(\"trans tfidf--count features\")    \n",
    "    df = df.merge(gen_user_tfidf_features(df=trans, value='type1'), on=['user'], how='left')\n",
    "    df = df.merge(gen_user_tfidf_features(df=trans, value='type2'), on=['user'], how='left')\n",
    "    \n",
    "    # op\n",
    "    print(\"op nuique features\")\n",
    "    for col in tqdm(['op_type','op_mode','channel']):\n",
    "        df = df.merge(gen_user_nunique_features(df=op, value=col, prefix='op'), on=['user'], how='left')\n",
    "    \n",
    "    print(\"op null features\")\n",
    "    for col in tqdm(['op_device', 'ip', 'net_type', 'channel', 'ip_3']):\n",
    "        df = df.merge(gen_user_null_features(df=op, value=col, prefix='op'), on=['user'], how='left')\n",
    "    \n",
    "    print(\"op days_diff features\")\n",
    "    df = df.merge(gen_user_status_features(op,'days_diff'), on=['user'], how='left')\n",
    "    df = df.merge(gen_user_status_features(op,'hour'), on=['user'], how='left')\n",
    "\n",
    "    print(\"op tfidf--cnt features\")\n",
    "    df = df.merge(gen_user_tfidf_features(df=op, value='op_mode'), on=['user'], how='left')\n",
    "    df = df.merge(gen_user_tfidf_features(df=op, value='op_type'), on=['user'], how='left')\n",
    "    df = df.merge(gen_user_tfidf_features(df=op, value='op_device'), on=['user'], how='left')\n",
    "    df = df.merge(gen_user_countvec_features(df=op, value='op_mode'), on=['user'], how='left')\n",
    "    df = df.merge(gen_user_countvec_features(df=op, value='op_type'), on=['user'], how='left')\n",
    "    df = df.merge(gen_user_countvec_features(df=op, value='op_device'), on=['user'], how='left')\n",
    "    \n",
    "    df=df.merge(w2v_feat(op,[\"op_device\",\"ip\",\"channel\",\"net_type\",\"ip_3\"],\"train\"),on=['user'],how='left')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def lgb_model_origin(train, target, test, k):\n",
    "    feats = [f for f in train.columns if f not in ['user', 'label']]\n",
    "    print('Current num of features:', len(feats))\n",
    "    folds = StratifiedKFold(n_splits=k, shuffle=True, random_state=2020)\n",
    "    oof_probs = np.zeros(train.shape[0])\n",
    "    output_preds = 0\n",
    "    offline_score = []\n",
    "    valid_score = []\n",
    "    train_score = []\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    #parameters = {\n",
    "    #    'learning_rate': 0.05,\n",
    "    #    'boosting_type': 'gbdt',\n",
    "    #    'objective': 'binary',\n",
    "    #    'metric': 'auc',\n",
    "    #    'num_leaves': 63,\n",
    "    #    'feature_fraction': 0.8,\n",
    "    #    'bagging_fraction': 0.8,\n",
    "    #    'min_data_in_leaf': 20,\n",
    "    #    'verbose': -1,\n",
    "    #    'nthread': 8\n",
    "    #}\n",
    "    parameters = {\n",
    "        'learning_rate': 0.05,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'num_leaves': 50,\n",
    "        'feature_fraction': 0.8,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'min_data_in_leaf': 20,\n",
    "        'reg_alpha':10,\n",
    "        'reg_lambda':8,\n",
    "        'verbose': -1,\n",
    "        'nthread': 8,\n",
    "        'colsample_bytree':0.77,\n",
    "        'min_child_weight':4,\n",
    "        'min_child_samples':10,\n",
    "        'min_split_gain':0,\n",
    "        'lambda_l1': 0.8,\n",
    "    }\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(folds.split(train, target)):\n",
    "        train_y, test_y = target[train_index], target[test_index]\n",
    "        train_X, test_X = train[feats].iloc[train_index, :], train[feats].iloc[test_index, :]\n",
    "        dtrain = lgb.Dataset(train_X,\n",
    "                             label=train_y)\n",
    "        dval = lgb.Dataset(test_X,\n",
    "                           label=test_y)\n",
    "        lgb_model = lgb.train(\n",
    "                parameters,\n",
    "                dtrain,\n",
    "                num_boost_round=5000,\n",
    "                valid_sets=[dval],\n",
    "                early_stopping_rounds=100,\n",
    "                verbose_eval=100,\n",
    "        )\n",
    "        oof_probs[test_index] = lgb_model.predict(test_X[feats], num_iteration=lgb_model.best_iteration)\n",
    "        output_preds += lgb_model.predict(test[feats], num_iteration=lgb_model.best_iteration)/folds.n_splits\n",
    "        \n",
    "        offline_score.append(lgb_model.best_score['valid_0']['auc'])\n",
    "        print(offline_score)\n",
    "        \n",
    "        # feature importance\n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df[\"feature\"] = feats\n",
    "        fold_importance_df[\"importance\"] = lgb_model.feature_importance(importance_type='gain')\n",
    "        fold_importance_df[\"fold\"] = i + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "\n",
    "    valid_auc = roc_auc_score(target, oof_probs)\n",
    "    print('OOF-MEAN-AUC:%.6f, OOF-STD-AUC:%.6f' % (np.mean(offline_score), np.std(offline_score)))\n",
    "    print(\"valid_auc:  {}\".format(valid_auc))\n",
    "    print('feature importance:')\n",
    "    print(feature_importance_df.groupby(['feature'])['importance'].mean().sort_values(ascending=False).head(40))\n",
    "\n",
    "    return output_preds, oof_probs, np.mean(offline_score),feature_importance_df.groupby(['feature'])['importance'].mean().sort_values(ascending=False)\n",
    "# This way we have randomness and are able to reproduce the behaviour within this cell.\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-19T15:22:51.690540Z",
     "iopub.status.busy": "2020-08-19T15:22:51.688915Z",
     "iopub.status.idle": "2020-08-19T15:22:51.694107Z",
     "shell.execute_reply": "2020-08-19T15:22:51.693187Z"
    },
    "papermill": {
     "duration": 0.03518,
     "end_time": "2020-08-19T15:22:51.694304",
     "exception": false,
     "start_time": "2020-08-19T15:22:51.659124",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def add_noise(series, noise_level):\n",
    "    return series * (1 + noise_level * np.random.randn(len(series)))\n",
    "\n",
    "def target_encoding(train,test, feature, k,sigma=1.0,a=1.0):\n",
    "    \n",
    "    kf = StratifiedKFold(n_splits=k, shuffle=True, random_state=2020)    \n",
    "    \n",
    "    print(\"Target encoding\")\n",
    "    for col in tqdm(feature):\n",
    "        #train['target_'+col] = 0\n",
    "        #test['target_' + col] =0\n",
    "        train['catboost_'+col] = 0\n",
    "        test['catboost_' + col] =0\n",
    "        if col not in origin_features:\n",
    "            #smoothing=50\n",
    "            #noise_level=0.7\n",
    "            sigma=1.5\n",
    "            a=5.0\n",
    "        for trn_idx,val_idx in kf.split(train,train['label']):\n",
    "            X = train.iloc[trn_idx][col]\n",
    "            #enc_tar = TargetEncoder(smoothing=smoothing).fit(X,train.iloc[trn_idx]['label'])\n",
    "            enc_cat = CatBoostEncoder(sigma=sigma).fit(X,train.iloc[trn_idx]['label'])\n",
    "            #train['target_'+col] += add_noise(enc_tar.transform(train[col])[col],noise_level) / k\n",
    "            #test['target_'+col] += add_noise(enc_tar.transform(test[col])[col],noise_level) / k\n",
    "            train['catboost_'+col] += enc_cat.transform(train[col])[col] / k\n",
    "            test['catboost_'+col] += enc_cat.transform(test[col])[col] / k\n",
    "    print('Target encoding finish')\n",
    "    return  train,test\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-19T15:22:51.735832Z",
     "iopub.status.busy": "2020-08-19T15:22:51.734667Z",
     "iopub.status.idle": "2020-08-19T15:23:45.982721Z",
     "shell.execute_reply": "2020-08-19T15:23:45.983560Z"
    },
    "papermill": {
     "duration": 54.275124,
     "end_time": "2020-08-19T15:23:45.983938",
     "exception": false,
     "start_time": "2020-08-19T15:22:51.708814",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "读取数据...\n",
      "CPU times: user 48.7 s, sys: 4.95 s, total: 53.7 s\n",
      "Wall time: 54.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "DATA_PATH = '/kaggle/input/financial-risk/'\n",
    "print('读取数据...')\n",
    "base_df, op_df, trans_df = data_preprocess(DATA_PATH=DATA_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-19T15:23:46.025032Z",
     "iopub.status.busy": "2020-08-19T15:23:46.024148Z",
     "iopub.status.idle": "2020-08-19T15:35:29.819983Z",
     "shell.execute_reply": "2020-08-19T15:35:29.820778Z"
    },
    "papermill": {
     "duration": 703.822693,
     "end_time": "2020-08-19T15:35:29.821306",
     "exception": false,
     "start_time": "2020-08-19T15:23:45.998613",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始特征工程...\n",
      "base logistic features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trans nunique && null features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:07<00:00,  1.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trans group amount features\n",
      "trans group days_diff features\n",
      "trans group nunique features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trans days window features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:02<00:00,  1.26it/s]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trans hour window features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:02<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trans tfidf--count features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "op nuique features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:10<00:00,  3.63s/it]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "op null features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:12<00:00,  2.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "op days_diff features\n",
      "op tfidf--cnt features\n",
      "Start train word2vec ...\n",
      "Start gen feat of op_device ...\n",
      "Start gen feat of ip ...\n",
      "Start gen feat of channel ...\n",
      "Start gen feat of net_type ...\n",
      "Start gen feat of ip_3 ...\n",
      "CPU times: user 18min 19s, sys: 20.1 s, total: 18min 39s\n",
      "Wall time: 11min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('开始特征工程...')\n",
    "data = gen_features(base_df, op_df, trans_df)\n",
    "data.to_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-19T15:35:29.891038Z",
     "iopub.status.busy": "2020-08-19T15:35:29.890234Z",
     "iopub.status.idle": "2020-08-19T15:35:29.894085Z",
     "shell.execute_reply": "2020-08-19T15:35:29.893460Z"
    },
    "papermill": {
     "duration": 0.040712,
     "end_time": "2020-08-19T15:35:29.894257",
     "exception": false,
     "start_time": "2020-08-19T15:35:29.853545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#data['city_level'] = data['city'].map(str) + '_' + data['level'].map(str)\n",
    "#data['city_product1_amount'] = data['city'].map(str) + '_' + data['product1_amount'].map(str)\n",
    "#data['city_balance1_avg'] = data['city'].map(str) + '_' + data['balance1_avg'].map(str)\n",
    "#data['city_balance2_avg'] = data['city'].map(str) + '_' + data['balance2_avg'].map(str)\n",
    "#data['city_balance_avg'] = data['city'].map(str) + '_' + data['balance_avg'].map(str)\n",
    "target_encode_cols = [\n",
    "'user_trans_ip_count',   \n",
    "'product7_fail_ratio',\n",
    "'product7_fail_cnt',\n",
    "'user_amount_med_9h',\n",
    "'service3',\n",
    "'age',\n",
    "'login_cnt_period_CV',\n",
    "'product7_cnt',\n",
    "'svd_countvec_op_type_3',\n",
    "'svd_countvec_op_mode_3',\n",
    "'svd_tfidf_op_type_3',\n",
    "'using_time'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-19T15:35:29.966667Z",
     "iopub.status.busy": "2020-08-19T15:35:29.965272Z",
     "iopub.status.idle": "2020-08-19T15:51:54.155479Z",
     "shell.execute_reply": "2020-08-19T15:51:54.153689Z"
    },
    "papermill": {
     "duration": 984.229991,
     "end_time": "2020-08-19T15:51:54.156022",
     "exception": false,
     "start_time": "2020-08-19T15:35:29.926031",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始模型训练...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target encoding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:51<00:00,  4.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target encoding finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:01<00:00, 17.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current num of features: 746\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's auc: 0.737254\n",
      "[200]\tvalid_0's auc: 0.738362\n",
      "[300]\tvalid_0's auc: 0.738113\n",
      "Early stopping, best iteration is:\n",
      "[218]\tvalid_0's auc: 0.738675\n",
      "[0.7386746182007863]\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's auc: 0.74499\n",
      "[200]\tvalid_0's auc: 0.745613\n",
      "Early stopping, best iteration is:\n",
      "[130]\tvalid_0's auc: 0.748426\n",
      "[0.7386746182007863, 0.7484257725336398]\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's auc: 0.731833\n",
      "[200]\tvalid_0's auc: 0.732699\n",
      "Early stopping, best iteration is:\n",
      "[162]\tvalid_0's auc: 0.733984\n",
      "[0.7386746182007863, 0.7484257725336398, 0.7339841593120615]\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's auc: 0.725435\n",
      "[200]\tvalid_0's auc: 0.727196\n",
      "[300]\tvalid_0's auc: 0.728543\n",
      "Early stopping, best iteration is:\n",
      "[268]\tvalid_0's auc: 0.72877\n",
      "[0.7386746182007863, 0.7484257725336398, 0.7339841593120615, 0.7287695557064192]\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's auc: 0.728594\n",
      "[200]\tvalid_0's auc: 0.729215\n",
      "[300]\tvalid_0's auc: 0.729167\n",
      "Early stopping, best iteration is:\n",
      "[217]\tvalid_0's auc: 0.730273\n",
      "[0.7386746182007863, 0.7484257725336398, 0.7339841593120615, 0.7287695557064192, 0.7302730632873199]\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's auc: 0.726486\n",
      "[200]\tvalid_0's auc: 0.728638\n",
      "Early stopping, best iteration is:\n",
      "[182]\tvalid_0's auc: 0.72925\n",
      "[0.7386746182007863, 0.7484257725336398, 0.7339841593120615, 0.7287695557064192, 0.7302730632873199, 0.7292503583012748]\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's auc: 0.714738\n",
      "[200]\tvalid_0's auc: 0.716587\n",
      "[300]\tvalid_0's auc: 0.715879\n",
      "Early stopping, best iteration is:\n",
      "[236]\tvalid_0's auc: 0.716955\n",
      "[0.7386746182007863, 0.7484257725336398, 0.7339841593120615, 0.7287695557064192, 0.7302730632873199, 0.7292503583012748, 0.716954559178744]\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's auc: 0.74265\n",
      "[200]\tvalid_0's auc: 0.745259\n",
      "Early stopping, best iteration is:\n",
      "[150]\tvalid_0's auc: 0.745538\n",
      "[0.7386746182007863, 0.7484257725336398, 0.7339841593120615, 0.7287695557064192, 0.7302730632873199, 0.7292503583012748, 0.716954559178744, 0.7455380434782609]\n",
      "OOF-MEAN-AUC:0.733984, OOF-STD-AUC:0.009470\n",
      "valid_auc:  0.7335549883146465\n",
      "feature importance:\n",
      "feature\n",
      "user_trans_ip_count                                 12284.482084\n",
      "user_days_diff_sum_x                                 5502.990014\n",
      "w2c_channel_0_max                                    4659.755668\n",
      "product7_fail_ratio                                  4160.993517\n",
      "product7_fail_cnt                                    1701.915530\n",
      "user_amount_med_9h                                   1674.926635\n",
      "login_cnt_period_CV                                  1598.253322\n",
      "service3                                             1373.912462\n",
      "age                                                  1243.205934\n",
      "product7_cnt                                         1213.685704\n",
      "user_trans_ip_3_count                                1186.843404\n",
      "svd_tfidf_type1_1                                    1046.881080\n",
      "agreement3                                           1006.360973\n",
      "user_group_type1_amount_45a1168437c708ff_sum         1002.558652\n",
      "svd_countvec_op_mode_3                                901.821514\n",
      "catboost_product7_fail_ratio                          854.141350\n",
      "svd_countvec_op_mode_2                                847.006345\n",
      "svd_countvec_op_type_3                                822.810387\n",
      "svd_tfidf_type2_3                                     788.123547\n",
      "svd_tfidf_op_type_3                                   754.822514\n",
      "user_trans_type1_nuniq                                746.555585\n",
      "user_group_type1_days_diff_45a1168437c708ff_mean      740.872508\n",
      "user_group_type1_amount_45a1168437c708ff_nunique      703.925125\n",
      "user_amount_med                                       699.612865\n",
      "svd_countvec_op_mode_4                                666.524955\n",
      "province                                              655.913372\n",
      "ip_cnt_per_using_time                                 646.828839\n",
      "svd_tfidf_type2_4                                     646.007933\n",
      "op_cnt_per_usign_time                                 642.838314\n",
      "city_null_ip_amount_percent                           640.013590\n",
      "login_cnt_period_var                                  627.233440\n",
      "catboost_user_amount_med_9h                           609.024240\n",
      "card_a_cnt_avg                                        596.332794\n",
      "user_group_type1_days_diff_45a1168437c708ff_CV        583.753107\n",
      "user_hour_min_y                                       578.805773\n",
      "ip_cnt_per_login_days_cnt                             575.683877\n",
      "city_user_trans_ip_count_mea                          573.539549\n",
      "user_group_type1_days_diff_45a1168437c708ff_min       570.453639\n",
      "card_d_cnt                                            553.242953\n",
      "user_group_tunnel_in_amount_b2e7fa260df4998d_min      549.968838\n",
      "Name: importance, dtype: float64\n",
      "CPU times: user 47min 14s, sys: 11min 28s, total: 58min 42s\n",
      "Wall time: 16min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('开始模型训练...')\n",
    "train = data[~data['label'].isnull()].copy()\n",
    "target = train['label']\n",
    "test = data[data['label'].isnull()].copy()\n",
    "\n",
    "#train, test = kfold_stats_feature(train, test, target_encode_cols, 8)\n",
    "train,test = target_encoding(train,test,target_encode_cols,k=8)\n",
    "\n",
    "data = pd.concat([train, test], axis=0, ignore_index=True) \n",
    "for col in tqdm([f for f in data.select_dtypes('object').columns if f not in ['user']]):\n",
    "    le = LabelEncoder()\n",
    "    data[col].fillna(\"-1\", inplace=True)\n",
    "    data[col] = le.fit_transform(data[col])\n",
    "    \n",
    "train = data[~data['label'].isnull()].copy()\n",
    "test = data[data['label'].isnull()].copy()\n",
    "#train.loc[train.fold == fold_, colname]=train.iloc[trn_idx].groupby([feat])[f].transform('mean')\n",
    "train.drop(['service3_level'], axis=1, inplace=True)\n",
    "test.drop(['service3_level'], axis=1, inplace=True)\n",
    "#train.drop(target_encode_cols, axis=1, inplace=True)\n",
    "#test.drop(target_encode_cols, axis=1, inplace=True)\n",
    "\n",
    "lgb_preds, lgb_oof, lgb_score,feature_importance_df = lgb_model_origin(train=train, target=target, test=test, k=8)\n",
    "\n",
    "sub_df = test[['user']].copy()\n",
    "sub_df['prob'] = lgb_preds\n",
    "sub_df.to_csv('sub(8folds).csv', index=False)\n",
    "feature_importance_df.to_csv('feature(1).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.050321,
     "end_time": "2020-08-19T15:51:54.261484",
     "exception": false,
     "start_time": "2020-08-19T15:51:54.211163",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2020-08-19T15:51:54.393700Z",
     "iopub.status.busy": "2020-08-19T15:51:54.391170Z",
     "iopub.status.idle": "2020-08-19T16:13:12.606398Z",
     "shell.execute_reply": "2020-08-19T16:13:12.604956Z"
    },
    "papermill": {
     "duration": 1278.286844,
     "end_time": "2020-08-19T16:13:12.606902",
     "exception": false,
     "start_time": "2020-08-19T15:51:54.320058",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始模型训练...\n",
      "Current num of features: 746\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's auc: 0.741433\n",
      "[200]\tvalid_0's auc: 0.74257\n",
      "[300]\tvalid_0's auc: 0.74352\n",
      "[400]\tvalid_0's auc: 0.743421\n",
      "[500]\tvalid_0's auc: 0.743912\n",
      "Early stopping, best iteration is:\n",
      "[498]\tvalid_0's auc: 0.744054\n",
      "[0.7440540710017827]\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's auc: 0.746102\n",
      "[200]\tvalid_0's auc: 0.747838\n",
      "Early stopping, best iteration is:\n",
      "[135]\tvalid_0's auc: 0.748626\n",
      "[0.7440540710017827, 0.7486255232818275]\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's auc: 0.727834\n",
      "[200]\tvalid_0's auc: 0.730777\n",
      "Early stopping, best iteration is:\n",
      "[181]\tvalid_0's auc: 0.731173\n",
      "[0.7440540710017827, 0.7486255232818275, 0.7311728919071874]\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's auc: 0.738917\n",
      "[200]\tvalid_0's auc: 0.738258\n",
      "Early stopping, best iteration is:\n",
      "[117]\tvalid_0's auc: 0.739563\n",
      "[0.7440540710017827, 0.7486255232818275, 0.7311728919071874, 0.7395630541407282]\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's auc: 0.724947\n",
      "[200]\tvalid_0's auc: 0.72587\n",
      "Early stopping, best iteration is:\n",
      "[142]\tvalid_0's auc: 0.727063\n",
      "[0.7440540710017827, 0.7486255232818275, 0.7311728919071874, 0.7395630541407282, 0.7270630541407281]\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's auc: 0.728611\n",
      "[200]\tvalid_0's auc: 0.731491\n",
      "Early stopping, best iteration is:\n",
      "[176]\tvalid_0's auc: 0.73219\n",
      "[0.7440540710017827, 0.7486255232818275, 0.7311728919071874, 0.7395630541407282, 0.7270630541407281, 0.7321896811922279]\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's auc: 0.72566\n",
      "[200]\tvalid_0's auc: 0.727048\n",
      "[300]\tvalid_0's auc: 0.727982\n",
      "[400]\tvalid_0's auc: 0.728422\n",
      "[500]\tvalid_0's auc: 0.728547\n",
      "Early stopping, best iteration is:\n",
      "[479]\tvalid_0's auc: 0.728625\n",
      "[0.7440540710017827, 0.7486255232818275, 0.7311728919071874, 0.7395630541407282, 0.7270630541407281, 0.7321896811922279, 0.7286254951895869]\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's auc: 0.732671\n",
      "[200]\tvalid_0's auc: 0.731914\n",
      "Early stopping, best iteration is:\n",
      "[118]\tvalid_0's auc: 0.734167\n",
      "[0.7440540710017827, 0.7486255232818275, 0.7311728919071874, 0.7395630541407282, 0.7270630541407281, 0.7321896811922279, 0.7286254951895869, 0.7341666666666666]\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's auc: 0.721414\n",
      "[200]\tvalid_0's auc: 0.723338\n",
      "[300]\tvalid_0's auc: 0.723107\n",
      "Early stopping, best iteration is:\n",
      "[211]\tvalid_0's auc: 0.724267\n",
      "[0.7440540710017827, 0.7486255232818275, 0.7311728919071874, 0.7395630541407282, 0.7270630541407281, 0.7321896811922279, 0.7286254951895869, 0.7341666666666666, 0.7242673552159969]\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's auc: 0.739353\n",
      "[200]\tvalid_0's auc: 0.742686\n",
      "[300]\tvalid_0's auc: 0.741266\n",
      "Early stopping, best iteration is:\n",
      "[208]\tvalid_0's auc: 0.743103\n",
      "[0.7440540710017827, 0.7486255232818275, 0.7311728919071874, 0.7395630541407282, 0.7270630541407281, 0.7321896811922279, 0.7286254951895869, 0.7341666666666666, 0.7242673552159969, 0.7431027164685908]\n",
      "OOF-MEAN-AUC:0.735283, OOF-STD-AUC:0.007716\n",
      "valid_auc:  0.7349509350970787\n",
      "feature importance:\n",
      "feature\n",
      "user_trans_ip_count                                 12797.469691\n",
      "user_days_diff_sum_x                                 5531.171284\n",
      "w2c_channel_0_max                                    4743.990702\n",
      "product7_fail_ratio                                  4305.930253\n",
      "user_amount_med_9h                                   1949.303580\n",
      "product7_fail_cnt                                    1678.038702\n",
      "login_cnt_period_CV                                  1616.490397\n",
      "service3                                             1405.921999\n",
      "age                                                  1296.452521\n",
      "product7_cnt                                         1271.586729\n",
      "user_trans_ip_3_count                                1222.925117\n",
      "user_group_type1_amount_45a1168437c708ff_sum         1043.244573\n",
      "agreement3                                           1032.305384\n",
      "svd_tfidf_type1_1                                    1003.001820\n",
      "svd_countvec_op_type_3                                921.474966\n",
      "catboost_product7_fail_ratio                          881.464822\n",
      "svd_countvec_op_mode_3                                878.398253\n",
      "svd_countvec_op_mode_2                                850.330658\n",
      "user_amount_med                                       823.937077\n",
      "svd_tfidf_type2_3                                     820.477639\n",
      "svd_tfidf_op_type_3                                   767.468557\n",
      "user_trans_type1_nuniq                                762.435458\n",
      "province                                              714.232056\n",
      "user_group_type1_days_diff_45a1168437c708ff_mean      711.809549\n",
      "user_group_type1_amount_45a1168437c708ff_nunique      711.268487\n",
      "svd_countvec_op_mode_4                                702.945008\n",
      "op_cnt_per_usign_time                                 700.557904\n",
      "ip_cnt_per_using_time                                 696.629752\n",
      "svd_tfidf_type2_4                                     682.046024\n",
      "city_null_ip_amount_percent                           663.106741\n",
      "login_cnt_period_var                                  648.218566\n",
      "user_group_type1_days_diff_45a1168437c708ff_min       639.692652\n",
      "user_group_type1_amount_45a1168437c708ff_count        606.069466\n",
      "user_group_type1_days_diff_45a1168437c708ff_CV        598.861701\n",
      "user_hour_min_y                                       595.895270\n",
      "city_user_trans_ip_count_mea                          593.487371\n",
      "user_group_tunnel_in_amount_b2e7fa260df4998d_min      589.077887\n",
      "ip_cnt_per_login_days_cnt                             586.083557\n",
      "card_d_cnt                                            577.351711\n",
      "card_a_cnt_avg                                        569.525291\n",
      "Name: importance, dtype: float64\n",
      "CPU times: user 1h 4min 47s, sys: 14min 47s, total: 1h 19min 35s\n",
      "Wall time: 21min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('开始模型训练...')\n",
    "lgb_preds, lgb_oof, lgb_score,feature_importance_df = lgb_model_origin(train=train, target=target, test=test, k=10)\n",
    "\n",
    "sub_df = test[['user']].copy()\n",
    "sub_df['prob'] = lgb_preds\n",
    "sub_df.to_csv('sub(10folds).csv', index=False)\n",
    "feature_importance_df.to_csv('feature(2).csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.069408,
     "end_time": "2020-08-19T16:13:12.753456",
     "exception": false,
     "start_time": "2020-08-19T16:13:12.684048",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ---分界线---"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.075459,
     "end_time": "2020-08-19T16:13:12.904278",
     "exception": false,
     "start_time": "2020-08-19T16:13:12.828819",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 测试target encoder是否有效"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-19T16:13:13.089526Z",
     "iopub.status.busy": "2020-08-19T16:13:13.088100Z",
     "iopub.status.idle": "2020-08-19T16:13:32.019345Z",
     "shell.execute_reply": "2020-08-19T16:13:32.018374Z"
    },
    "papermill": {
     "duration": 19.035311,
     "end_time": "2020-08-19T16:13:32.019521",
     "exception": false,
     "start_time": "2020-08-19T16:13:12.984210",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "DATA_PATH = '/kaggle/input/financial-risk/'\n",
    "train_trans = pd.read_csv(DATA_PATH+'train_trans.csv')\n",
    "test_trans = pd.read_csv(DATA_PATH+'test_a_trans.csv')\n",
    "train_base = pd.read_csv(DATA_PATH+'train_base.csv')\n",
    "train_label = pd.read_csv(DATA_PATH+'train_label.csv')\n",
    "train_base = train_base.merge(train_label,on=['user'],how='left')\n",
    "test_base= pd.read_csv(DATA_PATH+  'test_a_base.csv')\n",
    "targer = pd.read_csv(DATA_PATH+'train_label.csv')\n",
    "\n",
    "train_op = pd.read_csv(DATA_PATH+'train_op.csv')\n",
    "test_op = pd.read_csv(DATA_PATH+'test_a_op.csv')\n",
    "\n",
    "global orgin_features \n",
    "orgin_features = train_base.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-19T16:13:32.192370Z",
     "iopub.status.busy": "2020-08-19T16:13:32.177618Z",
     "iopub.status.idle": "2020-08-19T16:13:32.438311Z",
     "shell.execute_reply": "2020-08-19T16:13:32.437342Z"
    },
    "papermill": {
     "duration": 0.347508,
     "end_time": "2020-08-19T16:13:32.438488",
     "exception": false,
     "start_time": "2020-08-19T16:13:32.090980",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "train_base['city_balance_avg'] = train_base['city'].map(str) + '_' + train_base['balance_avg'].map(str)\n",
    "train_base['city_level'] = train_base['city'].map(str) + '_' + train_base['level'].map(str)\n",
    "train_base['city_product1_amount'] = train_base['city'].map(str) + '_' + train_base['product1_amount'].map(str)\n",
    "train_base['city_balance1_avg'] = train_base['city'].map(str) + '_' + train_base['balance1_avg'].map(str)\n",
    "train_base['city_balance2_avg'] = train_base['city'].map(str) + '_' + train_base['balance2_avg'].map(str)\n",
    "\n",
    "test_base['city_balance_avg'] =  test_base['city'].map(str) + '_' + test_base['balance_avg'].map(str)\n",
    "test_base['city_level'] =        test_base['city'].map(str) + '_' + test_base['level'].map(str)\n",
    "test_base['city_product1_amount'] = test_base['city'].map(str) + '_' + test_base['product1_amount'].map(str)\n",
    "test_base['city_balance1_avg'] = test_base['city'].map(str) + '_' + test_base['balance1_avg'].map(str)\n",
    "test_base['city_balance2_avg'] = test_base['city'].map(str) + '_' + test_base['balance2_avg'].map(str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.251053,
     "end_time": "2020-08-19T16:13:32.763767",
     "exception": false,
     "start_time": "2020-08-19T16:13:32.512714",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 自己定义的target encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-19T16:13:32.945048Z",
     "iopub.status.busy": "2020-08-19T16:13:32.943297Z",
     "iopub.status.idle": "2020-08-19T16:13:32.950904Z",
     "shell.execute_reply": "2020-08-19T16:13:32.949140Z"
    },
    "papermill": {
     "duration": 0.112594,
     "end_time": "2020-08-19T16:13:32.951228",
     "exception": false,
     "start_time": "2020-08-19T16:13:32.838634",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def target_encoding_1(train,test, feature, k,noise_level=0.01,smoothing=20):\n",
    "    \n",
    "    kf = StratifiedKFold(n_splits=k, shuffle=True, random_state=2020)    \n",
    "\n",
    "    for col in feature:\n",
    "        train['target_'+col] = 0\n",
    "        test['target_' + col] =0\n",
    "        if col not in orgin_features:\n",
    "            smoothing=50\n",
    "            noise_level=0.7\n",
    "        for trn_idx,val_idx in kf.split(train,train['label']):\n",
    "            X = train.iloc[trn_idx][col]\n",
    "            enc_tar = TargetEncoder(smoothing=smoothing).fit(X,train.iloc[trn_idx]['label'])\n",
    "            train['target_'+col] += add_noise(enc_tar.transform(train[col])[col],noise_level) / k\n",
    "            test['target_'+col] += add_noise(enc_tar.transform(test[col])[col],noise_level) / k\n",
    "       \n",
    "    return  train,test\n",
    "\n",
    "\n",
    "\n",
    "def Catboost_encoding(train,test, feature, k,noise_level=0.01,sigma=2.0,a=1):\n",
    "    \n",
    "    kf = StratifiedKFold(n_splits=k, shuffle=True, random_state=2020)    \n",
    "\n",
    "    for col in feature:\n",
    "        train['catboost_'+col] = 0\n",
    "        test['catboost_' + col] =0\n",
    "        if col not in orgin_features:\n",
    "            sigma=5.0\n",
    "            a = 2\n",
    "        for trn_idx,val_idx in kf.split(train,train['label']):\n",
    "            X = train.iloc[trn_idx][col]\n",
    "            enc_tar = CatBoostEncoder(sigma=sigma,a=a).fit(X,train.iloc[trn_idx]['label'])\n",
    "            train['catboost_'+col] += add_noise(enc_tar.transform(train[col])[col],noise_level) / k\n",
    "            test['catboost_'+col] += add_noise(enc_tar.transform(test[col])[col],noise_level) / k        \n",
    "    return  train,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-19T16:13:33.123422Z",
     "iopub.status.busy": "2020-08-19T16:13:33.121631Z",
     "iopub.status.idle": "2020-08-19T16:18:05.687528Z",
     "shell.execute_reply": "2020-08-19T16:18:05.688363Z"
    },
    "papermill": {
     "duration": 272.662933,
     "end_time": "2020-08-19T16:18:05.688682",
     "exception": false,
     "start_time": "2020-08-19T16:13:33.025749",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             Raw Categories |   Encoded Categories |  Baseline Categories  |  CatBoost Categories\n",
      "province             :  0.571175 + 0.000598 | 0.571213 + 0.000621  | 0.566010 + 0.001213  | 0.571106 + 0.000575\n",
      "city                 :  0.599885 + 0.000483 | 0.599572 + 0.000446  | 0.564816 + 0.001815  | 0.599797 + 0.000481\n",
      "city_level           :  0.619688 + 0.000586 | 0.571063 + 0.003032  | 0.561827 + 0.002352  | 0.616616 + 0.000635\n",
      "city_balance1_avg    :  0.734679 + 0.000766 | 0.648458 + 0.001810  | 0.572507 + 0.002086  | 0.704834 + 0.000692\n",
      "city_balance2_avg    :  0.651628 + 0.000864 | 0.595657 + 0.002532  | 0.567518 + 0.002903  | 0.646448 + 0.000937\n",
      "city_product1_amount :  0.646222 + 0.000623 | 0.594867 + 0.002254  | 0.591823 + 0.001756  | 0.642557 + 0.000612\n",
      "city_balance_avg     :  0.761732 + 0.000644 | 0.674203 + 0.001903  | 0.547982 + 0.001843  | 0.737928 + 0.000764\n"
     ]
    }
   ],
   "source": [
    "# 评价编码函数\n",
    "folds = StratifiedKFold(n_splits=10, shuffle=True, random_state=2020)\n",
    "f_cats = ['province','city',\"city_level\",'city_balance1_avg','city_balance2_avg',\n",
    "                      \"city_product1_amount\",'city_balance_avg']\n",
    "\n",
    "print(\"%20s   %20s | %20s | %20s  | %20s\" % (\"\", \"Raw Categories\",\"Encoded Categories\",\"Baseline Categories\",\"CatBoost Categories\"))\n",
    "for f in f_cats:\n",
    "    print(\"%-20s : \" % f, end=\"\")\n",
    "    origin_scores = []\n",
    "    baseline_scores = []\n",
    "    target_scores=[]   \n",
    "    catboost_scores=[]\n",
    "    for trn_idx, val_idx in folds.split(train_base, train_base.label):\n",
    "        \n",
    "        val_f = train_base.iloc[trn_idx].groupby([f])['label'].transform('mean')\n",
    "        val_f.name=f\n",
    "        val_tgt = train_base.label.iloc[trn_idx]\n",
    "        \n",
    "        trn_f = train_base.iloc[trn_idx][f]\n",
    "        trn_tgt = train_base.label.iloc[trn_idx]\n",
    "        \n",
    "        val_tf ,temp = target_encoding_1(train_base.iloc[trn_idx],test_base,[f],5)\n",
    "        val_tf1 ,temp = kfold_stats_feature(train_base.iloc[trn_idx].reset_index(),test_base,[f],5)  \n",
    "        val_tf2 ,temp = Catboost_encoding(train_base.iloc[trn_idx],test_base,[f],5)\n",
    "        \n",
    "        origin_scores.append(max(roc_auc_score(val_tgt, val_f), 1 - roc_auc_score(val_tgt, val_f)))\n",
    "        target_scores.append(roc_auc_score(val_tgt, val_tf['target_'+f]))\n",
    "        baseline_scores.append(roc_auc_score(val_tgt,val_tf1[f+'_label_kfold_mean']))\n",
    "        catboost_scores.append(roc_auc_score(val_tgt, val_tf2['catboost_'+f]))\n",
    "    val_tf1 ,temp = kfold_stats_feature(train_base,test_base,[f],5)    \n",
    "    print(\" %.6f + %.6f | %6f + %.6f  | %.6f + %.6f  | %.6f + %.6f\" \n",
    "          % (np.mean( origin_scores), np.std( origin_scores), np.mean(target_scores), np.std(target_scores),\n",
    "             np.mean(baseline_scores), np.std(baseline_scores),np.mean(catboost_scores), np.std(catboost_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-19T16:18:05.859612Z",
     "iopub.status.busy": "2020-08-19T16:18:05.858495Z",
     "iopub.status.idle": "2020-08-19T16:18:07.254764Z",
     "shell.execute_reply": "2020-08-19T16:18:07.253971Z"
    },
    "papermill": {
     "duration": 1.487079,
     "end_time": "2020-08-19T16:18:07.255190",
     "exception": false,
     "start_time": "2020-08-19T16:18:05.768111",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASELINE Encode score: 0.5667795997751115\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Mean_encoded_province'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2888\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2889\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2890\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Mean_encoded_province'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-128187e7e8d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m cols = ['province','city',\"city_level\",'city_balance1_avg','city_balance2_avg',\n\u001b[1;32m     16\u001b[0m                       \"city_product1_amount\",'city_balance_avg']\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mencode_val\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_base\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_base\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-128187e7e8d4>\u001b[0m in \u001b[0;36mencode_val\u001b[0;34m(train_base, test_base, cols)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"BASELINE Encode score: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtemp_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'{}_label_kfold_mean'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Myown Encode score:    {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtemp_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Mean_encoded_{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Origin score:          {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtemp_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2897\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2898\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2899\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2900\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2901\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2889\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2890\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2891\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2893\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Mean_encoded_province'"
     ]
    }
   ],
   "source": [
    "#  自己写target encoder的评价方法,没有采用5折方法\n",
    "def encode_val(train_base,test_base,cols):\n",
    "    for col in cols:\n",
    "    \n",
    "        temp_train,temp_test=kfold_stats_feature(train_base,test_base,[col],5)\n",
    "        temp=train_base.groupby([col])['label'].agg(cnt='count',mean='mean')\n",
    "        temp=temp.reset_index()\n",
    "        temp_train=temp_train.merge(temp,on=[col],how='left')\n",
    "        \n",
    "        \n",
    "        print(\"BASELINE Encode score: {}\".format(roc_auc_score(temp_train['label'],temp_train['{}_label_kfold_mean'.format(col)])))\n",
    "        print(\"Myown Encode score:    {}\".format(roc_auc_score(temp_train['label'],temp_train['Mean_encoded_{}'.format(col)])))\n",
    "        print(\"Origin score:          {}\".format(roc_auc_score(temp_train['label'],temp_train['mean'])))\n",
    "        print()\n",
    "cols = ['province','city',\"city_level\",'city_balance1_avg','city_balance2_avg',\n",
    "                      \"city_product1_amount\",'city_balance_avg']\n",
    "encode_val(train_base,test_base,cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.077632,
     "end_time": "2020-08-19T16:18:07.409279",
     "exception": false,
     "start_time": "2020-08-19T16:18:07.331647",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 3325.45645,
   "end_time": "2020-08-19T16:18:07.711497",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-08-19T15:22:42.255047",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
