{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "papermill": {
      "duration": 3325.45645,
      "end_time": "2020-08-19T16:18:07.711497",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2020-08-19T15:22:42.255047",
      "version": "2.1.0"
    },
    "colab": {
      "name": "yi-pay-model (0.699701).ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2020-08-19T15:22:48.850324Z",
          "iopub.status.busy": "2020-08-19T15:22:48.849322Z",
          "iopub.status.idle": "2020-08-19T15:22:51.519915Z",
          "shell.execute_reply": "2020-08-19T15:22:51.518773Z"
        },
        "papermill": {
          "duration": 2.698172,
          "end_time": "2020-08-19T15:22:51.520132",
          "exception": false,
          "start_time": "2020-08-19T15:22:48.821960",
          "status": "completed"
        },
        "tags": [],
        "id": "CJAG6Up3T-vp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3b5cc232-5888-4ff7-9ac5-3b5fe2d355d4"
      },
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.decomposition import TruncatedSVD,PCA\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn import preprocessing\n",
        "from tqdm import tqdm\n",
        "import multiprocessing\n",
        "import gc\n",
        "from category_encoders import *\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "warnings.filterwarnings('ignore')\n",
        "os.chdir(r\"/content/drive/My Drive/Colab Notebooks/Datacastle/code\")\n",
        "print(multiprocessing.cpu_count())"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "execution": {
          "iopub.execute_input": "2020-08-19T15:22:51.613294Z",
          "iopub.status.busy": "2020-08-19T15:22:51.573431Z",
          "iopub.status.idle": "2020-08-19T15:22:51.643024Z",
          "shell.execute_reply": "2020-08-19T15:22:51.642159Z"
        },
        "papermill": {
          "duration": 0.108709,
          "end_time": "2020-08-19T15:22:51.643229",
          "exception": false,
          "start_time": "2020-08-19T15:22:51.534520",
          "status": "completed"
        },
        "tags": [],
        "id": "VUPCzybZT-vx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "d36a565f-e381-4e09-cfb9-c323a3782f48"
      },
      "source": [
        "%%time\n",
        "def gen_user_status_features(df,value):\n",
        "    group_df = df.groupby(['user'])[value].agg(\n",
        "        a='mean',\n",
        "        b= 'std',\n",
        "        c='max',\n",
        "        d='min',\n",
        "        e='sum',\n",
        "        f='median',\n",
        "        g='count',\n",
        "        h='median',\n",
        "        i='skew',\n",
        "        ).reset_index()\n",
        "    group_df['j']=group_df['b'] / group_df['a']\n",
        "    group_df.rename(columns={\n",
        "                    'a':'user_{}_mea'.format(value),\n",
        "                    'b':'user_{}_std'.format(value),\n",
        "                    'c':'user_{}_max'.format(value),\n",
        "                    'd':'user_{}_min'.format(value),\n",
        "                    'e':'user_{}_sum'.format(value),\n",
        "                    'f':'user_{}_med'.format(value),\n",
        "                    'g':'user_{}_cnt'.format(value),\n",
        "                    'h':'user_{}_median'.format(value),\n",
        "                    'i':'user_{}_skew'.format(value),\n",
        "                    'j':'user_{}_CV'.format(value)\n",
        "                    },\n",
        "                   inplace=True\n",
        "                   )\n",
        "    return group_df\n",
        "def gen_city_status_features(df,value):\n",
        "    group_df = df.groupby(['city'])[value].agg(\n",
        "        a='mean',\n",
        "        b= 'std',\n",
        "        c='max',\n",
        "        d='min',\n",
        "        e='sum',\n",
        "        f='median',\n",
        "        g='count',\n",
        "        h='median',\n",
        "        i='skew',\n",
        "        ).reset_index()\n",
        "    group_df['j']=group_df['b'] / group_df['a']\n",
        "    group_df.rename(columns={\n",
        "                    'a':'city_{}_mea'.format(value),\n",
        "                    'b':'city_{}_std'.format(value),\n",
        "                    'c':'city_{}_max'.format(value),\n",
        "                    'd':'city_{}_min'.format(value),\n",
        "                    'e':'city_{}_sum'.format(value),\n",
        "                    'f':'city_{}_med'.format(value),\n",
        "                    'g':'city_{}_cnt'.format(value),\n",
        "                    'h':'city_{}_median'.format(value),\n",
        "                    'i':'city_{}_skew'.format(value),\n",
        "                    'j':'city_{}_CV'.format(value)\n",
        "                    },\n",
        "                   inplace=True\n",
        "                   )\n",
        "    return group_df\n",
        "    \n",
        "def gen_user_group_features(df, col,value):\n",
        "    \n",
        "    group_df = df.pivot_table(index='user',\n",
        "                              columns=col,\n",
        "                              values=value,\n",
        "                              dropna=False,\n",
        "                              aggfunc=['count', 'sum','nunique','mean','std','max','min']).fillna(0)\n",
        "    cols=group_df.columns\n",
        "    group_df.columns = ['user_group_{}_{}_{}_{}'.format(col,value, f[1], f[0]) for f in group_df.columns]\n",
        "    for f in cols:\n",
        "        group_df['user_group_{}_{}_{}_CV'.format(col,value,f[1])] = group_df['user_group_{}_{}_{}_std'.format(col,value,f[1])] / group_df['user_group_{}_{}_{}_mean'.format(col,value,f[1])]\n",
        "    group_df.reset_index(inplace=True)\n",
        "    return group_df\n",
        "\n",
        "def gen_user_group_nunique_features(df, col,value):\n",
        "    \n",
        "    group_df = df.pivot_table(index='user',\n",
        "                              columns=col,\n",
        "                              values=value,\n",
        "                              dropna=False,\n",
        "                              aggfunc=['count','nunique']).fillna(0)\n",
        "    group_df.columns = ['user_{}_{}_{}_{}'.format(col,value, f[1], f[0]) for f in group_df.columns]\n",
        "    group_df.reset_index(inplace=True)\n",
        "    return group_df\n",
        "\n",
        "def gen_user_nunique_features(df, value, prefix):\n",
        "    a=\"user_{}_{}_nuniq\".format(prefix, value)\n",
        "    b=\"user_{}_{}_count\".format(prefix, value)\n",
        "    group_df = df.groupby(['user'])[value].agg(\n",
        "        a='nunique',b='count'\n",
        "    ).reset_index()\n",
        "    \n",
        "    group_df.rename(columns={'a':a,'b':b},inplace=True) \n",
        "    return group_df\n",
        "\n",
        "def gen_user_null_features(df, value, prefix):\n",
        "    df['is_null'] = 0\n",
        "    df.loc[df[value].isnull(), 'is_null'] = 1\n",
        "\n",
        "    group_df = df.groupby(['user'])['is_null'].agg(a='sum',\n",
        "                                                    b='mean').reset_index()\n",
        "    \n",
        "    group_df.rename(columns={'a':'user_{}_{}_null_cnt'.format(prefix, value),\n",
        "                    'b':'user_{}_{}_null_ratio'.format(prefix, value)},\n",
        "                   inplace=True\n",
        "                   )\n",
        "\n",
        "    return group_df\n",
        "\n",
        "    \n",
        "def gen_user_window_amount_features(df, window):\n",
        "    \n",
        "    group_df = df[df['days_diff']>window].groupby('user')['amount'].agg(\n",
        "        a='mean',\n",
        "         b='std',\n",
        "         c='max',\n",
        "         d='min',\n",
        "         e='sum',\n",
        "        f='median',\n",
        "         g='count',\n",
        "        ).reset_index()\n",
        "    \n",
        "    group_df.rename(columns={'a':'user_amount_mean_{}d'.format(window),\n",
        "                    'b':'user_amount_std_{}d'.format(window),\n",
        "                    'c':'user_amount_max_{}d'.format(window),\n",
        "                    'd':'user_amount_min_{}d'.format(window),\n",
        "                    'e':'user_amount_sum_{}d'.format(window),\n",
        "                    'f':'user_amount_med_{}d'.format(window),\n",
        "                    'g':'user_amount_cnt_{}d'.format(window)},\n",
        "                   inplace=True\n",
        "                   )\n",
        "    \n",
        "    return group_df\n",
        "\n",
        "def gen_user_window_hour_amount_features(df, window):\n",
        "    \n",
        "    group_df = df[df['hour']>window].groupby('user')['amount'].agg(\n",
        "        a='mean',\n",
        "         b='std',\n",
        "         c='max',\n",
        "         d='min',\n",
        "         e='sum',\n",
        "         f='median',\n",
        "         g='count',\n",
        "        ).reset_index()\n",
        "    \n",
        "    group_df.rename(columns={'a':'user_amount_mean_{}h'.format(window),\n",
        "                    'b':'user_amount_std_{}h'.format(window),\n",
        "                    'c':'user_amount_max_{}h'.format(window),\n",
        "                    'd':'user_amount_min_{}h'.format(window),\n",
        "                    'e':'user_amount_sum_{}h'.format(window),\n",
        "                    'f':'user_amount_med_{}h'.format(window),\n",
        "                    'g':'user_amount_cnt_{}h'.format(window)},\n",
        "                   inplace=True\n",
        "                   )\n",
        "    \n",
        "    return group_df\n",
        "\n",
        "\n",
        "def gen_user_tfidf_features(df, value):\n",
        "    df[value] = df[value].astype(str)\n",
        "    df[value].fillna('-1', inplace=True)\n",
        "    group_df = df.groupby(['user']).apply(lambda x: x[value].tolist()).reset_index()\n",
        "    group_df.columns = ['user', 'list']\n",
        "    group_df['list'] = group_df['list'].apply(lambda x: ','.join(x))\n",
        "    enc_vec = TfidfVectorizer()\n",
        "    tfidf_vec = enc_vec.fit_transform(group_df['list'])\n",
        "    svd_enc = TruncatedSVD(n_components=5, n_iter=20, random_state=2020)\n",
        "    vec_svd = svd_enc.fit_transform(tfidf_vec)\n",
        "    vec_svd = pd.DataFrame(vec_svd)\n",
        "    vec_svd.columns = ['svd_tfidf_{}_{}'.format(value, i) for i in range(5)]\n",
        "    group_df = pd.concat([group_df, vec_svd], axis=1)\n",
        "    del group_df['list']\n",
        "    return group_df\n",
        "\n",
        "def gen_user_countvec_features(df, value):\n",
        "    df[value] = df[value].astype(str)\n",
        "    df[value].fillna('-1', inplace=True)\n",
        "    group_df = df.groupby(['user']).apply(lambda x: x[value].tolist()).reset_index()\n",
        "    group_df.columns = ['user', 'list']\n",
        "    group_df['list'] = group_df['list'].apply(lambda x: ','.join(x))\n",
        "    enc_vec = CountVectorizer()\n",
        "    tfidf_vec = enc_vec.fit_transform(group_df['list'])\n",
        "    svd_enc = TruncatedSVD(n_components=5, n_iter=20, random_state=2020)\n",
        "    vec_svd = svd_enc.fit_transform(tfidf_vec)\n",
        "    vec_svd = pd.DataFrame(vec_svd)\n",
        "    vec_svd.columns = ['svd_countvec_{}_{}'.format(value, i) for i in range(5)]\n",
        "    group_df = pd.concat([group_df, vec_svd], axis=1)\n",
        "    del group_df['list']\n",
        "    return group_df\n",
        "\n",
        "#定义加载函数\n",
        "def load_dataset(DATA_PATH):\n",
        "    train_label = pd.read_csv(DATA_PATH+'train_label.csv')\n",
        "    test_base = pd.read_csv(DATA_PATH+'test_a_base.csv')\n",
        "    \n",
        "    train_base = pd.read_csv(DATA_PATH+'train_base.csv')\n",
        "    train_op = pd.read_csv(DATA_PATH+'train_op.csv')\n",
        "    train_trans = pd.read_csv(DATA_PATH+'train_trans.csv')\n",
        "    test_op = pd.read_csv(DATA_PATH+'test_a_op.csv')\n",
        "    test_trans = pd.read_csv(DATA_PATH+'test_a_trans.csv')\n",
        "    origin_features = list(train_base.columns)+list(train_trans.columns)+list(train_op.columns)\n",
        "    return train_label, train_base, test_base, train_op, train_trans, test_op, test_trans\n",
        "\n",
        "#tran_trans和train_op文件中的tm_diff时间转换\n",
        "def transform_time(x):\n",
        "    day = int(x.split(' ')[0])\n",
        "    hour = int(x.split(' ')[2].split('.')[0].split(':')[0])\n",
        "    minute = int(x.split(' ')[2].split('.')[0].split(':')[1])\n",
        "    second = int(x.split(' ')[2].split('.')[0].split(':')[2])\n",
        "    return 86400*day+3600*hour+60*minute+second\n",
        "\n",
        "\n",
        "\n",
        "#讲所有的数据进行初步的整合，并且把时间days_diff进行了数字化提取\n",
        "def data_preprocess(DATA_PATH):\n",
        "    train_label, train_base, test_base, train_op, train_trans, test_op, test_trans = load_dataset(DATA_PATH=DATA_PATH)\n",
        "    # 拼接数据\n",
        "    train_df = train_base.copy()\n",
        "    test_df = test_base.copy()\n",
        "        #将train_base数据和train_label整合\n",
        "    train_df = train_label.merge(train_df, on=['user'], how='left')\n",
        "    del train_base, test_base\n",
        "        #将train_op数据和test_op整合--方便将train和test中所有的数据进行统一的处理\n",
        "    op_df = pd.concat([train_op, test_op], axis=0, ignore_index=True)\n",
        "        #将train_trans数据和test_trans整合--方便将train和test中所有的数据进行统一的处理\n",
        "    trans_df = pd.concat([train_trans, test_trans], axis=0, ignore_index=True)\n",
        "        #将train_base数据和test_base整合\n",
        "    data = pd.concat([train_df, test_df], axis=0, ignore_index=True) \n",
        "    del train_op, test_op, train_df, test_df\n",
        "    \n",
        "    \n",
        "    # 时间维度的处理\n",
        "    op_df['days_diff'] = op_df['tm_diff'].apply(lambda x: int(x.split(' ')[0]))\n",
        "    trans_df['days_diff'] = trans_df['tm_diff'].apply(lambda x: int(x.split(' ')[0]))\n",
        "    \n",
        "    op_df['timestamp'] = op_df['tm_diff'].apply(lambda x: transform_time(x))\n",
        "    trans_df['timestamp'] = trans_df['tm_diff'].apply(lambda x: transform_time(x))\n",
        "    op_df['hour'] = op_df['tm_diff'].apply(lambda x: int(x.split(' ')[2].split('.')[0].split(':')[0]))\n",
        "    trans_df['hour'] = trans_df['tm_diff'].apply(lambda x: int(x.split(' ')[2].split('.')[0].split(':')[0]))\n",
        "    trans_df['week'] = trans_df['days_diff'].apply(lambda x: x % 7)\n",
        "    \n",
        "    \n",
        "    # 排序\n",
        "    trans_df = trans_df.sort_values(by=['user', 'timestamp'])\n",
        "    op_df = op_df.sort_values(by=['user', 'timestamp'])\n",
        "    trans_df.reset_index(inplace=True, drop=True)\n",
        "    op_df.reset_index(inplace=True, drop=True)\n",
        "\n",
        "    gc.collect()\n",
        "    return data, op_df, trans_df\n",
        "\n",
        "\n",
        "def kfold_stats_feature(train, test, feats, k):\n",
        "    folds = StratifiedKFold(n_splits=k, shuffle=True, random_state=2020)  # 这里最好和后面模型的K折交叉验证保持一致\n",
        "\n",
        "    train['fold'] = None\n",
        "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(train, train['label'])):\n",
        "        train.loc[val_idx, 'fold'] = fold_\n",
        "\n",
        "    kfold_features = []\n",
        "    for feat in feats:\n",
        "        nums_columns = ['label']\n",
        "        for f in nums_columns:\n",
        "            colname = feat + '_' + f + '_kfold_mean'\n",
        "            kfold_features.append(colname)\n",
        "            train[colname] = None\n",
        "            for fold_, (trn_idx, val_idx) in enumerate(folds.split(train, train['label'])):\n",
        "                tmp_trn = train.iloc[trn_idx]\n",
        "                order_label = tmp_trn.groupby([feat])[f].mean()\n",
        "                tmp = train.loc[train.fold == fold_, [feat]]\n",
        "                train.loc[train.fold == fold_, colname] = tmp[feat].map(order_label)\n",
        "                # fillna\n",
        "                global_mean = train[f].mean()\n",
        "                train.loc[train.fold == fold_, colname] = train.loc[train.fold == fold_, colname].fillna(global_mean)\n",
        "            train[colname] = train[colname].astype(float)\n",
        "\n",
        "        for f in nums_columns:\n",
        "            colname = feat + '_' + f + '_kfold_mean'\n",
        "            test[colname] = None\n",
        "            order_label = train.groupby([feat])[f].mean()\n",
        "            test[colname] = test[feat].map(order_label)\n",
        "            # fillna\n",
        "            global_mean = train[f].mean()\n",
        "            test[colname] = test[colname].fillna(global_mean)\n",
        "            test[colname] = test[colname].astype(float)\n",
        "    del train['fold']\n",
        "    return train, test\n",
        "def w2v_feat(df, feat, mode):\n",
        "    data_frame=df.copy()\n",
        "    for i in feat:\n",
        "        if data_frame[i].dtype != 'object':\n",
        "            data_frame[i] = data_frame[i].astype(str)\n",
        "    data_frame.fillna('nan', inplace=True)\n",
        "\n",
        "    print(f'Start {mode} word2vec ...')\n",
        "    model = Word2Vec(data_frame[feat].values.tolist(), size=10, window=2, min_count=1,\n",
        "                     workers=multiprocessing.cpu_count(), iter=10)\n",
        "    stat_list = ['min', 'max', 'mean', 'std']\n",
        "    new_all = pd.DataFrame()\n",
        "    for m, t in enumerate(feat):\n",
        "        print(f'Start gen feat of {t} ...')\n",
        "        tmp = []\n",
        "        for i in data_frame[t].unique():\n",
        "            tmp_v = [i]\n",
        "            tmp_v.extend(model[i])\n",
        "            tmp.append(tmp_v)\n",
        "        tmp_df = pd.DataFrame(tmp)\n",
        "        w2c_list = [f'w2c_{t}_{n}' for n in range(10)]\n",
        "        tmp_df.columns = [t] + w2c_list\n",
        "        tmp_df = data_frame[['user', t]].merge(tmp_df, on=t)\n",
        "        tmp_df = tmp_df.drop_duplicates().groupby('user').agg(stat_list).reset_index()\n",
        "        tmp_df.columns = ['user'] + [f'{p}_{q}' for p in w2c_list for q in stat_list]\n",
        "        if m == 0:\n",
        "            new_all = pd.concat([new_all, tmp_df], axis=1)\n",
        "        else:\n",
        "            new_all = pd.merge(new_all, tmp_df, how='left', on='user')\n",
        "    return new_all\n",
        "\n",
        "def gen_features(df, op, trans):\n",
        "    \n",
        "    # base数据处理\n",
        "    print(\"base logistic features\")\n",
        "    df['card_a_cnt_avg'] = df[\"card_a_cnt\"]/df[\"using_time\"]\n",
        "    df['card_b_cnt_avg'] = df[\"card_b_cnt\"]/df[\"using_time\"]\n",
        "    df['card_c_cnt_avg'] = df[\"card_c_cnt\"]/df[\"using_time\"]\n",
        "    df['card_d_cnt_avg'] = df[\"card_d_cnt\"]/df[\"using_time\"]\n",
        "    df['login_cnt_didive'] = df[\"login_cnt_period1\"]/df[\"login_cnt_period2\"]\n",
        "    df['product7_fail_ratio'] = df['product7_fail_cnt'] / df['product7_cnt']\n",
        "    df['city_count'] = df.groupby(['city'])['user'].transform('count')  ## 与原始数据集相同数量的项目\n",
        "    df['city_product7_fail_ratio'] =  df.groupby(['city'])['product7_fail_cnt'].transform('sum')/df.groupby(['city'])['product7_cnt'].transform('sum')\n",
        "    df['province_count'] = df.groupby(['province'])['user'].transform('count')\n",
        "    df['province_product7_fail_ratio'] = df.groupby(['province'])['product7_fail_cnt'].transform('sum')/df.groupby(['province'])['product7_cnt'].transform('sum')\n",
        "    \n",
        "    df['login_cnt_period_var']=df[['login_cnt_period1','login_cnt_period2']].std(axis=1)\n",
        "    df['login_cnt_period_CV']=df['login_cnt_period_var'] / df[['login_cnt_period1','login_cnt_period2']].mean(axis=1)\n",
        "    df['card_var']=df[['card_a_cnt','card_b_cnt','card_c_cnt','card_d_cnt']].std(axis=1)\n",
        "    df['card_CV']=df['card_var'] / df[['card_a_cnt','card_b_cnt','card_c_cnt','card_d_cnt']].mean(axis=1)\n",
        "    df['op_cnt_var']=df[['op1_cnt','op2_cnt']].std(axis=1)\n",
        "    df['op_cnt_CV']=df['op_cnt_var'] / df[['op1_cnt','op2_cnt']].mean(axis=1)\n",
        "    df['service_var']=df[['service1_cnt','service2_cnt']].std(axis=1)\n",
        "    df['service_CV']=df['service_var'] / df[['service1_cnt','service2_cnt']].mean(axis=1)\n",
        "    df['login_var']=df[['login_cnt_period1','login_cnt_period2','login_cnt_avg']].std(axis=1)\n",
        "    df['login_CV']=df['login_var'] / df[['login_cnt_period1','login_cnt_period2','login_cnt_avg']].mean(axis=1)\n",
        "    df['login_cnt'] = df[\"using_time\"]*df[\"login_cnt_avg\"]\n",
        "    \n",
        "    df['ip_cnt_per_login_cnt_avg'] = df['ip_cnt']/df[['login_cnt_period1','login_cnt_period2','login_cnt_avg']].mean(axis=1)\n",
        "    df['ip_cnt_per_login_days_cnt'] = df['ip_cnt']/df['login_days_cnt']\n",
        "    df['acc_count_per_time'] = df[\"acc_count\"]/df[\"using_time\"]\n",
        "    df['ip_cnt_per_using_time'] = df[\"ip_cnt\"]/df[\"using_time\"]\n",
        "    df['agreement_total_per_using_time'] = df[\"agreement_total\"]/df[\"using_time\"]\n",
        "    df['agreement_total_per_account_cnt'] = df[\"agreement_total\"]/df[\"acc_count\"]\n",
        "    df['ip_cnt_per_acc_count'] = df[\"ip_cnt\"]/df[\"acc_count\"]\n",
        "    df['service1_amt_per_using_time'] = df[\"service1_amt\"]/df[\"using_time\"]\n",
        "    df[\"service_cnt_per_time\"]=(df[\"service1_cnt\"]+df[\"service1_cnt\"])/df[\"using_time\"]\n",
        "    df[\"service_cnt_per_time\"]=(df[\"service1_cnt\"]+df[\"service1_cnt\"])/df[\"using_time\"]\n",
        "    \n",
        "    df['op_cnt_per_usign_time'] = (df[\"op1_cnt\"]+df[\"op2_cnt\"])/df[\"using_time\"]\n",
        "    df['op_cnt_per_login_cnt_avg'] = (df[\"op1_cnt\"]+df[\"op2_cnt\"])/df['login_cnt_avg']\n",
        "    df['op_cnt_per_login_cnt_p1'] =  (df[\"op1_cnt\"]+df[\"op2_cnt\"])/df['login_cnt_period1']\n",
        "    df['op_cnt_per_login_cnt_p2'] =  (df[\"op1_cnt\"]+df[\"op2_cnt\"])/df['login_cnt_period2']\n",
        "\n",
        "    # trans\n",
        "    df = df.merge(gen_user_status_features(trans,'amount'), on=['user'], how='left')\n",
        "    df = df.merge(gen_user_status_features(trans,'days_diff'), on=['user'], how='left')\n",
        "    df = df.merge(gen_user_status_features(trans,'hour'), on=['user'], how='left')\n",
        "    df=df.merge(df.groupby(['city'])['user_amount_sum'].agg(city_amount_mean='mean',city_amount_sum='sum',city_amount_std='std').reset_index(),\n",
        "             on=['city'],how='left')\n",
        "  \n",
        "    print(\"trans nunique && null features\")\n",
        "    for col in tqdm(['days_diff', 'platform', 'tunnel_in', 'tunnel_out', 'type1', 'type2','ip', 'ip_3']):\n",
        "        df = df.merge(gen_user_nunique_features(df=trans, value=col, prefix='trans'), on=['user'], how='left')\n",
        "        df = df.merge(gen_user_null_features(df=trans, value=col, prefix='trans'), on=['user'], how='left')\n",
        "    df['user_amount_per_days'] = df['user_amount_sum'].div(df['user_trans_days_diff_nuniq']) \n",
        "    df['user_amount_per_cnt'] = df['user_amount_sum'] / df['user_amount_cnt']\n",
        "    df['user_amount_per_tunnel_in'] = df['user_amount_sum'] / df['user_trans_tunnel_in_nuniq']\n",
        "    df['user_amount_per_cnt'] = df['user_amount_sum'] / df['user_trans_tunnel_out_nuniq']\n",
        "    df['user_amount_per_platform'] = df['user_amount_sum'] / df['user_trans_platform_nuniq']\n",
        "    df=df.merge(gen_city_status_features(df,'user_trans_ip_null_cnt'),on=['city'],how='left')\n",
        "    df=df.merge(gen_city_status_features(df,'user_trans_ip_count'),on=['city'],how='left')\n",
        "    df['city_ipcnt_vs_nullipcnt']=df['city_user_trans_ip_count_sum'] / df['city_user_trans_ip_null_cnt_sum']\n",
        "    \n",
        "    \n",
        "    temp=trans[trans['ip'].isnull()].groupby(['user'])['amount'].agg(user_null_ip_amount_sum='sum').reset_index()\n",
        "    df=df.merge(temp,on=['user'],how='left')\n",
        "    df['user_null_ip_amount_sum'].fillna(0,inplace=True)\n",
        "    df['user_null_ip_amount_percent'] = df['user_null_ip_amount_sum'] / df['user_amount_sum']\n",
        "    df['user_ip_amount_percent'] = 1-df['user_null_ip_amount_percent'] \n",
        "    df=df.merge(gen_city_status_features(df,'user_null_ip_amount_sum'),on=['city'],how='left')\n",
        "    df['city_null_ip_amount_percent'] = df['city_user_null_ip_amount_sum_sum'] / df['city_amount_sum']\n",
        "    \n",
        "    \n",
        "    print(\"trans group amount features\")\n",
        "    trans_type1=trans.query(\"type1==['f67d4b5a05a1352a','19d44f1a51919482','674e8d5860bc033d', '45a1168437c708ff']\")\n",
        "    trans_type2=trans.query(\"type2==['11a213398ee0c623']\")\n",
        "    trans_plat=trans.query(\"platform==['46c69cbbce5f1568','42573d7287a8c9c2']\")\n",
        "    trans_tunnel_in=trans.query(\"tunnel_in==['b2e7fa260df4998d','9c5701005a2d85bd']\")\n",
        "    df = df.merge(gen_user_group_features(df=trans_type2,col='type2',      value='amount'),on=['user'],how='left')\n",
        "    df = df.merge(gen_user_group_features(df=trans_type1,col='type1',      value='amount'),on=['user'],how='left')\n",
        "    df = df.merge(gen_user_group_features(df=trans_tunnel_in,col='tunnel_in',  value='amount'),on=['user'],how='left')\n",
        "    df = df.merge(gen_user_group_features(df=trans,col='tunnel_out',value='amount'), on=['user'], how='left')\n",
        "    df = df.merge(gen_user_group_features(df=trans_plat,col='platform',   value='amount'),on=['user'],how='left')\n",
        "    \n",
        "    print(\"trans group days_diff features\")\n",
        "    df = df.merge(gen_user_group_features(df=trans_type2,col='type2',      value='days_diff'),on=['user'],how='left')\n",
        "    df = df.merge(gen_user_group_features(df=trans_type1,col='type1',      value='days_diff'),on=['user'],how='left')\n",
        "    df = df.merge(gen_user_group_features(df=trans_tunnel_in,col='tunnel_in',  value='days_diff'),on=['user'],how='left')\n",
        "    df = df.merge(gen_user_group_features(df=trans,col='tunnel_out',value='days_diff'), on=['user'], how='left')\n",
        "    df = df.merge(gen_user_group_features(df=trans_plat,col='platform',   value='days_diff'),on=['user'],how='left')\n",
        "    \n",
        "    \n",
        "    print(\"trans group nunique features\")\n",
        "    df = df.merge(gen_user_group_nunique_features(df=trans,col='platform',   value='type1'),on=['user'],how='left')\n",
        "    df = df.merge(gen_user_group_nunique_features(df=trans,col='platform',   value='ip'),on=['user'],how='left')\n",
        "    \n",
        "    print(\"trans days window features\")\n",
        "    for window in tqdm([27,20,13,15]):\n",
        "        df = df.merge(gen_user_window_amount_features(df=trans, window=window), on=['user'], how='left')\n",
        "        \n",
        "    print(\"trans hour window features\")\n",
        "    for win_hour in tqdm([9,15,19,20]):\n",
        "        df = df.merge(gen_user_window_hour_amount_features(df=trans, window=win_hour), on=['user'], how='left')\n",
        "        \n",
        "    print(\"trans tfidf--count features\")    \n",
        "    df = df.merge(gen_user_tfidf_features(df=trans, value='type1'), on=['user'], how='left')\n",
        "    df = df.merge(gen_user_tfidf_features(df=trans, value='type2'), on=['user'], how='left')\n",
        "    \n",
        "    # op\n",
        "    print(\"op nuique features\")\n",
        "    for col in tqdm(['op_type','op_mode','channel']):\n",
        "        df = df.merge(gen_user_nunique_features(df=op, value=col, prefix='op'), on=['user'], how='left')\n",
        "\n",
        "    df=df.merge(op.groupby(\"user\").agg(user_op_days_diff_mean=('days_diff','mean')),on=['user'], how='left')\n",
        "    df=df.merge(op.groupby(\"user\").agg(user_op_hour_mean=('hour','mean')),on=['user'], how='left')\n",
        "    \n",
        "    print(\"op null features\")\n",
        "    for col in tqdm(['op_device', 'ip', 'net_type', 'channel', 'ip_3']):\n",
        "        df = df.merge(gen_user_null_features(df=op, value=col, prefix='op'), on=['user'], how='left')\n",
        "    \n",
        "    print(\"op days_diff features\")\n",
        "    df = df.merge(gen_user_status_features(op,'days_diff'), on=['user'], how='left')\n",
        "    df = df.merge(gen_user_status_features(op,'hour'), on=['user'], how='left')\n",
        "\n",
        "    print(\"op tfidf--cnt features\")\n",
        "    df = df.merge(gen_user_tfidf_features(df=op, value='op_mode'), on=['user'], how='left')\n",
        "    df = df.merge(gen_user_tfidf_features(df=op, value='op_type'), on=['user'], how='left')\n",
        "\n",
        "    df = df.merge(gen_user_countvec_features(df=op, value='op_mode'), on=['user'], how='left')\n",
        "    df = df.merge(gen_user_countvec_features(df=op, value='op_type'), on=['user'], how='left')\n",
        "\n",
        "    \n",
        "    df=df.merge(w2v_feat(op,[\"op_device\",\"ip\",\"channel\",\"net_type\",\"ip_3\"],\"train\"),on=['user'],how='left')\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def lgb_model_origin(train, target, test, k):\n",
        "    feats = [f for f in train.columns if f not in ['user', 'label']]\n",
        "    print('Current num of features:', len(feats))\n",
        "    folds = StratifiedKFold(n_splits=k, shuffle=True, random_state=2020)\n",
        "    oof_probs = np.zeros(train.shape[0])\n",
        "    output_preds = 0\n",
        "    offline_score = []\n",
        "    valid_score = []\n",
        "    train_score = []\n",
        "    feature_importance_df = pd.DataFrame()\n",
        "    parameters = {\n",
        "        'learning_rate': 0.03,\n",
        "        'boosting_type': 'gbdt',\n",
        "        'objective': 'binary',\n",
        "        'metric': 'auc',\n",
        "        'num_leaves': 50,\n",
        "        'feature_fraction': 0.8,\n",
        "        'bagging_fraction': 0.8,\n",
        "        'min_data_in_leaf': 20,\n",
        "        'reg_alpha':10,\n",
        "        'reg_lambda':8,\n",
        "        'verbose': -1,\n",
        "        'nthread': 8,\n",
        "        'colsample_bytree':0.77,\n",
        "        'min_child_weight':4,\n",
        "        'min_child_samples':10,\n",
        "        'min_split_gain':0,\n",
        "        'lambda_l1': 0.8,\n",
        "    }\n",
        "\n",
        "    for i, (train_index, test_index) in enumerate(folds.split(train, target)):\n",
        "        train_y, test_y = target[train_index], target[test_index]\n",
        "        train_X, test_X = train[feats].iloc[train_index, :], train[feats].iloc[test_index, :]\n",
        "        dtrain = lgb.Dataset(train_X,\n",
        "                             label=train_y)\n",
        "        dval = lgb.Dataset(test_X,\n",
        "                           label=test_y)\n",
        "        lgb_model = lgb.train(\n",
        "                parameters,\n",
        "                dtrain,\n",
        "                num_boost_round=5000,\n",
        "                valid_sets=[dval],\n",
        "                early_stopping_rounds=100,\n",
        "                verbose_eval=100,\n",
        "        )\n",
        "        oof_probs[test_index] = lgb_model.predict(test_X[feats], num_iteration=lgb_model.best_iteration)\n",
        "        output_preds += lgb_model.predict(test[feats], num_iteration=lgb_model.best_iteration)/folds.n_splits\n",
        "        \n",
        "        offline_score.append(lgb_model.best_score['valid_0']['auc'])\n",
        "        print(offline_score)\n",
        "        \n",
        "        # feature importance\n",
        "        fold_importance_df = pd.DataFrame()\n",
        "        fold_importance_df[\"feature\"] = feats\n",
        "        fold_importance_df[\"importance\"] = lgb_model.feature_importance(importance_type='gain')\n",
        "        fold_importance_df[\"fold\"] = i + 1\n",
        "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
        "\n",
        "    valid_auc = roc_auc_score(target, oof_probs)\n",
        "    print('OOF-MEAN-AUC:%.6f, OOF-STD-AUC:%.6f' % (np.mean(offline_score), np.std(offline_score)))\n",
        "    print(\"valid_auc:  {}\".format(valid_auc))\n",
        "    print('feature importance:')\n",
        "    print(feature_importance_df.groupby(['feature'])['importance'].mean().sort_values(ascending=False).head(40))\n",
        "\n",
        "    return output_preds, oof_probs, np.mean(offline_score),feature_importance_df.groupby(['feature'])['importance'].mean().sort_values(ascending=False)\n",
        "# This way we have randomness and are able to reproduce the behaviour within this cell.\n",
        "          "
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 95 µs, sys: 0 ns, total: 95 µs\n",
            "Wall time: 111 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-08-19T15:22:51.690540Z",
          "iopub.status.busy": "2020-08-19T15:22:51.688915Z",
          "iopub.status.idle": "2020-08-19T15:22:51.694107Z",
          "shell.execute_reply": "2020-08-19T15:22:51.693187Z"
        },
        "papermill": {
          "duration": 0.03518,
          "end_time": "2020-08-19T15:22:51.694304",
          "exception": false,
          "start_time": "2020-08-19T15:22:51.659124",
          "status": "completed"
        },
        "tags": [],
        "id": "H3uVf66OT-v0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def add_noise(series, noise_level):\n",
        "    return series * (1 + noise_level * np.random.randn(len(series)))\n",
        "\n",
        "def catboost_encoding(train,test, feature,k,sigma=1.0,a=1.0):\n",
        "    \n",
        "    kf = StratifiedKFold(n_splits=k, shuffle=True, random_state=2020)    \n",
        "    origin_features=[]\n",
        "    print(\"Target encoding\")\n",
        "    for col in tqdm(feature):\n",
        "        #train['target_'+col] = 0\n",
        "        #test['target_' + col] =0\n",
        "        train['catboost_'+col] = 0\n",
        "        test['catboost_' + col] =0\n",
        "        if col not in origin_features:\n",
        "            #smoothing=50\n",
        "            #noise_level=0.7\n",
        "            sigma=1.5\n",
        "            a=5.0\n",
        "        for trn_idx,val_idx in kf.split(train,train['label']):\n",
        "            X = train.iloc[trn_idx][col]\n",
        "            #enc_tar = TargetEncoder(smoothing=smoothing).fit(X,train.iloc[trn_idx]['label'])\n",
        "            enc_cat = CatBoostEncoder(sigma=sigma).fit(X,train.iloc[trn_idx]['label'])\n",
        "            #train['target_'+col] += add_noise(enc_tar.transform(train[col])[col],noise_level) / k\n",
        "            #test['target_'+col] += add_noise(enc_tar.transform(test[col])[col],noise_level) / k\n",
        "            train['catboost_'+col] += enc_cat.transform(train[col])[col] / k\n",
        "            test['catboost_'+col] += enc_cat.transform(test[col])[col] / k\n",
        "    print('Target encoding finish')\n",
        "    return  train,test\n",
        "            "
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-08-19T15:22:51.735832Z",
          "iopub.status.busy": "2020-08-19T15:22:51.734667Z",
          "iopub.status.idle": "2020-08-19T15:23:45.982721Z",
          "shell.execute_reply": "2020-08-19T15:23:45.983560Z"
        },
        "papermill": {
          "duration": 54.275124,
          "end_time": "2020-08-19T15:23:45.983938",
          "exception": false,
          "start_time": "2020-08-19T15:22:51.708814",
          "status": "completed"
        },
        "tags": [],
        "id": "AmJSj3JST-v3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "f0ac0f37-32a4-4e64-dd4c-c8d543d64876"
      },
      "source": [
        "%%time\n",
        "DATA_PATH = '../data/'\n",
        "print('读取数据...')\n",
        "base_df, op_df, trans_df = data_preprocess(DATA_PATH=DATA_PATH)\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "读取数据...\n",
            "CPU times: user 36.6 s, sys: 1 s, total: 37.6 s\n",
            "Wall time: 39.1 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-08-19T15:23:46.025032Z",
          "iopub.status.busy": "2020-08-19T15:23:46.024148Z",
          "iopub.status.idle": "2020-08-19T15:35:29.819983Z",
          "shell.execute_reply": "2020-08-19T15:35:29.820778Z"
        },
        "papermill": {
          "duration": 703.822693,
          "end_time": "2020-08-19T15:35:29.821306",
          "exception": false,
          "start_time": "2020-08-19T15:23:45.998613",
          "status": "completed"
        },
        "tags": [],
        "id": "a--Drq1iT-v6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 941
        },
        "outputId": "1f8c5d64-fd85-4f54-ad37-dad81f6bef46"
      },
      "source": [
        "%%time\n",
        "print('开始特征工程...')\n",
        "data = gen_features(base_df, op_df, trans_df)\n",
        "data.to_csv('../submission/data.csv')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "开始特征工程...\n",
            "base logistic features\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "trans nunique && null features\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 12%|█▎        | 1/8 [00:00<00:04,  1.67it/s]\u001b[A\n",
            " 25%|██▌       | 2/8 [00:01<00:03,  1.67it/s]\u001b[A\n",
            " 38%|███▊      | 3/8 [00:01<00:03,  1.65it/s]\u001b[A\n",
            " 50%|█████     | 4/8 [00:02<00:02,  1.65it/s]\u001b[A\n",
            " 62%|██████▎   | 5/8 [00:03<00:01,  1.60it/s]\u001b[A\n",
            " 75%|███████▌  | 6/8 [00:03<00:01,  1.58it/s]\u001b[A\n",
            " 88%|████████▊ | 7/8 [00:04<00:00,  1.54it/s]\u001b[A\n",
            "100%|██████████| 8/8 [00:05<00:00,  1.55it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "trans group amount features\n",
            "trans group days_diff features\n",
            "trans group nunique features\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "trans days window features\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 25%|██▌       | 1/4 [00:00<00:00,  3.75it/s]\u001b[A\n",
            " 50%|█████     | 2/4 [00:00<00:00,  3.47it/s]\u001b[A\n",
            " 75%|███████▌  | 3/4 [00:01<00:00,  3.09it/s]\u001b[A\n",
            "100%|██████████| 4/4 [00:01<00:00,  2.71it/s]\n",
            "\n",
            "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "trans hour window features\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 25%|██▌       | 1/4 [00:00<00:01,  2.07it/s]\u001b[A\n",
            " 50%|█████     | 2/4 [00:00<00:00,  2.23it/s]\u001b[A\n",
            " 75%|███████▌  | 3/4 [00:01<00:00,  2.52it/s]\u001b[A\n",
            "100%|██████████| 4/4 [00:01<00:00,  2.91it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "trans tfidf--count features\n",
            "op\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "op nuique features\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 33%|███▎      | 1/3 [00:01<00:02,  1.49s/it]\u001b[A\n",
            " 67%|██████▋   | 2/3 [00:03<00:01,  1.50s/it]\u001b[A\n",
            "100%|██████████| 3/3 [00:04<00:00,  1.51s/it]\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "op null features\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 20%|██        | 1/5 [00:00<00:03,  1.28it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:01<00:02,  1.33it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:02<00:01,  1.35it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:02<00:00,  1.37it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:03<00:00,  1.39it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "op days_diff features\n",
            "op tfidf--cnt features\n",
            "Start train word2vec ...\n",
            "Start gen feat of op_device ...\n",
            "Start gen feat of ip ...\n",
            "Start gen feat of channel ...\n",
            "Start gen feat of net_type ...\n",
            "CPU times: user 10min 10s, sys: 10.7 s, total: 10min 21s\n",
            "Wall time: 8min 8s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-08-19T15:35:29.891038Z",
          "iopub.status.busy": "2020-08-19T15:35:29.890234Z",
          "iopub.status.idle": "2020-08-19T15:35:29.894085Z",
          "shell.execute_reply": "2020-08-19T15:35:29.893460Z"
        },
        "papermill": {
          "duration": 0.040712,
          "end_time": "2020-08-19T15:35:29.894257",
          "exception": false,
          "start_time": "2020-08-19T15:35:29.853545",
          "status": "completed"
        },
        "tags": [],
        "id": "W3Za1veqT-v9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "target_encode_cols = [\n",
        "'user_trans_ip_count',   \n",
        "'product7_fail_ratio',\n",
        "'product7_fail_cnt',\n",
        "'user_amount_med_9h',\n",
        "'service3',\n",
        "'age',\n",
        "'login_cnt_period_CV',\n",
        "'product7_cnt',\n",
        "'svd_countvec_op_type_3',\n",
        "'svd_countvec_op_mode_3',\n",
        "'svd_tfidf_op_type_3',\n",
        "'using_time'\n",
        "]\n"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-08-19T15:35:29.966667Z",
          "iopub.status.busy": "2020-08-19T15:35:29.965272Z",
          "iopub.status.idle": "2020-08-19T15:51:54.155479Z",
          "shell.execute_reply": "2020-08-19T15:51:54.153689Z"
        },
        "papermill": {
          "duration": 984.229991,
          "end_time": "2020-08-19T15:51:54.156022",
          "exception": false,
          "start_time": "2020-08-19T15:35:29.926031",
          "status": "completed"
        },
        "tags": [],
        "id": "901opQNET-wA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "90069475-6e42-4cd5-d1e8-789426e7df7d"
      },
      "source": [
        "%%time\n",
        "print('开始模型训练...')\n",
        "train = data[~data['label'].isnull()].copy()\n",
        "target = train['label']\n",
        "test = data[data['label'].isnull()].copy()\n",
        "\n",
        "train,test = catboost_encoding(train,test,target_encode_cols,k=10)\n",
        "\n",
        "data = pd.concat([train, test], axis=0, ignore_index=True) \n",
        "for col in tqdm([f for f in data.select_dtypes('object').columns if f not in ['user']]):\n",
        "    le = LabelEncoder()\n",
        "    data[col].fillna(\"-1\", inplace=True)\n",
        "    data[col] = le.fit_transform(data[col])\n",
        "  \n",
        "train = data[~data['label'].isnull()].copy()\n",
        "test = data[data['label'].isnull()].copy()\n",
        "train.drop(['service3_level'], axis=1, inplace=True)\n",
        "test.drop(['service3_level'], axis=1, inplace=True)\n",
        "#data['city_level'] = data['city'].map(str) + '_' + data['level'].map(str)\n",
        "#data['city_product1_amount'] = data['city'].map(str) + '_' + data['product1_amount'].map(str)\n",
        "#data['city_balance1_avg'] = data['city'].map(str) + '_' + data['balance1_avg'].map(str)\n",
        "#data['city_balance2_avg'] = data['city'].map(str) + '_' + data['balance2_avg'].map(str)\n",
        "#data['city_balance_avg'] = data['city'].map(str) + '_' + data['balance_avg'].map(str)\n",
        "#train, test = kfold_stats_feature(train, test, target_encode_cols, 8)\n",
        "lgb_preds, lgb_oof, lgb_score,feature_importance_df = lgb_model_origin(train=train, target=target, test=test, k=10)\n",
        "\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "开始模型训练...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/12 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Target encoding\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  8%|▊         | 1/12 [00:01<00:18,  1.70s/it]\u001b[A\n",
            " 17%|█▋        | 2/12 [00:03<00:16,  1.66s/it]\u001b[A\n",
            " 25%|██▌       | 3/12 [00:05<00:15,  1.73s/it]\u001b[A\n",
            " 33%|███▎      | 4/12 [00:07<00:15,  1.89s/it]\u001b[A\n",
            " 42%|████▏     | 5/12 [00:10<00:16,  2.29s/it]\u001b[A\n",
            " 50%|█████     | 6/12 [00:12<00:12,  2.07s/it]\u001b[A\n",
            " 58%|█████▊    | 7/12 [00:13<00:09,  1.95s/it]\u001b[A\n",
            " 67%|██████▋   | 8/12 [00:15<00:07,  1.92s/it]\u001b[A\n",
            " 75%|███████▌  | 9/12 [00:17<00:05,  1.89s/it]\u001b[A\n",
            " 83%|████████▎ | 10/12 [00:19<00:03,  1.85s/it]\u001b[A\n",
            " 92%|█████████▏| 11/12 [00:20<00:01,  1.79s/it]\u001b[A\n",
            "100%|██████████| 12/12 [00:22<00:00,  1.87s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Target encoding finish\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
            " 16%|█▌        | 4/25 [00:00<00:00, 38.51it/s]\u001b[A\n",
            " 32%|███▏      | 8/25 [00:00<00:00, 37.49it/s]\u001b[A\n",
            " 48%|████▊     | 12/25 [00:00<00:00, 37.87it/s]\u001b[A\n",
            " 68%|██████▊   | 17/25 [00:00<00:00, 39.12it/s]\u001b[A\n",
            "100%|██████████| 25/25 [00:00<00:00, 40.77it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Current num of features: 710\n",
            "Training until validation scores don't improve for 100 rounds.\n",
            "[100]\tvalid_0's auc: 0.739815\n",
            "[200]\tvalid_0's auc: 0.741656\n",
            "[300]\tvalid_0's auc: 0.742479\n",
            "Early stopping, best iteration is:\n",
            "[237]\tvalid_0's auc: 0.743275\n",
            "[0.7432749533354487]\n",
            "Training until validation scores don't improve for 100 rounds.\n",
            "[100]\tvalid_0's auc: 0.743871\n",
            "[200]\tvalid_0's auc: 0.749926\n",
            "[300]\tvalid_0's auc: 0.749987\n",
            "Early stopping, best iteration is:\n",
            "[251]\tvalid_0's auc: 0.750709\n",
            "[0.7432749533354487, 0.7507087495503277]\n",
            "Training until validation scores don't improve for 100 rounds.\n",
            "[100]\tvalid_0's auc: 0.725078\n",
            "[200]\tvalid_0's auc: 0.73272\n",
            "[300]\tvalid_0's auc: 0.733731\n",
            "Early stopping, best iteration is:\n",
            "[254]\tvalid_0's auc: 0.734498\n",
            "[0.7432749533354487, 0.7507087495503277, 0.7344979720807395]\n",
            "Training until validation scores don't improve for 100 rounds.\n",
            "[100]\tvalid_0's auc: 0.73328\n",
            "[200]\tvalid_0's auc: 0.738394\n",
            "[300]\tvalid_0's auc: 0.73782\n",
            "Early stopping, best iteration is:\n",
            "[200]\tvalid_0's auc: 0.738394\n",
            "[0.7432749533354487, 0.7507087495503277, 0.7344979720807395, 0.7383939351065837]\n",
            "Training until validation scores don't improve for 100 rounds.\n",
            "[100]\tvalid_0's auc: 0.71958\n",
            "[200]\tvalid_0's auc: 0.725322\n",
            "[300]\tvalid_0's auc: 0.726278\n",
            "[400]\tvalid_0's auc: 0.727245\n",
            "[500]\tvalid_0's auc: 0.727479\n",
            "Early stopping, best iteration is:\n",
            "[478]\tvalid_0's auc: 0.727807\n",
            "[0.7432749533354487, 0.7507087495503277, 0.7344979720807395, 0.7383939351065837, 0.7278065459347293]\n",
            "Training until validation scores don't improve for 100 rounds.\n",
            "[100]\tvalid_0's auc: 0.728007\n",
            "[200]\tvalid_0's auc: 0.733166\n",
            "[300]\tvalid_0's auc: 0.732344\n",
            "Early stopping, best iteration is:\n",
            "[208]\tvalid_0's auc: 0.733706\n",
            "[0.7432749533354487, 0.7507087495503277, 0.7344979720807395, 0.7383939351065837, 0.7278065459347293, 0.7337061403508772]\n",
            "Training until validation scores don't improve for 100 rounds.\n",
            "[100]\tvalid_0's auc: 0.726926\n",
            "[200]\tvalid_0's auc: 0.73296\n",
            "[300]\tvalid_0's auc: 0.734521\n",
            "[400]\tvalid_0's auc: 0.734967\n",
            "Early stopping, best iteration is:\n",
            "[360]\tvalid_0's auc: 0.735345\n",
            "[0.7432749533354487, 0.7507087495503277, 0.7344979720807395, 0.7383939351065837, 0.7278065459347293, 0.7337061403508772, 0.7353454536879834]\n",
            "Training until validation scores don't improve for 100 rounds.\n",
            "[100]\tvalid_0's auc: 0.725674\n",
            "[200]\tvalid_0's auc: 0.731786\n",
            "[300]\tvalid_0's auc: 0.73382\n",
            "[400]\tvalid_0's auc: 0.734199\n",
            "Early stopping, best iteration is:\n",
            "[356]\tvalid_0's auc: 0.734701\n",
            "[0.7432749533354487, 0.7507087495503277, 0.7344979720807395, 0.7383939351065837, 0.7278065459347293, 0.7337061403508772, 0.7353454536879834, 0.7347007640067912]\n",
            "Training until validation scores don't improve for 100 rounds.\n",
            "[100]\tvalid_0's auc: 0.714871\n",
            "[200]\tvalid_0's auc: 0.721491\n",
            "[300]\tvalid_0's auc: 0.721547\n",
            "[400]\tvalid_0's auc: 0.722486\n",
            "Early stopping, best iteration is:\n",
            "[350]\tvalid_0's auc: 0.72261\n",
            "[0.7432749533354487, 0.7507087495503277, 0.7344979720807395, 0.7383939351065837, 0.7278065459347293, 0.7337061403508772, 0.7353454536879834, 0.7347007640067912, 0.7226101207319374]\n",
            "Training until validation scores don't improve for 100 rounds.\n",
            "[100]\tvalid_0's auc: 0.736859\n",
            "[200]\tvalid_0's auc: 0.742972\n",
            "[300]\tvalid_0's auc: 0.743843\n",
            "Early stopping, best iteration is:\n",
            "[280]\tvalid_0's auc: 0.744405\n",
            "[0.7432749533354487, 0.7507087495503277, 0.7344979720807395, 0.7383939351065837, 0.7278065459347293, 0.7337061403508772, 0.7353454536879834, 0.7347007640067912, 0.7226101207319374, 0.7444048292774949]\n",
            "OOF-MEAN-AUC:0.736545, OOF-STD-AUC:0.007738\n",
            "valid_auc:  0.7363259695344222\n",
            "feature importance:\n",
            "feature\n",
            "user_trans_ip_count                                 20520.190911\n",
            "user_days_diff_sum_x                                 7830.014270\n",
            "product7_fail_ratio                                  6527.370992\n",
            "w2c_channel_0_max                                    4963.595611\n",
            "user_amount_med_9h                                   3485.007788\n",
            "product7_fail_cnt                                    2905.787564\n",
            "age                                                  2518.729093\n",
            "user_trans_ip_3_count                                2371.000965\n",
            "login_cnt_didive                                     2142.578244\n",
            "service3                                             2064.163464\n",
            "product7_cnt                                         2019.020011\n",
            "user_group_type1_amount_45a1168437c708ff_sum         1917.523355\n",
            "catboost_product7_fail_ratio                         1694.147166\n",
            "svd_countvec_op_type_3                               1690.404995\n",
            "agreement3                                           1643.642592\n",
            "svd_tfidf_type2_3                                    1620.145703\n",
            "svd_tfidf_type1_1                                    1527.890063\n",
            "svd_countvec_op_mode_3                               1406.410611\n",
            "w2c_channel_7_std                                    1362.589635\n",
            "svd_countvec_op_mode_2                               1348.360975\n",
            "svd_tfidf_op_type_3                                  1327.010555\n",
            "user_amount_med                                      1206.761639\n",
            "svd_countvec_op_mode_4                               1179.241109\n",
            "user_group_type1_amount_45a1168437c708ff_nunique     1168.737486\n",
            "user_group_type1_days_diff_45a1168437c708ff_mean     1159.025747\n",
            "user_group_type1_amount_45a1168437c708ff_count       1117.722543\n",
            "w2c_channel_0_std                                    1115.088814\n",
            "card_d_cnt                                           1103.013129\n",
            "catboost_user_trans_ip_count                         1094.739838\n",
            "op_cnt_per_usign_time                                1074.282463\n",
            "ip_cnt_per_using_time                                1061.288291\n",
            "city_null_ip_amount_percent                          1060.363988\n",
            "province                                             1054.360494\n",
            "user_group_type1_days_diff_45a1168437c708ff_min      1051.076381\n",
            "user_op_op_device_null_ratio                         1047.975044\n",
            "login_cnt_period_CV                                  1043.500283\n",
            "login_cnt_period_var                                  955.083877\n",
            "city_user_trans_ip_count_mea                          928.832455\n",
            "user_trans_type1_nuniq                                922.116322\n",
            "user_hour_min_y                                       922.076183\n",
            "Name: importance, dtype: float64\n",
            "CPU times: user 43min 34s, sys: 1min 12s, total: 44min 47s\n",
            "Wall time: 24min 29s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6On0jrVlWsj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sub_df = test[['user']].copy()\n",
        "sub_df['prob'] = lgb_preds\n",
        "sub_df.to_csv('../submission/sub{%.5f}.csv'%(np.mean(lgb_score)), index=False)\n",
        "feature_importance_df.to_csv('feature_importance.csv')"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.069408,
          "end_time": "2020-08-19T16:13:12.753456",
          "exception": false,
          "start_time": "2020-08-19T16:13:12.684048",
          "status": "completed"
        },
        "tags": [],
        "id": "vA3dKKZ0T-wJ",
        "colab_type": "text"
      },
      "source": [
        "# ---分界线---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.075459,
          "end_time": "2020-08-19T16:13:12.904278",
          "exception": false,
          "start_time": "2020-08-19T16:13:12.828819",
          "status": "completed"
        },
        "tags": [],
        "id": "1LtBisryT-wJ",
        "colab_type": "text"
      },
      "source": [
        "## 测试target encoder是否有效"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-08-19T16:13:32.192370Z",
          "iopub.status.busy": "2020-08-19T16:13:32.177618Z",
          "iopub.status.idle": "2020-08-19T16:13:32.438311Z",
          "shell.execute_reply": "2020-08-19T16:13:32.437342Z"
        },
        "papermill": {
          "duration": 0.347508,
          "end_time": "2020-08-19T16:13:32.438488",
          "exception": false,
          "start_time": "2020-08-19T16:13:32.090980",
          "status": "completed"
        },
        "tags": [],
        "id": "iIVfU9_eT-wM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "train_base['city_balance_avg'] = train_base['city'].map(str) + '_' + train_base['balance_avg'].map(str)\n",
        "train_base['city_level'] = train_base['city'].map(str) + '_' + train_base['level'].map(str)\n",
        "train_base['city_product1_amount'] = train_base['city'].map(str) + '_' + train_base['product1_amount'].map(str)\n",
        "train_base['city_balance1_avg'] = train_base['city'].map(str) + '_' + train_base['balance1_avg'].map(str)\n",
        "train_base['city_balance2_avg'] = train_base['city'].map(str) + '_' + train_base['balance2_avg'].map(str)\n",
        "\n",
        "test_base['city_balance_avg'] =  test_base['city'].map(str) + '_' + test_base['balance_avg'].map(str)\n",
        "test_base['city_level'] =        test_base['city'].map(str) + '_' + test_base['level'].map(str)\n",
        "test_base['city_product1_amount'] = test_base['city'].map(str) + '_' + test_base['product1_amount'].map(str)\n",
        "test_base['city_balance1_avg'] = test_base['city'].map(str) + '_' + test_base['balance1_avg'].map(str)\n",
        "test_base['city_balance2_avg'] = test_base['city'].map(str) + '_' + test_base['balance2_avg'].map(str)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.251053,
          "end_time": "2020-08-19T16:13:32.763767",
          "exception": false,
          "start_time": "2020-08-19T16:13:32.512714",
          "status": "completed"
        },
        "tags": [],
        "id": "P6kxfZUzT-wO",
        "colab_type": "text"
      },
      "source": [
        "### 自己定义的target encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.077632,
          "end_time": "2020-08-19T16:18:07.409279",
          "exception": false,
          "start_time": "2020-08-19T16:18:07.331647",
          "status": "completed"
        },
        "tags": [],
        "id": "xk7eCAMrT-wX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}