{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nimport pandas as pd\nimport numpy as np\nimport warnings\nfrom sklearn.preprocessing import LabelEncoder\nimport lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.decomposition import TruncatedSVD,PCA\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import roc_auc_score\nfrom gensim.models import Word2Vec, FastText\nfrom gensim import corpora,models\nfrom sklearn import preprocessing\nfrom tqdm import tqdm\nimport multiprocessing\nwarnings.filterwarnings('ignore')\nprint(multiprocessing.cpu_count())\nimport gc\nfrom category_encoders import *\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nwarnings.filterwarnings('ignore')\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n## You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\norigin_features = []\ndrop_list=[]\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode_LE(data,test):\n    for f in tqdm([f for f in data.select_dtypes('object').columns if f not in ['user','label']]):\n        le = LabelEncoder()\n        \n        le.fit(list(data[f].values) + list(test[f].values))\n        data[f] = le.transform(list(data[f].values))\n        test[f] = le.transform(list(test[f].values))\n        \n    return data,test\n        \ndef encode_CB(df,col1_list,col2_list):\n    uids=[]\n    for col1 in col1_list:\n        for col2 in col2_list:\n            name = col1+'_'+col2\n            uids.append(name)\n            df[name] = df[col1].astype(str)+'_'+df[col2].astype(str)\n            df[name+'_FE'] = df[name].map(df[name].value_counts(dropna=True,normalize=True))\n            df[name+'_FE'] = df[name+\"_FE\"].astype('float32')\n    return df,uids\n\ndef encode_Cat(train,test, k,sigma=2.0,a=2.0):\n    \n    kf = StratifiedKFold(n_splits=k, shuffle=True, random_state=2020)    \n    \n    print(\"Cat encoding\")\n    for col in tqdm([f for f in data.select_dtypes('object').columns if f not in ['user','label']]):\n        train['catboost_'+col] = 0\n        test['catboost_' + col] =0\n        if col not in origin_features:\n            sigma=10\n            a=20\n        for trn_idx,val_idx in kf.split(train,train['label']):\n            X = train.iloc[trn_idx][col]\n            enc_cat = CatBoostEncoder(sigma=sigma).fit(X,train.iloc[trn_idx]['label'])\n            train['catboost_'+col] += enc_cat.transform(train[col])[col] / k\n            test['catboost_'+col] += enc_cat.transform(test[col])[col] / k\n    print('Cat encoding finish')\n    return  train,test\n            ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"%%time\ndef gen_user_status_features(df,index,value,aggregations=['mean','std','min','max','sum']):\n        \n    group_df = df.groupby([index])[value].agg(aggregations).reset_index() \n    if('mean' in aggregations and 'std' in aggregations):\n        group_df['CV']=group_df['std'] / group_df['mean']\n    group_df.rename(columns={i:\"{}_{}_\".format(index,value)+i for i in group_df.columns if i not in[index]},\n                   inplace=True\n                   )\n                \n    return group_df\n\n\ndef gen_user_group_features(df, col,value,aggregations=['mean','std','min','max','sum']):  \n    group_df = df.pivot_table(index='user',\n                              columns=col,\n                              values=value,\n                              dropna=False,\n                              aggfunc=aggregations).fillna(0)\n    cols=group_df.columns\n    group_df.columns = ['user_group_{}_{}_{}_{}'.format(col,value, f[1], f[0]) for f in group_df.columns]\n    for f in cols:\n        group_df['user_group_{}_{}_{}_CV'.format(col,value,f[1])] = group_df['user_group_{}_{}_{}_std'.format(col,value,f[1])] / group_df['user_group_{}_{}_{}_mean'.format(col,value,f[1])]\n    group_df.reset_index(inplace=True)\n    return group_df\n\ndef gen_user_group_nunique_features(df, col,value):\n    \n    group_df = df.pivot_table(index='user',\n                              columns=col,\n                              values=value,\n                              dropna=False,\n                              aggfunc=['count','nunique']).fillna(0)\n    group_df.columns = ['user_{}_{}_{}_{}'.format(col,value, f[1], f[0]) for f in group_df.columns]\n    group_df.reset_index(inplace=True)\n    return group_df\n\ndef gen_user_nunique_features(df, value, prefix):\n    a=\"user_{}_{}_nuniq\".format(prefix, value)\n    b=\"user_{}_{}_count\".format(prefix, value)\n    group_df = df.groupby(['user'])[value].agg(\n        a='nunique',b='count'\n    ).reset_index()\n    \n    group_df.rename(columns={'a':a,'b':b},inplace=True) \n    return group_df\n\ndef gen_user_null_features(df, value, prefix):\n    df['is_null'] = 0\n    df.loc[df[value].isnull(), 'is_null'] = 1\n\n    group_df = df.groupby(['user'])['is_null'].agg(a='sum',\n                                                    b='mean').reset_index()\n    \n    group_df.rename(columns={'a':'user_{}_{}_null_cnt'.format(prefix, value),\n                    'b':'user_{}_{}_null_ratio'.format(prefix, value)},\n                   inplace=True\n                   )\n\n    return group_df\n\n    \ndef gen_user_window_amount_features(df, window):\n    \n    group_df = df[df['days_diff']>window].groupby('user')['amount'].agg(\n        a='mean',\n         b='std',\n         c='max',\n         d='min',\n         e='sum',\n        f='median',\n         g='count',\n        ).reset_index()\n    \n    group_df.rename(columns={'a':'user_amount_mean_{}d'.format(window),\n                    'b':'user_amount_std_{}d'.format(window),\n                    'c':'user_amount_max_{}d'.format(window),\n                    'd':'user_amount_min_{}d'.format(window),\n                    'e':'user_amount_sum_{}d'.format(window),\n                    'f':'user_amount_med_{}d'.format(window),\n                    'g':'user_amount_cnt_{}d'.format(window)},\n                   inplace=True\n                   )\n    \n    return group_df\n\ndef gen_user_window_hour_amount_features(df, window):\n    \n    group_df = df[df['hour']>window].groupby('user')['amount'].agg(\n        a='mean',\n         b='std',\n         c='max',\n         d='min',\n         e='sum',\n         f='median',\n         g='count',\n        ).reset_index()\n    \n    group_df.rename(columns={'a':'user_amount_mean_{}h'.format(window),\n                    'b':'user_amount_std_{}h'.format(window),\n                    'c':'user_amount_max_{}h'.format(window),\n                    'd':'user_amount_min_{}h'.format(window),\n                    'e':'user_amount_sum_{}h'.format(window),\n                    'f':'user_amount_med_{}h'.format(window),\n                    'g':'user_amount_cnt_{}h'.format(window)},\n                   inplace=True\n                   )\n    \n    return group_df\n\n\ndef gen_user_tfidf_features(df, value):\n    #填充缺失值\n    df[value].replace(' ', np.nan, inplace=True)\n    df[value] = df[value].astype(str)\n    df[value].fillna('-1', inplace=True)\n\n    #\n    group_df = df.groupby(['user']).apply(lambda x: x[value].tolist()).reset_index()\n    group_df.columns = ['user', 'list']\n    group_df['list'] = group_df['list'].apply(lambda x: ','.join(x))\n    enc_vec = TfidfVectorizer()\n    tfidf_vec = enc_vec.fit_transform(group_df['list'])\n    svd_enc = TruncatedSVD(n_components=10, n_iter=20, random_state=2020)\n    vec_svd = svd_enc.fit_transform(tfidf_vec)\n    vec_svd = pd.DataFrame(vec_svd)\n    vec_svd.columns = ['svd_tfidf_{}_{}'.format(value, i) for i in range(10)]\n    group_df = pd.concat([group_df, vec_svd], axis=1)\n    del group_df['list']\n    return group_df\n\ndef gen_user_countvec_features(df, value):\n    df[value] = df[value].astype(str)\n    df[value].fillna('-1', inplace=True)\n    group_df = df.groupby(['user']).apply(lambda x: x[value].tolist()).reset_index()\n    group_df.columns = ['user', 'list']\n    group_df['list'] = group_df['list'].apply(lambda x: ','.join(x))\n    enc_vec = CountVectorizer()\n    tfidf_vec = enc_vec.fit_transform(group_df['list'])\n    svd_enc = TruncatedSVD(n_components=5, n_iter=20, random_state=2020)\n    vec_svd = svd_enc.fit_transform(tfidf_vec)\n    vec_svd = pd.DataFrame(vec_svd)\n    vec_svd.columns = ['svd_countvec_{}_{}'.format(value, i) for i in range(5)]\n    group_df = pd.concat([group_df, vec_svd], axis=1)\n    del group_df['list']\n    return group_df\n\n#定义加载函数\ndef load_dataset(DATA_PATH):\n    train_label = pd.read_csv(DATA_PATH+'train_label.csv')\n    test_base = pd.read_csv(DATA_PATH+'testb_base.csv')\n    \n    train_base = pd.read_csv(DATA_PATH+'train_base.csv')\n    train_op = pd.read_csv(DATA_PATH+'train_op.csv')\n    train_trans = pd.read_csv(DATA_PATH+'train_trans.csv')\n    test_op = pd.read_csv(DATA_PATH+'testb_op.csv')\n    test_trans = pd.read_csv(DATA_PATH+'testb_trans.csv')\n    \n    origin_features = list(train_base.columns)+list(train_trans.columns)+list(train_op.columns)\n    return train_label, train_base, test_base, train_op, train_trans, test_op, test_trans\n\n#tran_trans和train_op文件中的tm_diff时间转换\ndef transform_time(x):\n    day = int(x.split(' ')[0])\n    hour = int(x.split(' ')[2].split('.')[0].split(':')[0])\n    minute = int(x.split(' ')[2].split('.')[0].split(':')[1])\n    second = int(x.split(' ')[2].split('.')[0].split(':')[2])\n    return 86400*day+3600*hour+60*minute+second\n\n\n\n#讲所有的数据进行初步的整合，并且把时间days_diff进行了数字化提取\ndef data_preprocess(DATA_PATH):\n    train_label, train_base, test_base, train_op, train_trans, test_op, test_trans = load_dataset(DATA_PATH=DATA_PATH)\n    # 拼接数据\n    train_df = train_base.copy()\n    test_df = test_base.copy()\n        #将train_base数据和train_label整合\n    train_df = train_label.merge(train_df, on=['user'], how='left')\n    del train_base, test_base\n        #将train_op数据和test_op整合--方便将train和test中所有的数据进行统一的处理\n    op_df = pd.concat([train_op, test_op], axis=0, ignore_index=True)\n        #将train_trans数据和test_trans整合--方便将train和test中所有的数据进行统一的处理\n    trans_df = pd.concat([train_trans, test_trans], axis=0, ignore_index=True)\n        #将train_base数据和test_base整合\n    data = pd.concat([train_df, test_df], axis=0, ignore_index=True) \n    del train_op, test_op, train_df, test_df\n    \n    \n    # 时间维度的处理\n    op_df['days_diff'] = op_df['tm_diff'].apply(lambda x: int(x.split(' ')[0]))\n    trans_df['days_diff'] = trans_df['tm_diff'].apply(lambda x: int(x.split(' ')[0]))\n    \n    op_df['timestamp'] = op_df['tm_diff'].apply(lambda x: transform_time(x))\n    trans_df['timestamp'] = trans_df['tm_diff'].apply(lambda x: transform_time(x))\n    op_df['hour'] = op_df['tm_diff'].apply(lambda x: int(x.split(' ')[2].split('.')[0].split(':')[0]))\n    trans_df['hour'] = trans_df['tm_diff'].apply(lambda x: int(x.split(' ')[2].split('.')[0].split(':')[0]))\n    trans_df['week'] = trans_df['days_diff'].apply(lambda x: x % 7)\n    \n    \n    # 排序\n    trans_df = trans_df.sort_values(by=['user', 'timestamp'])\n    op_df = op_df.sort_values(by=['user', 'timestamp'])\n    trans_df.reset_index(inplace=True, drop=True)\n    op_df.reset_index(inplace=True, drop=True)\n\n    gc.collect()\n    return data, op_df, trans_df\n\ndef gen_features(df_, op, trans):\n    df=df_\n    # base数据处理\n    print(\"base logistic features\")\n    df['card_a_cnt_avg'] = df[\"card_a_cnt\"]/df[\"using_time\"]\n    df['product7_fail_ratio'] = df['product7_fail_cnt'] / df['product7_cnt']\n    \n    df['city_count'] = df.groupby(['city'])['user'].transform('count')  ## 与原始数据集相同数量的项目\n    df['city_product7_fail_ratio'] =  df.groupby(['city'])['product7_fail_cnt'].transform('sum')/df.groupby(['city'])['product7_cnt'].transform('sum')\n    df['province_count'] = df.groupby(['province'])['user'].transform('count')\n    df['province_product7_fail_ratio'] = df.groupby(['province'])['product7_fail_cnt'].transform('sum')/df.groupby(['province'])['product7_cnt'].transform('sum')\n    \n    df['login_cnt_period_var']=df[['login_cnt_period1','login_cnt_period2']].std(axis=1)\n    df['login_cnt_period_CV']=df['login_cnt_period_var'] / df[['login_cnt_period1','login_cnt_period2']].mean(axis=1)\n    df['card_var']=df[['card_a_cnt','card_b_cnt','card_c_cnt','card_d_cnt']].std(axis=1)\n    df['card_CV']=df['card_var'] / df[['card_a_cnt','card_b_cnt','card_c_cnt','card_d_cnt']].mean(axis=1)\n    df['op_cnt_var']=df[['op1_cnt','op2_cnt']].std(axis=1)\n    df['op_cnt_CV']=df['op_cnt_var'] / df[['op1_cnt','op2_cnt']].mean(axis=1)\n    df['login_var']=df[['login_cnt_period1','login_cnt_period2','login_cnt_avg']].std(axis=1)\n    df['login_CV']=df['login_var'] / df[['login_cnt_period1','login_cnt_period2','login_cnt_avg']].mean(axis=1)\n    \n    \n    df['ip_cnt_per_login_cnt_avg'] = df['ip_cnt']/df[['login_cnt_period1','login_cnt_period2','login_cnt_avg']].mean(axis=1)\n    df['ip_cnt_per_login_days_cnt'] = df['ip_cnt']/df['login_days_cnt']\n    df['acc_count_per_time'] = df[\"acc_count\"]/df[\"using_time\"]\n    df['ip_cnt_per_using_time'] = df[\"ip_cnt\"]/df[\"using_time\"]\n    df['agreement_total_per_using_time'] = df[\"agreement_total\"]/df[\"using_time\"]\n    df['agreement_total_per_account_cnt'] = df[\"agreement_total\"]/df[\"acc_count\"]\n    df['ip_cnt_per_acc_count'] = df[\"ip_cnt\"]/df[\"acc_count\"]\n    df['service1_amt_per_using_time'] = df[\"service1_amt\"]/df[\"using_time\"]\n    df[\"service_cnt_per_time\"]=(df[\"service1_cnt\"]+df[\"service2_cnt\"])/df[\"using_time\"]\n    \n    \n    df['op_cnt_per_usign_time'] = (df[\"op1_cnt\"]+df[\"op2_cnt\"])/df[\"using_time\"]\n    df['op_cnt_per_login_cnt_avg'] = (df[\"op1_cnt\"]+df[\"op2_cnt\"])/df['login_cnt_avg']\n    df['op_cnt_per_login_cnt_p1'] =  (df[\"op1_cnt\"]+df[\"op2_cnt\"])/df['login_cnt_period1']\n    df['op_cnt_per_login_cnt_p2'] =  (df[\"op1_cnt\"]+df[\"op2_cnt\"])/df['login_cnt_period2']\n    \n    # trans\n    df = df.merge(gen_user_status_features(trans,'user','amount'), on=['user'], how='left')\n    df = df.merge(gen_user_status_features(trans,'user','days_diff'), on=['user'], how='left')\n    df = df.merge(gen_user_status_features(trans,'user','hour'), on=['user'], how='left')\n\n  \n    print(\"trans nunique && null features\")\n    for col in tqdm(['days_diff', 'platform', 'tunnel_in', 'tunnel_out', 'type1', 'type2','ip', 'ip_3']):\n        df = df.merge(gen_user_nunique_features(df=trans, value=col, prefix='trans'), on=['user'], how='left')\n        df = df.merge(gen_user_null_features(df=trans, value=col, prefix='trans'), on=['user'], how='left')\n    df['user_amount_per_days'] = df['user_amount_sum'].div(df['user_trans_days_diff_nuniq'])\n    df['user_amount_per_tunnel_in'] = df['user_amount_sum'] / df['user_trans_tunnel_in_nuniq']\n    df['user_amount_per_ip_cnt']=df['user_amount_sum']/df['ip_cnt']\n    temp=trans[trans['ip'].isnull()].groupby(['user'])['amount'].agg(user_null_ip_amount_sum='sum',\n                                                                     user_null_ip_amount_mea='mean'\n                                                                    \n                                                                    ).reset_index()\n    \n    df=df.merge(temp,on=['user'],how='left')\n\n            \n    print(\"trans group amount features\")\n    trans_type1=trans.query(\"type1==['f67d4b5a05a1352a','19d44f1a51919482','674e8d5860bc033d', '45a1168437c708ff']\")\n    trans_type2=trans.query(\"type2==['11a213398ee0c623']\")\n    trans_plat=trans.query(\"platform==['46c69cbbce5f1568','42573d7287a8c9c2']\")\n    trans_tunnel_in=trans.query(\"tunnel_in==['b2e7fa260df4998d','9c5701005a2d85bd']\")\n    df = df.merge(gen_user_group_features(df=trans_type2,col='type2',      value='amount'),on=['user'],how='left')\n    df = df.merge(gen_user_group_features(df=trans_type1,col='type1',      value='amount'),on=['user'],how='left')\n    df = df.merge(gen_user_group_features(df=trans_tunnel_in,col='tunnel_in',  value='amount'),on=['user'],how='left')\n    df = df.merge(gen_user_group_features(df=trans,col='tunnel_out',value='amount'), on=['user'], how='left')\n    df = df.merge(gen_user_group_features(df=trans_plat,col='platform',   value='amount'),on=['user'],how='left')\n    \n    print(\"trans group days_diff features\")\n    df = df.merge(gen_user_group_features(df=trans_type2,col='type2',      value='days_diff'),on=['user'],how='left')\n    df = df.merge(gen_user_group_features(df=trans_type1,col='type1',      value='days_diff'),on=['user'],how='left')\n    df = df.merge(gen_user_group_features(df=trans_tunnel_in,col='tunnel_in',  value='days_diff'),on=['user'],how='left')\n    df = df.merge(gen_user_group_features(df=trans,col='tunnel_out',value='days_diff'), on=['user'], how='left')\n    df = df.merge(gen_user_group_features(df=trans_plat,col='platform',   value='days_diff'),on=['user'],how='left')\n    \n    \n    print(\"trans group nunique features\")\n    df = df.merge(gen_user_group_nunique_features(df=trans,col='platform',   value='type1'),on=['user'],how='left')\n    df = df.merge(gen_user_group_nunique_features(df=trans,col='platform',   value='ip'),on=['user'],how='left')\n    \n    print(\"trans days window features\")\n    for window in tqdm([20,13,15]):\n        df = df.merge(gen_user_window_amount_features(df=trans, window=window), on=['user'], how='left')\n        \n    print(\"trans hour window features\")\n    for win_hour in tqdm([9,18,19,20]):\n        df = df.merge(gen_user_window_hour_amount_features(df=trans, window=win_hour), on=['user'], how='left')\n        \n    print(\"trans tfidf--count features\")    \n    df = df.merge(gen_user_tfidf_features(df=trans, value='type1'), on=['user'], how='left')\n    df = df.merge(gen_user_tfidf_features(df=trans, value='type2'), on=['user'], how='left')\n    \n    print(\"trans LSI features\")\n    #for col in ['ip','platform','tunnel_in','tunnel_out','ip_3','type1','type2']:\n    #    df=df.merge(gen_user_lda_features(trans,col,5),on=['user'],how='left')\n        \n    #df=df.merge(fasttext(trans,['ip','type1',\"type2\",'tunnel_out','tunnel_in'],\"train\"),on=['user'],how='left')\n    # op\n    print(\"op nuique features\")\n    for col in tqdm(['op_type','op_mode','channel']):\n        df = df.merge(gen_user_nunique_features(df=op, value=col, prefix='op'), on=['user'], how='left')\n    \n    print(\"op null features\")\n    for col in tqdm(['op_device', 'ip', 'net_type', 'channel', 'ip_3']):\n        df = df.merge(gen_user_null_features(df=op, value=col, prefix='op'), on=['user'], how='left')\n    \n    print(\"op days_diff features\")\n    df = df.merge(gen_user_status_features(op,'user','days_diff'), on=['user'], how='left')\n    df = df.merge(gen_user_status_features(op,'user','hour'), on=['user'], how='left')\n\n    print(\"op tfidf--cnt features\")\n    df = df.merge(gen_user_tfidf_features(df=op, value='op_mode'), on=['user'], how='left')\n    df = df.merge(gen_user_tfidf_features(df=op, value='op_type'), on=['user'], how='left')\n    df = df.merge(gen_user_tfidf_features(df=op, value='op_device'), on=['user'], how='left')\n    df = df.merge(gen_user_countvec_features(df=op, value='op_mode'), on=['user'], how='left')\n    df = df.merge(gen_user_countvec_features(df=op, value='op_type'), on=['user'], how='left')\n    df = df.merge(gen_user_countvec_features(df=op, value='op_device'), on=['user'], how='left')\n    \n    print(\"op LSI features\")\n    #for col in [\"op_device\",\"ip\",\"channel\",\"net_type\",\"op_type\",\"op_mode\"]:\n    #    df=df.merge(gen_user_lda_features(op,col,5),on=['user'],how='left')\n    #df=df.merge(fasttext(op,['op_mode','op_type',\"op_device\",\"channel\",'ip_3',],\"train\"),on=['user'],how='left')\n    return df\ndef kfold_stats_feature(train, test, feats, k):\n    folds = StratifiedKFold(n_splits=k, shuffle=True, random_state=2020)  # 这里最好和后面模型的K折交叉验证保持一致\n\n    train['fold'] = None\n    for fold_, (trn_idx, val_idx) in enumerate(folds.split(train, train['label'])):\n        train.loc[val_idx, 'fold'] = fold_\n\n    kfold_features = []\n    for feat in feats:\n        nums_columns = ['label']\n        for f in nums_columns:\n            colname = feat + '_' + f + '_kfold_mean'\n            kfold_features.append(colname)\n            train[colname] = None\n            for fold_, (trn_idx, val_idx) in enumerate(folds.split(train, train['label'])):\n                tmp_trn = train.iloc[trn_idx]\n                order_label = tmp_trn.groupby([feat])[f].mean()\n                tmp = train.loc[train.fold == fold_, [feat]]\n                train.loc[train.fold == fold_, colname] = tmp[feat].map(order_label)\n                # fillna\n                global_mean = train[f].mean()\n                train.loc[train.fold == fold_, colname] = train.loc[train.fold == fold_, colname].fillna(global_mean)\n            train[colname] = train[colname].astype(float)\n\n        for f in nums_columns:\n            colname = feat + '_' + f + '_kfold_mean'\n            test[colname] = None\n            order_label = train.groupby([feat])[f].mean()\n            test[colname] = test[feat].map(order_label)\n            # fillna\n            global_mean = train[f].mean()\n            test[colname] = test[colname].fillna(global_mean)\n            test[colname] = test[colname].astype(float)\n    del train['fold']\n    return train, test\n\ndef lgb_model_origin(train, target, test, k):\n    feats = [f for f in train.columns if f not in ['user', 'label']]\n    print('Current num of features:', len(feats))\n    folds = StratifiedKFold(n_splits=k, shuffle=True, random_state=2020)\n    oof_probs = np.zeros(train.shape[0])\n    output_preds = 0\n    offline_score = []\n    valid_score = []\n    train_score = []\n    feature_importance_df = pd.DataFrame()\n    parameters = {\n        'learning_rate': 0.01,\n        'boosting_type': 'gbdt',\n        'objective': 'binary',\n        'metric': 'auc',\n        'num_leaves': 63,\n        'bagging_fraction': 0.9,\n        'bagging_seed':2020,\n        'min_data_in_leaf': 50,\n        \n        'tree_learner':'voting',\n        'reg_alpha':10,\n        'reg_lambda':8,\n        #'device':'gpu',\n        'verbose': -1,\n        'nthread': 8,\n        'colsample_bytree':0.77,\n        'min_child_weight':4,\n        'min_child_samples':10,\n        'min_split_gain':0,\n        'lambda_l1': 0.8,\n        'max_bin':300\n    }\n\n    for i, (train_index, test_index) in enumerate(folds.split(train, target)):\n        train_y, test_y = target[train_index], target[test_index]\n        train_X, test_X = train[feats].iloc[train_index, :], train[feats].iloc[test_index, :]\n        dtrain = lgb.Dataset(train_X,\n                             label=train_y)\n        dval = lgb.Dataset(test_X,\n                           label=test_y)\n        lgb_model = lgb.train(\n                parameters,\n                dtrain,\n                num_boost_round=2000,\n                valid_sets=[dval],\n                early_stopping_rounds=100,\n                verbose_eval=100\n        )\n        oof_probs[test_index] = lgb_model.predict(test_X[feats], num_iteration=lgb_model.best_iteration)\n        output_preds += lgb_model.predict(test[feats], num_iteration=lgb_model.best_iteration)/folds.n_splits\n        \n        offline_score.append(lgb_model.best_score['valid_0']['auc'])\n        print(offline_score)\n        \n        # feature importance\n        fold_importance_df = pd.DataFrame()\n        fold_importance_df[\"feature\"] = feats\n        fold_importance_df[\"importance\"] = lgb_model.feature_importance(importance_type='gain')\n        fold_importance_df[\"fold\"] = i + 1\n        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n\n    valid_auc = roc_auc_score(target, oof_probs)\n    print('OOF-MEAN-AUC:%.6f, OOF-STD-AUC:%.6f' % (np.mean(offline_score), np.std(offline_score)))\n    print(\"valid_auc:  {}\".format(valid_auc))\n    print('feature importance:')\n    print(feature_importance_df.groupby(['feature'])['importance'].mean().sort_values(ascending=False).head(40))\n\n    return output_preds, oof_probs, np.mean(offline_score),feature_importance_df.groupby(['feature'])['importance'].mean().sort_values(ascending=False)\n# This way we have randomness and are able to reproduce the behaviour within this cell.\n          ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 读取数据"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nDATA_PATH = '/kaggle/input/financial-risk/'\nprint('读取数据...')\nbase_df, op_df, trans_df = data_preprocess(DATA_PATH=DATA_PATH)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 特征工程"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nprint('开始特征工程...')\ndata = gen_features(base_df, op_df, trans_df)\ndata.to_csv('data.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_=data\ndrop_list=[]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# UID特征"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nprint(\"UID trans\")\ncol1_list=['platform']\ncol2_list=['ip','type2']\ntrans_temp ,uids= encode_CB(trans_df,col1_list,col2_list)\nuids_FE=[uid+\"_FE\" for uid in uids]\ndata_=data_.merge(gen_user_status_features(trans_temp,'user','platform_ip_FE',['mean','sum']),on=['user'],how='left')\ndata_=data_.merge(gen_user_status_features(trans_temp,'user','platform_type2_FE',['sum','min']),on=['user'],how='left')\n\n\ncol1_list=['type1']\ncol2_list=['hour']\ntrans_temp ,uids= encode_CB(trans_df,col1_list,col2_list)\nuid_FE=[uid+\"_FE\" for uid in uids]\ndata_=data_.merge(gen_user_status_features(trans_temp,'user',uids_FE[0],['mean']),on=['user'],how='left')\n#data_=data_.merge(gen_user_status_features(trans_temp,'user',uids_FE[1],['mean','std']),on=['user'],how='left')\n\n\n\ncol1_list=['type2']\ncol2_list=['hour']\ntrans_temp ,uids= encode_CB(trans_df,col1_list,col2_list)\nuid_FE=[uid+\"_FE\" for uid in uids]\ndata_=data_.merge(gen_user_status_features(trans_temp,'user',uids_FE[0],['sum']),on=['user'],how='left')\n#data_=data_.merge(gen_user_status_features(trans_temp,'user',uids_FE[1],['mean','sum','CV']),on=['user'],how='left')\n\n\ncol1_list=['days_diff']\ncol2_list=['hour','week']\ntrans_temp ,uids= encode_CB(trans_df,col1_list,col2_list)\nuid_FE=[uid+\"_FE\" for uid in uids]\ndata_=data_.merge(gen_user_status_features(trans_temp,'user',uids_FE[0],['mean','max']),on=['user'],how='left')\ndata_=data_.merge(gen_user_status_features(trans_temp,'user',uids_FE[1],['sum']),on=['user'],how='left')\n#data_=data_.merge(gen_user_status_features(trans_temp,'user',uids_FE[2],['mean']),on=['user'],how='left')\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nprint(\"UID op\")\ncol1_list=['op_device']\ncol2_list=['net_type']\ntrans_temp ,uids= encode_CB(op_df,col1_list,col2_list)\nuids_FE=[uid+\"_FE\" for uid in uids]\ndata_=data_.merge(gen_user_status_features(trans_temp,'user',uids_FE[0],['std']),on=['user'],how='left')\n\ncol1_list=['net_type']\ncol2_list=['days_diff']\ntrans_temp ,uids= encode_CB(op_df,col1_list,col2_list)\nuids_FE=[uid+\"_FE\" for uid in uids]\ndata_=data_.merge(gen_user_status_features(trans_temp,'user',uids_FE[0],['min']),on=['user'],how='left')\n\ncol1_list=['days_diff']\ncol2_list=['hour']\ntrans_temp ,uids= encode_CB(op_df,col1_list,col2_list)\nuids_FE=[uid+\"_FE\" for uid in uids]\ndata_=data_.merge(gen_user_status_features(trans_temp,'user',uids_FE[0],['min']),on=['user'],how='left')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nprint(\"UID BASE\")\ndata_['uid1']=data_['agreement1']+data_['agreement2']+data_['agreement3']+data_['agreement4']\ndrop_list+=['uid1']\ndata_=data_.merge(gen_user_status_features(data_,'uid1','user_trans_ip_count',['std']),on=['uid1'],how='left') \n\n\ndata_['uid2']=data_['product1_amount']+data_['product2_amount']+data_['product3_amount']+data_['product4_amount']+data_['product5_amount']+data_['product6_amount']\ndrop_list+=['uid2']\n\ndata_=data_.merge(gen_user_status_features(data_,'uid2','user_days_diff_sum_x',['mean']),on=['uid2'],how='left')\ndata_=data_.merge(gen_user_status_features(data_,'uid2','user_trans_ip_count',['mean']),on=['uid2'],how='left') \n\n\nzip_list=[\n            [['provider'],['using_time'],['user_amount_CV','svd_countvec_op_mode_4'],['mean']],\n          [['provider'],['province'],['product7_fail_cnt'],['mean']],\n          [['provider'],['service3'],['user_amount_sum'],['sum']],\n     [['level'],['province'],['product7_fail_cnt'],['std','mean']],\n     [['level'],['province'],['user_trans_ip_count'],['std','mean']],\n     [['level'],['province'],['user_days_diff_sum_x'],['std','mean','max']],\n     [['level'],['city'],['user_trans_ip_count'],['std','mean']],\n     [['level'],['city'],['user_days_diff_sum_x'],['std','mean']],\n     [['level'],['city'],['product7_fail_cnt'],['std','mean']],\n     [['level'],['city'],['user_days_diff_sum_x'],['std','mean','max']],\n     [['level'],['using_time'],['user_days_diff_sum_x'],['std','mean']],\n     [['level'],['service3'],['user_trans_ip_count'],['std','mean']],\n    [['age'],['service3'], ['user_trans_ip_count'],['mean']],\n    [['age'],['service3'], ['user_days_diff_sum_x'],['mean']],\n    [['age'],['city'], ['user_days_diff_sum_x'],['mean','std']],\n     \n     [['using_time'],['service3'], ['user_trans_ip_count'],['mean']],\n    [['using_time'],['service3'], ['user_days_diff_sum_x'],['mean']],\n    [['regist_type'],['city'], ['product7_fail_cnt'],['std']],\n    [['province'],['login_days_cnt'], ['user_trans_ip_count'],['mean','std']],\n    [['city'],['balance'], ['product7_fail_cnt'],['std']],\n    [['city'],['balance1'], ['product7_fail_cnt'],['std']],\n    [['acc_count'],['login_days_cnt'], ['user_trans_ip_count'],['mean','std']],\n    [['balance'],['ip_cnt'], ['user_days_diff_sum_x'],['mean','std','max','sum']],\n    [['balance'],['ip_cnt'], ['product7_fail_cnt'],['std','mean','sum']],\n    [['balance'],['login_cnt_avg'], ['user_trans_ip_count'],['mean','std']],\n    [['balance'],['login_cnt_avg'], ['product7_fail_cnt'],['std','sum','mean']],\n    [['balance'],['login_cnt_avg'], ['user_days_diff_sum_x'],['mean','std']],\n    \n    [['balance'],['login_days_cnt'], ['user_days_diff_sum_x'],['mean','std']],\n    [['balance'],['login_days_cnt'], ['user_trans_ip_count'],['mean','std']],\n    [['balance'],['balance_avg'], ['product7_fail_cnt'],['std']],\n     [['balance'],['balance1_avg'], ['product7_fail_cnt'],['std','mean']],\n    [['balance'],['balance1_avg'], ['user_trans_ip_count'],['mean']],\n    [['balance'],['balance1_avg'], ['user_days_diff_sum_x'],['mean']],\n    [['balance'],['balance_avg'], ['user_trans_ip_count'],['sum']],\n    [['balance'],['acc_count'], ['user_trans_ip_count'],['mean','std']],\n    [['balance'],['acc_count'], ['product7_fail_cnt'],['mean','std','sum']],\n    [['balance1_avg'],['service3'], ['user_trans_ip_count'],['mean']],\n         ]\n\nfor i in zip_list:\n    data_ ,uids= encode_CB(data_,i[0],i[1])\n    uid_FE=[uid+\"_FE\" for uid in uids]\n    drop_list+=uid_FE\n    drop_list+=uids\n    for j in i[2]:\n        data_=data_.merge(gen_user_status_features(data_,uids[0],j,i[3]),on=[uids[0]],how='left')  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 清理drop_List"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in drop_list:\n    if i not in data_.columns:\n        print(i)\n        drop_list.remove(i)\ndrop_list=list(set(drop_list))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nprint('开始模型训练...')\nk_fold=10\n\ntrain = data_[~data_['label'].isnull()].copy()\ntest = data_[data_['label'].isnull()].copy()\ntarget = train['label']\n#train,test = encode_Cat(train,test,k=k_fold)\n#train, test = kfold_stats_feature(train, test, uids_2d, k_fold)\n\nfor i in ['service3_level']+drop_list:    \n    train.drop([i], axis=1, inplace=True)\n    test.drop([i], axis=1, inplace=True)\n        \n\ntrain,test=encode_LE(train,test)\n\nlgb_preds, lgb_oof, lgb_score,feature_importance_df = lgb_model_origin(train=train, target=target, test=test, k=k_fold)\n\nsub_df = test[['user']].copy()\nsub_df['prob'] = lgb_preds\nsub_df.to_csv('sub(10folds).csv', index=False)\nfeature_importance_df.to_csv('feature(1).csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f=pd.DataFrame(feature_importance_df,columns=['importance']).reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}