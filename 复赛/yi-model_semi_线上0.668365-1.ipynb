{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nimport pandas as pd\nimport numpy as np\nimport warnings\nfrom sklearn.preprocessing import LabelEncoder\nimport lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.decomposition import TruncatedSVD,PCA\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import roc_auc_score\nfrom gensim.models import Word2Vec, FastText\nfrom gensim import corpora,models\nfrom sklearn import preprocessing\nfrom tqdm import tqdm\nimport multiprocessing\nwarnings.filterwarnings('ignore')\nprint(multiprocessing.cpu_count())\nimport gc\nfrom category_encoders import *\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nwarnings.filterwarnings('ignore')\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n## You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\norigin_features = []\ndrop_list=[]\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":1,"outputs":[{"output_type":"stream","text":"4\n/kaggle/input/financial-risk/testb_op.csv\n/kaggle/input/financial-risk/.doc\n/kaggle/input/financial-risk/submit_example.csv\n/kaggle/input/financial-risk/train_trans.csv\n/kaggle/input/financial-risk/test_a_op.csv\n/kaggle/input/financial-risk/test_a_base.csv\n/kaggle/input/financial-risk/train_base.csv\n/kaggle/input/financial-risk/train_label.csv\n/kaggle/input/financial-risk/testb_base.csv\n/kaggle/input/financial-risk/test_a_trans.csv\n/kaggle/input/financial-risk/testb_trans.csv\n/kaggle/input/financial-risk/train_op.csv\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode_LE(data,test):\n    for f in tqdm([f for f in data.select_dtypes('object').columns if f not in ['user','label']]):\n        le = LabelEncoder()\n        \n        le.fit(list(data[f].values) + list(test[f].values))\n        data[f] = le.transform(list(data[f].values))\n        test[f] = le.transform(list(test[f].values))\n        \n    return data,test\n        \ndef encode_CB(df,col1_list,col2_list):\n    uids=[]\n    for col1 in col1_list:\n        for col2 in col2_list:\n            name = col1+'_'+col2\n            uids.append(name)\n            df[name] = df[col1].astype(str)+'_'+df[col2].astype(str)\n            df[name+'_FE'] = df[name].map(df[name].value_counts(dropna=True,normalize=True))\n            df[name+'_FE'] = df[name+\"_FE\"].astype('float32')\n    return df,uids\n\ndef encode_Cat(train,test, k,sigma=2.0,a=2.0):\n    \n    kf = StratifiedKFold(n_splits=k, shuffle=True, random_state=2020)    \n    \n    print(\"Cat encoding\")\n    for col in tqdm([f for f in data.select_dtypes('object').columns if f not in ['user','label']]):\n        train['catboost_'+col] = 0\n        test['catboost_' + col] =0\n        if col not in origin_features:\n            sigma=10\n            a=20\n        for trn_idx,val_idx in kf.split(train,train['label']):\n            X = train.iloc[trn_idx][col]\n            enc_cat = CatBoostEncoder(sigma=sigma).fit(X,train.iloc[trn_idx]['label'])\n            train['catboost_'+col] += enc_cat.transform(train[col])[col] / k\n            test['catboost_'+col] += enc_cat.transform(test[col])[col] / k\n    print('Cat encoding finish')\n    return  train,test\n            ","execution_count":2,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"%%time\ndef gen_user_status_features(df,index,value,aggregations=['mean','std','min','max','sum']):\n        \n    group_df = df.groupby([index])[value].agg(aggregations).reset_index() \n    if('mean' in aggregations and 'std' in aggregations):\n        group_df['CV']=group_df['std'] / group_df['mean']\n    group_df.rename(columns={i:\"{}_{}_\".format(index,value)+i for i in group_df.columns if i not in[index]},\n                   inplace=True\n                   )\n                \n    return group_df\n\n\ndef gen_user_group_features(df, col,value,aggregations=['mean','std','min','max','sum']):  \n    group_df = df.pivot_table(index='user',\n                              columns=col,\n                              values=value,\n                              dropna=False,\n                              aggfunc=aggregations).fillna(0)\n    cols=group_df.columns\n    group_df.columns = ['user_group_{}_{}_{}_{}'.format(col,value, f[1], f[0]) for f in group_df.columns]\n    for f in cols:\n        group_df['user_group_{}_{}_{}_CV'.format(col,value,f[1])] = group_df['user_group_{}_{}_{}_std'.format(col,value,f[1])] / group_df['user_group_{}_{}_{}_mean'.format(col,value,f[1])]\n    group_df.reset_index(inplace=True)\n    return group_df\n\ndef gen_user_group_nunique_features(df, col,value):\n    \n    group_df = df.pivot_table(index='user',\n                              columns=col,\n                              values=value,\n                              dropna=False,\n                              aggfunc=['count','nunique']).fillna(0)\n    group_df.columns = ['user_{}_{}_{}_{}'.format(col,value, f[1], f[0]) for f in group_df.columns]\n    group_df.reset_index(inplace=True)\n    return group_df\n\ndef gen_user_nunique_features(df, value, prefix):\n    a=\"user_{}_{}_nuniq\".format(prefix, value)\n    b=\"user_{}_{}_count\".format(prefix, value)\n    group_df = df.groupby(['user'])[value].agg(\n        a='nunique',b='count'\n    ).reset_index()\n    \n    group_df.rename(columns={'a':a,'b':b},inplace=True) \n    return group_df\n\ndef gen_user_null_features(df, value, prefix):\n    df['is_null'] = 0\n    df.loc[df[value].isnull(), 'is_null'] = 1\n\n    group_df = df.groupby(['user'])['is_null'].agg(a='sum',\n                                                    b='mean').reset_index()\n    \n    group_df.rename(columns={'a':'user_{}_{}_null_cnt'.format(prefix, value),\n                    'b':'user_{}_{}_null_ratio'.format(prefix, value)},\n                   inplace=True\n                   )\n\n    return group_df\n\n    \ndef gen_user_window_amount_features(df, window):\n    \n    group_df = df[df['days_diff']>window].groupby('user')['amount'].agg(\n        a='mean',\n         b='std',\n         c='max',\n         d='min',\n         e='sum',\n        f='median',\n         g='count',\n        ).reset_index()\n    \n    group_df.rename(columns={'a':'user_amount_mean_{}d'.format(window),\n                    'b':'user_amount_std_{}d'.format(window),\n                    'c':'user_amount_max_{}d'.format(window),\n                    'd':'user_amount_min_{}d'.format(window),\n                    'e':'user_amount_sum_{}d'.format(window),\n                    'f':'user_amount_med_{}d'.format(window),\n                    'g':'user_amount_cnt_{}d'.format(window)},\n                   inplace=True\n                   )\n    \n    return group_df\n\ndef gen_user_window_hour_amount_features(df, window):\n    \n    group_df = df[df['hour']>window].groupby('user')['amount'].agg(\n        a='mean',\n         b='std',\n         c='max',\n         d='min',\n         e='sum',\n         f='median',\n         g='count',\n        ).reset_index()\n    \n    group_df.rename(columns={'a':'user_amount_mean_{}h'.format(window),\n                    'b':'user_amount_std_{}h'.format(window),\n                    'c':'user_amount_max_{}h'.format(window),\n                    'd':'user_amount_min_{}h'.format(window),\n                    'e':'user_amount_sum_{}h'.format(window),\n                    'f':'user_amount_med_{}h'.format(window),\n                    'g':'user_amount_cnt_{}h'.format(window)},\n                   inplace=True\n                   )\n    \n    return group_df\n\n\ndef gen_user_tfidf_features(df, value):\n    #填充缺失值\n    df[value].replace(' ', np.nan, inplace=True)\n    df[value] = df[value].astype(str)\n    df[value].fillna('-1', inplace=True)\n\n    #\n    group_df = df.groupby(['user']).apply(lambda x: x[value].tolist()).reset_index()\n    group_df.columns = ['user', 'list']\n    group_df['list'] = group_df['list'].apply(lambda x: ','.join(x))\n    enc_vec = TfidfVectorizer()\n    tfidf_vec = enc_vec.fit_transform(group_df['list'])\n    svd_enc = TruncatedSVD(n_components=10, n_iter=20, random_state=2020)\n    vec_svd = svd_enc.fit_transform(tfidf_vec)\n    vec_svd = pd.DataFrame(vec_svd)\n    vec_svd.columns = ['svd_tfidf_{}_{}'.format(value, i) for i in range(10)]\n    group_df = pd.concat([group_df, vec_svd], axis=1)\n    del group_df['list']\n    return group_df\n\ndef gen_user_countvec_features(df, value):\n    df[value] = df[value].astype(str)\n    df[value].fillna('-1', inplace=True)\n    group_df = df.groupby(['user']).apply(lambda x: x[value].tolist()).reset_index()\n    group_df.columns = ['user', 'list']\n    group_df['list'] = group_df['list'].apply(lambda x: ','.join(x))\n    enc_vec = CountVectorizer()\n    tfidf_vec = enc_vec.fit_transform(group_df['list'])\n    svd_enc = TruncatedSVD(n_components=5, n_iter=20, random_state=2020)\n    vec_svd = svd_enc.fit_transform(tfidf_vec)\n    vec_svd = pd.DataFrame(vec_svd)\n    vec_svd.columns = ['svd_countvec_{}_{}'.format(value, i) for i in range(5)]\n    group_df = pd.concat([group_df, vec_svd], axis=1)\n    del group_df['list']\n    return group_df\n\n#定义加载函数\ndef load_dataset(DATA_PATH):\n    train_label = pd.read_csv(DATA_PATH+'train_label.csv')\n    test_base = pd.read_csv(DATA_PATH+'testb_base.csv')\n    \n    train_base = pd.read_csv(DATA_PATH+'train_base.csv')\n    train_op = pd.read_csv(DATA_PATH+'train_op.csv')\n    train_trans = pd.read_csv(DATA_PATH+'train_trans.csv')\n    test_op = pd.read_csv(DATA_PATH+'testb_op.csv')\n    test_trans = pd.read_csv(DATA_PATH+'testb_trans.csv')\n    \n    origin_features = list(train_base.columns)+list(train_trans.columns)+list(train_op.columns)\n    return train_label, train_base, test_base, train_op, train_trans, test_op, test_trans\n\n#tran_trans和train_op文件中的tm_diff时间转换\ndef transform_time(x):\n    day = int(x.split(' ')[0])\n    hour = int(x.split(' ')[2].split('.')[0].split(':')[0])\n    minute = int(x.split(' ')[2].split('.')[0].split(':')[1])\n    second = int(x.split(' ')[2].split('.')[0].split(':')[2])\n    return 86400*day+3600*hour+60*minute+second\n\n\n\n#讲所有的数据进行初步的整合，并且把时间days_diff进行了数字化提取\ndef data_preprocess(DATA_PATH):\n    train_label, train_base, test_base, train_op, train_trans, test_op, test_trans = load_dataset(DATA_PATH=DATA_PATH)\n    # 拼接数据\n    train_df = train_base.copy()\n    test_df = test_base.copy()\n        #将train_base数据和train_label整合\n    train_df = train_label.merge(train_df, on=['user'], how='left')\n    del train_base, test_base\n        #将train_op数据和test_op整合--方便将train和test中所有的数据进行统一的处理\n    op_df = pd.concat([train_op, test_op], axis=0, ignore_index=True)\n        #将train_trans数据和test_trans整合--方便将train和test中所有的数据进行统一的处理\n    trans_df = pd.concat([train_trans, test_trans], axis=0, ignore_index=True)\n        #将train_base数据和test_base整合\n    data = pd.concat([train_df, test_df], axis=0, ignore_index=True) \n    del train_op, test_op, train_df, test_df\n    \n    \n    # 时间维度的处理\n    op_df['days_diff'] = op_df['tm_diff'].apply(lambda x: int(x.split(' ')[0]))\n    trans_df['days_diff'] = trans_df['tm_diff'].apply(lambda x: int(x.split(' ')[0]))\n    \n    op_df['timestamp'] = op_df['tm_diff'].apply(lambda x: transform_time(x))\n    trans_df['timestamp'] = trans_df['tm_diff'].apply(lambda x: transform_time(x))\n    op_df['hour'] = op_df['tm_diff'].apply(lambda x: int(x.split(' ')[2].split('.')[0].split(':')[0]))\n    trans_df['hour'] = trans_df['tm_diff'].apply(lambda x: int(x.split(' ')[2].split('.')[0].split(':')[0]))\n    trans_df['week'] = trans_df['days_diff'].apply(lambda x: x % 7)\n    \n    \n    # 排序\n    trans_df = trans_df.sort_values(by=['user', 'timestamp'])\n    op_df = op_df.sort_values(by=['user', 'timestamp'])\n    trans_df.reset_index(inplace=True, drop=True)\n    op_df.reset_index(inplace=True, drop=True)\n\n    gc.collect()\n    return data, op_df, trans_df\n\ndef gen_features(df_, op, trans):\n    df=df_\n    # base数据处理\n    print(\"base logistic features\")\n    df['card_a_cnt_avg'] = df[\"card_a_cnt\"]/df[\"using_time\"]\n    df['product7_fail_ratio'] = df['product7_fail_cnt'] / df['product7_cnt']\n    \n    df['city_count'] = df.groupby(['city'])['user'].transform('count')  ## 与原始数据集相同数量的项目\n    df['city_product7_fail_ratio'] =  df.groupby(['city'])['product7_fail_cnt'].transform('sum')/df.groupby(['city'])['product7_cnt'].transform('sum')\n    df['province_count'] = df.groupby(['province'])['user'].transform('count')\n    df['province_product7_fail_ratio'] = df.groupby(['province'])['product7_fail_cnt'].transform('sum')/df.groupby(['province'])['product7_cnt'].transform('sum')\n    \n    df['login_cnt_period_var']=df[['login_cnt_period1','login_cnt_period2']].std(axis=1)\n    df['login_cnt_period_CV']=df['login_cnt_period_var'] / df[['login_cnt_period1','login_cnt_period2']].mean(axis=1)\n    df['card_var']=df[['card_a_cnt','card_b_cnt','card_c_cnt','card_d_cnt']].std(axis=1)\n    df['card_CV']=df['card_var'] / df[['card_a_cnt','card_b_cnt','card_c_cnt','card_d_cnt']].mean(axis=1)\n    df['op_cnt_var']=df[['op1_cnt','op2_cnt']].std(axis=1)\n    df['op_cnt_CV']=df['op_cnt_var'] / df[['op1_cnt','op2_cnt']].mean(axis=1)\n    df['login_var']=df[['login_cnt_period1','login_cnt_period2','login_cnt_avg']].std(axis=1)\n    df['login_CV']=df['login_var'] / df[['login_cnt_period1','login_cnt_period2','login_cnt_avg']].mean(axis=1)\n    \n    \n    df['ip_cnt_per_login_cnt_avg'] = df['ip_cnt']/df[['login_cnt_period1','login_cnt_period2','login_cnt_avg']].mean(axis=1)\n    df['ip_cnt_per_login_days_cnt'] = df['ip_cnt']/df['login_days_cnt']\n    df['acc_count_per_time'] = df[\"acc_count\"]/df[\"using_time\"]\n    df['ip_cnt_per_using_time'] = df[\"ip_cnt\"]/df[\"using_time\"]\n    df['agreement_total_per_using_time'] = df[\"agreement_total\"]/df[\"using_time\"]\n    df['agreement_total_per_account_cnt'] = df[\"agreement_total\"]/df[\"acc_count\"]\n    df['ip_cnt_per_acc_count'] = df[\"ip_cnt\"]/df[\"acc_count\"]\n    df['service1_amt_per_using_time'] = df[\"service1_amt\"]/df[\"using_time\"]\n    df[\"service_cnt_per_time\"]=(df[\"service1_cnt\"]+df[\"service2_cnt\"])/df[\"using_time\"]\n    \n    \n    df['op_cnt_per_usign_time'] = (df[\"op1_cnt\"]+df[\"op2_cnt\"])/df[\"using_time\"]\n    df['op_cnt_per_login_cnt_avg'] = (df[\"op1_cnt\"]+df[\"op2_cnt\"])/df['login_cnt_avg']\n    df['op_cnt_per_login_cnt_p1'] =  (df[\"op1_cnt\"]+df[\"op2_cnt\"])/df['login_cnt_period1']\n    df['op_cnt_per_login_cnt_p2'] =  (df[\"op1_cnt\"]+df[\"op2_cnt\"])/df['login_cnt_period2']\n    \n    # trans\n    df = df.merge(gen_user_status_features(trans,'user','amount'), on=['user'], how='left')\n    df = df.merge(gen_user_status_features(trans,'user','days_diff'), on=['user'], how='left')\n    df = df.merge(gen_user_status_features(trans,'user','hour'), on=['user'], how='left')\n\n  \n    print(\"trans nunique && null features\")\n    for col in tqdm(['days_diff', 'platform', 'tunnel_in', 'tunnel_out', 'type1', 'type2','ip', 'ip_3']):\n        df = df.merge(gen_user_nunique_features(df=trans, value=col, prefix='trans'), on=['user'], how='left')\n        df = df.merge(gen_user_null_features(df=trans, value=col, prefix='trans'), on=['user'], how='left')\n    df['user_amount_per_days'] = df['user_amount_sum'].div(df['user_trans_days_diff_nuniq'])\n    df['user_amount_per_tunnel_in'] = df['user_amount_sum'] / df['user_trans_tunnel_in_nuniq']\n    df['user_amount_per_ip_cnt']=df['user_amount_sum']/df['ip_cnt']\n    temp=trans[trans['ip'].isnull()].groupby(['user'])['amount'].agg(user_null_ip_amount_sum='sum',\n                                                                     user_null_ip_amount_mea='mean'\n                                                                    \n                                                                    ).reset_index()\n    \n    df=df.merge(temp,on=['user'],how='left')\n\n            \n    print(\"trans group amount features\")\n    trans_type1=trans.query(\"type1==['f67d4b5a05a1352a','19d44f1a51919482','674e8d5860bc033d', '45a1168437c708ff']\")\n    trans_type2=trans.query(\"type2==['11a213398ee0c623']\")\n    trans_plat=trans.query(\"platform==['46c69cbbce5f1568','42573d7287a8c9c2']\")\n    trans_tunnel_in=trans.query(\"tunnel_in==['b2e7fa260df4998d','9c5701005a2d85bd']\")\n    df = df.merge(gen_user_group_features(df=trans_type2,col='type2',      value='amount'),on=['user'],how='left')\n    df = df.merge(gen_user_group_features(df=trans_type1,col='type1',      value='amount'),on=['user'],how='left')\n    df = df.merge(gen_user_group_features(df=trans_tunnel_in,col='tunnel_in',  value='amount'),on=['user'],how='left')\n    df = df.merge(gen_user_group_features(df=trans,col='tunnel_out',value='amount'), on=['user'], how='left')\n    df = df.merge(gen_user_group_features(df=trans_plat,col='platform',   value='amount'),on=['user'],how='left')\n    \n    print(\"trans group days_diff features\")\n    df = df.merge(gen_user_group_features(df=trans_type2,col='type2',      value='days_diff'),on=['user'],how='left')\n    df = df.merge(gen_user_group_features(df=trans_type1,col='type1',      value='days_diff'),on=['user'],how='left')\n    df = df.merge(gen_user_group_features(df=trans_tunnel_in,col='tunnel_in',  value='days_diff'),on=['user'],how='left')\n    df = df.merge(gen_user_group_features(df=trans,col='tunnel_out',value='days_diff'), on=['user'], how='left')\n    df = df.merge(gen_user_group_features(df=trans_plat,col='platform',   value='days_diff'),on=['user'],how='left')\n    \n    \n    print(\"trans group nunique features\")\n    df = df.merge(gen_user_group_nunique_features(df=trans,col='platform',   value='type1'),on=['user'],how='left')\n    df = df.merge(gen_user_group_nunique_features(df=trans,col='platform',   value='ip'),on=['user'],how='left')\n    \n    print(\"trans days window features\")\n    for window in tqdm([20,13,15]):\n        df = df.merge(gen_user_window_amount_features(df=trans, window=window), on=['user'], how='left')\n        \n    print(\"trans hour window features\")\n    for win_hour in tqdm([9,18,19,20]):\n        df = df.merge(gen_user_window_hour_amount_features(df=trans, window=win_hour), on=['user'], how='left')\n        \n    print(\"trans tfidf--count features\")    \n    df = df.merge(gen_user_tfidf_features(df=trans, value='type1'), on=['user'], how='left')\n    df = df.merge(gen_user_tfidf_features(df=trans, value='type2'), on=['user'], how='left')\n    \n    print(\"trans LSI features\")\n    #for col in ['ip','platform','tunnel_in','tunnel_out','ip_3','type1','type2']:\n    #    df=df.merge(gen_user_lda_features(trans,col,5),on=['user'],how='left')\n        \n    #df=df.merge(fasttext(trans,['ip','type1',\"type2\",'tunnel_out','tunnel_in'],\"train\"),on=['user'],how='left')\n    # op\n    print(\"op nuique features\")\n    for col in tqdm(['op_type','op_mode','channel']):\n        df = df.merge(gen_user_nunique_features(df=op, value=col, prefix='op'), on=['user'], how='left')\n    \n    print(\"op null features\")\n    for col in tqdm(['op_device', 'ip', 'net_type', 'channel', 'ip_3']):\n        df = df.merge(gen_user_null_features(df=op, value=col, prefix='op'), on=['user'], how='left')\n    \n    print(\"op days_diff features\")\n    df = df.merge(gen_user_status_features(op,'user','days_diff'), on=['user'], how='left')\n    df = df.merge(gen_user_status_features(op,'user','hour'), on=['user'], how='left')\n\n    print(\"op tfidf--cnt features\")\n    df = df.merge(gen_user_tfidf_features(df=op, value='op_mode'), on=['user'], how='left')\n    df = df.merge(gen_user_tfidf_features(df=op, value='op_type'), on=['user'], how='left')\n    df = df.merge(gen_user_tfidf_features(df=op, value='op_device'), on=['user'], how='left')\n    df = df.merge(gen_user_countvec_features(df=op, value='op_mode'), on=['user'], how='left')\n    df = df.merge(gen_user_countvec_features(df=op, value='op_type'), on=['user'], how='left')\n    df = df.merge(gen_user_countvec_features(df=op, value='op_device'), on=['user'], how='left')\n    \n    print(\"op LSI features\")\n    #for col in [\"op_device\",\"ip\",\"channel\",\"net_type\",\"op_type\",\"op_mode\"]:\n    #    df=df.merge(gen_user_lda_features(op,col,5),on=['user'],how='left')\n    #df=df.merge(fasttext(op,['op_mode','op_type',\"op_device\",\"channel\",'ip_3',],\"train\"),on=['user'],how='left')\n    return df\ndef kfold_stats_feature(train, test, feats, k):\n    folds = StratifiedKFold(n_splits=k, shuffle=True, random_state=2020)  # 这里最好和后面模型的K折交叉验证保持一致\n\n    train['fold'] = None\n    for fold_, (trn_idx, val_idx) in enumerate(folds.split(train, train['label'])):\n        train.loc[val_idx, 'fold'] = fold_\n\n    kfold_features = []\n    for feat in feats:\n        nums_columns = ['label']\n        for f in nums_columns:\n            colname = feat + '_' + f + '_kfold_mean'\n            kfold_features.append(colname)\n            train[colname] = None\n            for fold_, (trn_idx, val_idx) in enumerate(folds.split(train, train['label'])):\n                tmp_trn = train.iloc[trn_idx]\n                order_label = tmp_trn.groupby([feat])[f].mean()\n                tmp = train.loc[train.fold == fold_, [feat]]\n                train.loc[train.fold == fold_, colname] = tmp[feat].map(order_label)\n                # fillna\n                global_mean = train[f].mean()\n                train.loc[train.fold == fold_, colname] = train.loc[train.fold == fold_, colname].fillna(global_mean)\n            train[colname] = train[colname].astype(float)\n\n        for f in nums_columns:\n            colname = feat + '_' + f + '_kfold_mean'\n            test[colname] = None\n            order_label = train.groupby([feat])[f].mean()\n            test[colname] = test[feat].map(order_label)\n            # fillna\n            global_mean = train[f].mean()\n            test[colname] = test[colname].fillna(global_mean)\n            test[colname] = test[colname].astype(float)\n    del train['fold']\n    return train, test\n\ndef lgb_model_origin(train, target, test, k):\n    feats = [f for f in train.columns if f not in ['user', 'label']]\n    print('Current num of features:', len(feats))\n    folds = StratifiedKFold(n_splits=k, shuffle=True, random_state=2020)\n    oof_probs = np.zeros(train.shape[0])\n    output_preds = 0\n    offline_score = []\n    valid_score = []\n    train_score = []\n    feature_importance_df = pd.DataFrame()\n    parameters = {\n        'learning_rate': 0.05,\n        'boosting_type': 'gbdt',\n        'objective': 'binary',\n        'metric': 'auc',\n        'num_leaves': 63,\n        'bagging_fraction': 0.9,\n        'bagging_seed':2020,\n        'min_data_in_leaf': 50,\n        \n        'tree_learner':'voting',\n        'reg_alpha':10,\n        'reg_lambda':8,\n        #'device':'gpu',\n        'verbose': -1,\n        'nthread': 8,\n        'colsample_bytree':0.77,\n        'min_child_weight':4,\n        'min_child_samples':10,\n        'min_split_gain':0,\n        'lambda_l1': 0.8,\n        'max_bin':300\n    }\n\n    for i, (train_index, test_index) in enumerate(folds.split(train, target)):\n        train_y, test_y = target[train_index], target[test_index]\n        train_X, test_X = train[feats].iloc[train_index, :], train[feats].iloc[test_index, :]\n        dtrain = lgb.Dataset(train_X,\n                             label=train_y)\n        dval = lgb.Dataset(test_X,\n                           label=test_y)\n        lgb_model = lgb.train(\n                parameters,\n                dtrain,\n                num_boost_round=500,\n                valid_sets=[dval],\n                early_stopping_rounds=100,\n                verbose_eval=100\n        )\n        oof_probs[test_index] = lgb_model.predict(test_X[feats], num_iteration=lgb_model.best_iteration)\n        output_preds += lgb_model.predict(test[feats], num_iteration=lgb_model.best_iteration)/folds.n_splits\n        \n        offline_score.append(lgb_model.best_score['valid_0']['auc'])\n        print(offline_score)\n        \n        # feature importance\n        fold_importance_df = pd.DataFrame()\n        fold_importance_df[\"feature\"] = feats\n        fold_importance_df[\"importance\"] = lgb_model.feature_importance(importance_type='gain')\n        fold_importance_df[\"fold\"] = i + 1\n        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n\n    valid_auc = roc_auc_score(target, oof_probs)\n    print('OOF-MEAN-AUC:%.6f, OOF-STD-AUC:%.6f' % (np.mean(offline_score), np.std(offline_score)))\n    print(\"valid_auc:  {}\".format(valid_auc))\n    print('feature importance:')\n    print(feature_importance_df.groupby(['feature'])['importance'].mean().sort_values(ascending=False).head(40))\n\n    return output_preds, oof_probs, np.mean(offline_score),feature_importance_df.groupby(['feature'])['importance'].mean().sort_values(ascending=False)\n# This way we have randomness and are able to reproduce the behaviour within this cell.\n          ","execution_count":3,"outputs":[{"output_type":"stream","text":"CPU times: user 20 µs, sys: 0 ns, total: 20 µs\nWall time: 24.6 µs\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# 读取数据"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nDATA_PATH = '/kaggle/input/financial-risk/'\nprint('读取数据...')\nbase_df, op_df, trans_df = data_preprocess(DATA_PATH=DATA_PATH)\n","execution_count":4,"outputs":[{"output_type":"stream","text":"读取数据...\nCPU times: user 57.4 s, sys: 3.66 s, total: 1min 1s\nWall time: 1min 1s\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# 特征工程"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nprint('开始特征工程...')\ndata = gen_features(base_df, op_df, trans_df)\ndata.to_csv('data.csv')","execution_count":5,"outputs":[{"output_type":"stream","text":"开始特征工程...\nbase logistic features\n","name":"stdout"},{"output_type":"stream","text":"\r  0%|          | 0/8 [00:00<?, ?it/s]","name":"stderr"},{"output_type":"stream","text":"trans nunique && null features\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 8/8 [00:07<00:00,  1.11it/s]\n","name":"stderr"},{"output_type":"stream","text":"trans group amount features\ntrans group days_diff features\ntrans group nunique features\n","name":"stdout"},{"output_type":"stream","text":"\r  0%|          | 0/3 [00:00<?, ?it/s]","name":"stderr"},{"output_type":"stream","text":"trans days window features\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 3/3 [00:01<00:00,  2.17it/s]\n  0%|          | 0/4 [00:00<?, ?it/s]","name":"stderr"},{"output_type":"stream","text":"trans hour window features\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 4/4 [00:01<00:00,  2.90it/s]\n","name":"stderr"},{"output_type":"stream","text":"trans tfidf--count features\n","name":"stdout"},{"output_type":"stream","text":"\r  0%|          | 0/3 [00:00<?, ?it/s]","name":"stderr"},{"output_type":"stream","text":"trans LSI features\nop nuique features\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 3/3 [00:06<00:00,  2.03s/it]\n  0%|          | 0/5 [00:00<?, ?it/s]","name":"stderr"},{"output_type":"stream","text":"op null features\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 5/5 [00:06<00:00,  1.26s/it]\n","name":"stderr"},{"output_type":"stream","text":"op days_diff features\nop tfidf--cnt features\nop LSI features\nCPU times: user 3min 8s, sys: 3.12 s, total: 3min 11s\nWall time: 3min 6s\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_=data","execution_count":65,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# UID特征"},{"metadata":{"trusted":true},"cell_type":"code","source":"## trans_df uid 查找函数\n#uid_list=[]\n#l=list(trans_df.columns)\n#l.remove('user')\n#for i in range(len(l)):\n#    for j in range(i+1,len(l)):\n#        temp=pd.DataFrame()\n#        name=l[i]+'_'+l[j]\n#        temp[name]=trans_df[l[i]].astype(str)+trans_df[l[j]].astype(str)\n#        if(temp[name].nunique()<10000 and temp[name].nunique()>6):\n#            uid_list.append(name)\n#            print(\"{}_{}:  {}\".format(l[i],l[j],temp[name].nunique()))\n#            \n#            trans_temp ,uids= encode_CB(trans_df,[l[i]],[l[j]])\n#            uids_FE=[uid+\"_FE\" for uid in uids]\n#            for uid_FE in uids_FE:\n#                data_=data_.merge(gen_user_status_features(trans_temp,'user',uid_FE),on=['user'],how='left')","execution_count":56,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nprint(\"UID trans\")\ncol1_list=['platform']\ncol2_list=['ip','type2']\ntrans_temp ,uids= encode_CB(trans_df,col1_list,col2_list)\nuids_FE=[uid+\"_FE\" for uid in uids]\ndata_=data_.merge(gen_user_status_features(trans_temp,'user','platform_ip_FE',['mean','sum']),on=['user'],how='left')\ndata_=data_.merge(gen_user_status_features(trans_temp,'user','platform_type2_FE',['sum','min']),on=['user'],how='left')\n\n\ncol1_list=['type1']\ncol2_list=['hour']\ntrans_temp ,uids= encode_CB(trans_df,col1_list,col2_list)\nuid_FE=[uid+\"_FE\" for uid in uids]\ndata_=data_.merge(gen_user_status_features(trans_temp,'user',uids_FE[0],['mean']),on=['user'],how='left')\n#data_=data_.merge(gen_user_status_features(trans_temp,'user',uids_FE[1],['mean','std']),on=['user'],how='left')\n\n\n\ncol1_list=['type2']\ncol2_list=['hour']\ntrans_temp ,uids= encode_CB(trans_df,col1_list,col2_list)\nuid_FE=[uid+\"_FE\" for uid in uids]\ndata_=data_.merge(gen_user_status_features(trans_temp,'user',uids_FE[0],['sum']),on=['user'],how='left')\n#data_=data_.merge(gen_user_status_features(trans_temp,'user',uids_FE[1],['mean','sum','CV']),on=['user'],how='left')\n\n\ncol1_list=['days_diff']\ncol2_list=['hour','week']\ntrans_temp ,uids= encode_CB(trans_df,col1_list,col2_list)\nuid_FE=[uid+\"_FE\" for uid in uids]\ndata_=data_.merge(gen_user_status_features(trans_temp,'user',uids_FE[0],['mean','max']),on=['user'],how='left')\ndata_=data_.merge(gen_user_status_features(trans_temp,'user',uids_FE[1],['sum']),on=['user'],how='left')\n#data_=data_.merge(gen_user_status_features(trans_temp,'user',uids_FE[2],['mean']),on=['user'],how='left')\n\n\n","execution_count":66,"outputs":[{"output_type":"stream","text":"UID trans\nCPU times: user 9.24 s, sys: 575 ms, total: 9.81 s\nWall time: 9.81 s\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"## trans_df uid 查找函数\n#\n#uid_list=[]\n#def find_uid(base_df):\n#     l=list(base_df.columns)\n#     l.remove('user')\n#     l.remove('label')\n#    \n#     for i in range(len(l)):\n#         for j in range(i+1,len(l)):\n#            if(base_df[l[i]].dtype=='object' and base_df[l[j]].dtype=='object' and\n#              'card_' not in l[i] and 'card_' not in l[j] and 'agreement' not in l[i] \n#               and 'agreement' not in l[j] and \n#               '_amount' not in l[i] and '_amount' not in  l[j] ):\n#                 temp=pd.DataFrame()\n#                 name=l[i]+'_'+l[j]\n#                 temp[name]=base_df[l[i]].astype(str)+base_df[l[j]].astype(str)\n#                 if(temp[name].nunique()<500 and temp[name].nunique()>20):\n#                     uid_list.append(name)\n#                     print(\"{}_{}:  {}\".format(l[i],l[j],temp[name].nunique()))\n#                     data_[name]=data_[l[i]]+data_[l[j]]\n#     print(len(uid_list))\n#drop_list+=uid_list\n#find_uid(base_df)\n#\n## trans_df uid 查找函数\n   ","execution_count":58,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#col1_list=['provider']\n#col2_list=['level', 'verified',\n#       'using_time', 'regist_type','op1_cnt', 'op2_cnt','province', 'city', 'balance', 'balance_avg', 'balance1',\n#       'balance1_avg', 'balance2', 'balance2_avg', 'service3']\n#data_ ,uid_list= encode_CB(data_,col1_list,col2_list)\n#drop_list+=uid_list\n#u=[\n#'user_trans_ip_count',\n#'user_days_diff_sum_x',\n#'product7_fail_ratio',\n#'user_trans_ip_3_count',\n#'product7_fail_cnt',\n#'login_cnt_period_CV',\n#'svd_countvec_op_mode_4',\n#'svd_tfidf_type1_1'\n#]\n#\n#\n#\n#for i in u:\n#    for j in tqdm():\n#        data_=data_.merge(gen_user_status_features(data_,j,i),on=[j],how='left')","execution_count":59,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nprint(\"UID BASE\")\ndata_['uid1']=data_['agreement1']+data_['agreement2']+data_['agreement3']+data_['agreement4']\ndrop_list+=['uid1']\ndata_=data_.merge(gen_user_status_features(data_,'uid1','user_trans_ip_count',['std']),on=['uid1'],how='left')          ","execution_count":75,"outputs":[{"output_type":"stream","text":"UID BASE\nCPU times: user 187 ms, sys: 9.87 ms, total: 197 ms\nWall time: 195 ms\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"zip_list=[\n            [['provider'],['using_time'],['user_amount_CV','svd_countvec_op_mode_4'],['mean']],\n          [['provider'],['province'],['product7_fail_cnt'],['mean']],\n          [['provider'],['service3'],['user_amount_sum'],['sum']]\n         ]\n\nfor i in zip_list:\n    data_ ,uids= encode_CB(data_,i[0],i[1])\n    uid_FE=[uid+\"_FE\" for uid in uids]\n    drop_list+=uid_FE\n    drop_list+=uids\n    for j in i[2]:\n        data_=data_.merge(gen_user_status_features(data_,uids[0],j,i[3]),on=[uids[0]],how='left') ","execution_count":83,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in drop_list:\n    if i not in data_.columns:\n        print(i)\n        drop_list.remove(i)","execution_count":91,"outputs":[{"output_type":"stream","text":"provider_verified\nprovider_balance2\nprovider_op2_cnt\nprovider_balance_avg\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_list=list(set(drop_list))\ndrop_list","execution_count":92,"outputs":[{"output_type":"execute_result","execution_count":92,"data":{"text/plain":"['provider_province_FE',\n 'provider_using_time',\n 'uid1',\n 'provider_province',\n 'provider_using_time_FE',\n 'provider_service3_FE',\n 'provider_service3']"},"metadata":{}}]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"%%time\nprint('开始模型训练...')\nk_fold=10\n\ntrain = data_[~data_['label'].isnull()].copy()\ntest = data_[data_['label'].isnull()].copy()\ntarget = train['label']\n#train,test = encode_Cat(train,test,k=k_fold)\n#train, test = kfold_stats_feature(train, test, uids_2d, k_fold)\n\nfor i in ['service3_level']+drop_list:    \n    train.drop([i], axis=1, inplace=True)\n    test.drop([i], axis=1, inplace=True)\n        \n\ntrain,test=encode_LE(train,test)\n\nlgb_preds, lgb_oof, lgb_score,feature_importance_df = lgb_model_origin(train=train, target=target, test=test, k=k_fold)\n\nsub_df = test[['user']].copy()\nsub_df['prob'] = lgb_preds\nsub_df.to_csv('sub(10folds).csv', index=False)\nfeature_importance_df.to_csv('feature(1).csv')","execution_count":93,"outputs":[{"output_type":"stream","text":"开始模型训练...\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 24/24 [00:01<00:00, 14.05it/s]\n","name":"stderr"},{"output_type":"stream","text":"Current num of features: 463\nTraining until validation scores don't improve for 100 rounds\n[100]\tvalid_0's auc: 0.740916\n[200]\tvalid_0's auc: 0.741953\nEarly stopping, best iteration is:\n[190]\tvalid_0's auc: 0.742598\n[0.7425982384544308]\nTraining until validation scores don't improve for 100 rounds\n[100]\tvalid_0's auc: 0.735165\n[200]\tvalid_0's auc: 0.736412\nEarly stopping, best iteration is:\n[159]\tvalid_0's auc: 0.736848\n[0.7425982384544308, 0.7368482353304543]\nTraining until validation scores don't improve for 100 rounds\n[100]\tvalid_0's auc: 0.726977\n[200]\tvalid_0's auc: 0.72805\nEarly stopping, best iteration is:\n[146]\tvalid_0's auc: 0.728352\n[0.7425982384544308, 0.7368482353304543, 0.7283524924542539]\nTraining until validation scores don't improve for 100 rounds\n[100]\tvalid_0's auc: 0.730454\n[200]\tvalid_0's auc: 0.731726\nEarly stopping, best iteration is:\n[179]\tvalid_0's auc: 0.732291\n[0.7425982384544308, 0.7368482353304543, 0.7283524924542539, 0.7322909592529712]\nTraining until validation scores don't improve for 100 rounds\n[100]\tvalid_0's auc: 0.727257\n[200]\tvalid_0's auc: 0.728151\nEarly stopping, best iteration is:\n[158]\tvalid_0's auc: 0.728936\n[0.7425982384544308, 0.7368482353304543, 0.7283524924542539, 0.7322909592529712, 0.728935813997359]\nOOF-MEAN-AUC:0.733805, OOF-STD-AUC:0.005335\nvalid_auc:  0.733784382757834\nfeature importance:\nfeature\nuser_trans_ip_count                                 10337.739832\nuser_days_diff_sum_x                                 4747.655838\nproduct7_fail_ratio                                  4328.713969\nuser_amount_med_9h                                   2767.837856\nprovider_province_product7_fail_cnt_mean             2140.321957\nuser_trans_ip_3_count                                1893.463502\nservice3                                             1892.233747\nproduct7_fail_cnt                                    1727.765409\nage                                                  1616.872047\nuser_trans_type2_null_cnt                            1557.915955\nproduct7_cnt                                         1477.099887\nlogin_cnt_period_CV                                  1372.889073\nsvd_countvec_op_mode_4                               1297.478113\nuser_group_type1_amount_45a1168437c708ff_sum         1296.991626\nlogin_cnt_period_var                                 1116.982436\nsvd_tfidf_op_type_3                                  1095.199556\nsvd_countvec_op_type_3                               1056.649490\nprovider_using_time_svd_countvec_op_mode_4_mean      1028.630099\nuid1_user_trans_ip_count_std_x                       1019.954070\nuser_trans_type1_nuniq                                959.871774\nsvd_countvec_op_mode_2                                943.724159\nsvd_tfidf_type1_1                                     928.007686\nsvd_tfidf_type2_3                                     908.134781\ncity_product7_fail_ratio                              886.466836\nprovider_using_time_user_amount_CV_mean               884.601129\nsvd_countvec_op_mode_3                                833.445219\ncity_count                                            827.470917\ncity                                                  780.963164\nuser_group_type1_days_diff_45a1168437c708ff_CV        779.927530\nuid1_user_trans_ip_count_std_y                        770.359192\nop_cnt_per_usign_time                                 763.159461\nuser_group_type1_days_diff_45a1168437c708ff_mean      744.607495\nuser_platform_ip_FE_mean_x                            736.269803\nuser_days_diff_mean_y                                 732.730807\nip_cnt_per_login_days_cnt                             723.104010\nsvd_tfidf_op_mode_3                                   721.037445\nuser_op_net_type_null_ratio                           709.742829\nip_cnt_per_using_time                                 690.006203\nuser_op_op_device_null_ratio                          684.623225\nacc_count_per_time                                    683.033073\nName: importance, dtype: float64\nCPU times: user 13min 18s, sys: 2min 54s, total: 16min 12s\nWall time: 4min 13s\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"f=pd.DataFrame(feature_importance_df,columns=['importance']).reset_index()","execution_count":94,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"good_uid=[]\nfor i in drop_list:\n    for j in f.feature:\n        if((i in j )):\n            if (f[f.feature==j].importance.values > 1000):\n                good_uid.append(j)\n                print(\"{}      :     {}\".format(i,j))\nprint(len(good_uid))","execution_count":101,"outputs":[{"output_type":"stream","text":"provider_using_time      :     provider_using_time_svd_countvec_op_mode_4_mean\nuid1      :     uid1_user_trans_ip_count_std_x\nprovider_province      :     provider_province_product7_fail_cnt_mean\n3\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"good_uid=[]\nfor i in uid_list:\n    for j in f.feature:\n        if( i+\"_FE\" in j ):\n            if (f[f.feature==j].importance.values > 100):\n                good_uid.append(j)\n                print(\"{}:{}:      {}\".format(i,j,f[f.feature==j].importance.values))\nprint(len(good_uid))","execution_count":53,"outputs":[{"output_type":"stream","text":"provider_using_time:provider_using_time_FE:      [170.32847862]\n1\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# --------------分界线--------------"},{"metadata":{"trusted":true},"cell_type":"code","source":"def w2v_feat(df, feat, mode):\n    data_frame=df.copy()\n    for i in feat:\n        if data_frame[i].dtype != 'object':\n            data_frame[i] = data_frame[i].astype(str)\n    data_frame.fillna('nan', inplace=True)\n\n    print(f'Start {mode} word2vec ...')\n    model = Word2Vec(data_frame[feat].values.tolist(), size=10, window=2, min_count=1,\n                     workers=multiprocessing.cpu_count(), iter=10)\n    stat_list = ['min', 'max', 'mean', 'std']\n    new_all = pd.DataFrame()\n    for m, t in enumerate(feat):\n        print(f'Start gen feat of {t} ...')\n        tmp = []\n        for i in data_frame[t].unique():\n            tmp_v = [i]\n            tmp_v.extend(model[i])\n            tmp.append(tmp_v)\n        tmp_df = pd.DataFrame(tmp)\n        w2c_list = [f'w2c_{t}_{n}' for n in range(10)]\n        tmp_df.columns = [t] + w2c_list\n        tmp_df = data_frame[['user', t]].merge(tmp_df, on=t)\n        tmp_df = tmp_df.drop_duplicates().groupby('user').agg(stat_list).reset_index()\n        tmp_df.columns = ['user'] + [f'{p}_{q}' for p in w2c_list for q in stat_list]\n        if m == 0:\n            new_all = pd.concat([new_all, tmp_df], axis=1)\n        else:\n            new_all = pd.merge(new_all, tmp_df, how='left', on='user')\n    return new_all\ndef gen_user_lda_features(df, value,num_topics):\n    df[value] = df[value].astype(str)\n    df[value].fillna('-1', inplace=True)\n    group_df = df.groupby(['user']).apply(lambda x: x[value].tolist()).reset_index()\n    group_df.columns = ['user', 'list']\n    \n    texts=group_df['list']\n    dictionary = corpora.Dictionary(texts)\n    corpus = [dictionary.doc2bow(text) for text in texts]\n#    tfidf = models.TfidfModel(corpus)\n#   corpus_tfidf = tfidf[corpus]\n    lda_model = models.LdaMulticore(corpus, id2word=dictionary, num_topics=num_topics,workers=multiprocessing.cpu_count(),random_state=2020)\n    corpus_lda = lda_model.get_document_topics(corpus,minimum_probability=0)\n    \n    new_all = pd.DataFrame(corpus_lda)\n    new_all.columns = [\"LDA_{}_{}\".format(value,i) for i in range(num_topics)]\n    for col in tqdm(new_all.columns):\n        try:\n           new_all[col]=new_all[col].apply(lambda x:x[1])\n        except:\n            None\n             \n    new_all['user']=group_df['user']\n    \n    return new_all\ndef fasttext(df, feat, mode):\n    data_frame=df.copy()\n    # 转化为str\n    for i in feat:\n        if data_frame[i].dtype != 'object':\n            data_frame[i] = data_frame[i].astype(str)\n    data_frame.fillna('nan', inplace=True) # 甜宠\n\n    print(f'Start {mode} FastText ...')\n    model = FastText(data_frame[feat].values.tolist(),  size=10, window=3, min_count=1, \n                     workers=multiprocessing.cpu_count(), iter=10,min_n = 3 , max_n = 6,word_ngrams = 0)\n    stat_list = ['min', 'max', 'mean', 'std']\n    new_all = pd.DataFrame()\n    for m, t in enumerate(feat):\n        print(f'Start gen feat of {t} ...')\n        tmp = []\n        for i in data_frame[t].unique():\n            tmp_v = [i]\n            tmp_v.extend(model[i])\n            tmp.append(tmp_v)\n        tmp_df = pd.DataFrame(tmp)\n        w2c_list = [f'fast_text_{t}_{n}' for n in range(10)]\n        tmp_df.columns = [t] + w2c_list\n        tmp_df = data_frame[['user', t]].merge(tmp_df, on=t)\n        tmp_df = tmp_df.drop_duplicates().groupby('user').agg(stat_list).reset_index()\n        tmp_df.columns = ['user'] + [f'{p}_{q}' for p in w2c_list for q in stat_list]\n        if m == 0:\n            new_all = pd.concat([new_all, tmp_df], axis=1)\n        else:\n            new_all = pd.merge(new_all, tmp_df, how='left', on='user')\n    return new_all\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ---分界线---"},{"metadata":{},"cell_type":"markdown","source":"## 测试target encoder是否有效"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom collections import defaultdict\nfrom gensim import corpora\nfrom gensim import models\nDATA_PATH = '/kaggle/input/financial-risk/'\ntrain_trans = pd.read_csv(DATA_PATH+'train_trans.csv')\ntest_trans = pd.read_csv(DATA_PATH+'test_a_trans.csv')\ntrain_base = pd.read_csv(DATA_PATH+'train_base.csv')\ntrain_label = pd.read_csv(DATA_PATH+'train_label.csv')\ntrain_base = train_base.merge(train_label,on=['user'],how='left')\ntest_base= pd.read_csv(DATA_PATH+  'test_a_base.csv')\ntarger = pd.read_csv(DATA_PATH+'train_label.csv')\n\ntrain_op = pd.read_csv(DATA_PATH+'train_op.csv')\ntest_op = pd.read_csv(DATA_PATH+'test_a_op.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col1_list=['platform']\ncol2_list=['ip','type1','type2','tunnel_in','tunnel_out']\ntrans_temp ,uids= encode_CB(train_trans,col1_list,col2_list)\n#for index in uids:\n#    df = df.merge(gen_user_status_features(trans_temp,index,'amount'), on=['user'], how='left')\n#    drop_list.append(uids)\n            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"[\"LDS_{}_{}\".format('ip',i) for i in range(10)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntrain_base['city_balance_avg'] = train_base['city'].map(str) + '_' + train_base['balance_avg'].map(str)\ntrain_base['city_level'] = train_base['city'].map(str) + '_' + train_base['level'].map(str)\ntrain_base['city_product1_amount'] = train_base['city'].map(str) + '_' + train_base['product1_amount'].map(str)\ntrain_base['city_balance1_avg'] = train_base['city'].map(str) + '_' + train_base['balance1_avg'].map(str)\ntrain_base['city_balance2_avg'] = train_base['city'].map(str) + '_' + train_base['balance2_avg'].map(str)\n\ntest_base['city_balance_avg'] =  test_base['city'].map(str) + '_' + test_base['balance_avg'].map(str)\ntest_base['city_level'] =        test_base['city'].map(str) + '_' + test_base['level'].map(str)\ntest_base['city_product1_amount'] = test_base['city'].map(str) + '_' + test_base['product1_amount'].map(str)\ntest_base['city_balance1_avg'] = test_base['city'].map(str) + '_' + test_base['balance1_avg'].map(str)\ntest_base['city_balance2_avg'] = test_base['city'].map(str) + '_' + test_base['balance2_avg'].map(str)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 自己定义的target encoder"},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode_Cat_1(train,test, feature, k,noise_level=0.01,smoothing=20):\n    \n    kf = StratifiedKFold(n_splits=k, shuffle=True, random_state=2020)    \n\n    for col in feature:\n        train['target_'+col] = 0\n        test['target_' + col] =0\n        if col not in orgin_features:\n            smoothing=50\n            noise_level=0.7\n        for trn_idx,val_idx in kf.split(train,train['label']):\n            X = train.iloc[trn_idx][col]\n            enc_tar = TargetEncoder(smoothing=smoothing).fit(X,train.iloc[trn_idx]['label'])\n            train['target_'+col] += add_noise(enc_tar.transform(train[col])[col],noise_level) / k\n            test['target_'+col] += add_noise(enc_tar.transform(test[col])[col],noise_level) / k\n       \n    return  train,test\n\n\n\ndef Catboost_encoding(train,test, feature, k,noise_level=0.01,sigma=2.0,a=1):\n    \n    kf = StratifiedKFold(n_splits=k, shuffle=True, random_state=2020)    \n\n    for col in feature:\n        train['catboost_'+col] = 0\n        test['catboost_' + col] =0\n        if col not in orgin_features:\n            sigma=5.0\n            a = 2\n        for trn_idx,val_idx in kf.split(train,train['label']):\n            X = train.iloc[trn_idx][col]\n            enc_tar = CatBoostEncoder(sigma=sigma,a=a).fit(X,train.iloc[trn_idx]['label'])\n            train['catboost_'+col] += add_noise(enc_tar.transform(train[col])[col],noise_level) / k\n            test['catboost_'+col] += add_noise(enc_tar.transform(test[col])[col],noise_level) / k        \n    return  train,test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 评价编码函数\nfolds = StratifiedKFold(n_splits=10, shuffle=True, random_state=2020)\nf_cats = ['province','city',\"city_level\",'city_balance1_avg','city_balance2_avg',\n                      \"city_product1_amount\",'city_balance_avg']\n\nprint(\"%20s   %20s | %20s | %20s  | %20s\" % (\"\", \"Raw Categories\",\"Encoded Categories\",\"Baseline Categories\",\"CatBoost Categories\"))\nfor f in f_cats:\n    print(\"%-20s : \" % f, end=\"\")\n    origin_scores = []\n    baseline_scores = []\n    target_scores=[]   \n    catboost_scores=[]\n    for trn_idx, val_idx in folds.split(train_base, train_base.label):\n        \n        val_f = train_base.iloc[trn_idx].groupby([f])['label'].transform('mean')\n        val_f.name=f\n        val_tgt = train_base.label.iloc[trn_idx]\n        \n        trn_f = train_base.iloc[trn_idx][f]\n        trn_tgt = train_base.label.iloc[trn_idx]\n        \n        val_tf ,temp = encode_Cat_1(train_base.iloc[trn_idx],test_base,[f],5)\n        val_tf1 ,temp = kfold_stats_feature(train_base.iloc[trn_idx].reset_index(),test_base,[f],5)  \n        val_tf2 ,temp = Catboost_encoding(train_base.iloc[trn_idx],test_base,[f],5)\n        \n        origin_scores.append(max(roc_auc_score(val_tgt, val_f), 1 - roc_auc_score(val_tgt, val_f)))\n        target_scores.append(roc_auc_score(val_tgt, val_tf['target_'+f]))\n        baseline_scores.append(roc_auc_score(val_tgt,val_tf1[f+'_label_kfold_mean']))\n        catboost_scores.append(roc_auc_score(val_tgt, val_tf2['catboost_'+f]))\n    val_tf1 ,temp = kfold_stats_feature(train_base,test_base,[f],5)    \n    print(\" %.6f + %.6f | %6f + %.6f  | %.6f + %.6f  | %.6f + %.6f\" \n          % (np.mean( origin_scores), np.std( origin_scores), np.mean(target_scores), np.std(target_scores),\n             np.mean(baseline_scores), np.std(baseline_scores),np.mean(catboost_scores), np.std(catboost_scores)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  自己写target encoder的评价方法,没有采用5折方法\ndef encode_val(train_base,test_base,cols):\n    for col in cols:\n    \n        temp_train,temp_test=kfold_stats_feature(train_base,test_base,[col],5)\n        temp=train_base.groupby([col])['label'].agg(cnt='count',mean='mean')\n        temp=temp.reset_index()\n        temp_train=temp_train.merge(temp,on=[col],how='left')\n        \n        \n        print(\"BASELINE Encode score: {}\".format(roc_auc_score(temp_train['label'],temp_train['{}_label_kfold_mean'.format(col)])))\n        print(\"Myown Encode score:    {}\".format(roc_auc_score(temp_train['label'],temp_train['Mean_encoded_{}'.format(col)])))\n        print(\"Origin score:          {}\".format(roc_auc_score(temp_train['label'],temp_train['mean'])))\n        print()\ncols = ['province','city',\"city_level\",'city_balance1_avg','city_balance2_avg',\n                      \"city_product1_amount\",'city_balance_avg']\nencode_val(train_base,test_base,cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}