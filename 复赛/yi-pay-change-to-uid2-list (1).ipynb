{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nimport pandas as pd\nimport numpy as np\nimport warnings\nfrom sklearn.preprocessing import LabelEncoder\nimport lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.decomposition import TruncatedSVD,PCA\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import roc_auc_score\nfrom gensim.models import Word2Vec, FastText\nfrom gensim import corpora,models\nfrom sklearn import preprocessing\nfrom tqdm import tqdm\nimport multiprocessing\nwarnings.filterwarnings('ignore')\nprint(multiprocessing.cpu_count())\nimport gc\nfrom category_encoders import *\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nwarnings.filterwarnings('ignore')\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n## You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\norigin_features = []\ndrop_list=[]\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":1,"outputs":[{"output_type":"stream","text":"4\n/kaggle/input/financial-risk/testb_base.csv\n/kaggle/input/financial-risk/test_a_trans.csv\n/kaggle/input/financial-risk/train_op.csv\n/kaggle/input/financial-risk/test_a_op.csv\n/kaggle/input/financial-risk/submit_example.csv\n/kaggle/input/financial-risk/testb_op.csv\n/kaggle/input/financial-risk/train_trans.csv\n/kaggle/input/financial-risk/testb_trans.csv\n/kaggle/input/financial-risk/train_base.csv\n/kaggle/input/financial-risk/test_a_base.csv\n/kaggle/input/financial-risk/.doc\n/kaggle/input/financial-risk/train_label.csv\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def w2v_feat(df, feat, mode):\n    data_frame=df.copy()\n    for i in feat:\n        if data_frame[i].dtype != 'object':\n            data_frame[i] = data_frame[i].astype(str)\n    data_frame.fillna('nan', inplace=True)\n\n    print(f'Start {mode} word2vec ...')\n    model = Word2Vec(data_frame[feat].values.tolist(), size=10, window=2, min_count=1,\n                     workers=multiprocessing.cpu_count(), iter=10)\n    stat_list = ['min', 'max', 'mean', 'std']\n    new_all = pd.DataFrame()\n    for m, t in enumerate(feat):\n        print(f'Start gen feat of {t} ...')\n        tmp = []\n        for i in data_frame[t].unique():\n            tmp_v = [i]\n            tmp_v.extend(model[i])\n            tmp.append(tmp_v)\n        tmp_df = pd.DataFrame(tmp)\n        w2c_list = [f'w2c_{t}_{n}' for n in range(10)]\n        tmp_df.columns = [t] + w2c_list\n        tmp_df = data_frame[['user', t]].merge(tmp_df, on=t)\n        tmp_df = tmp_df.drop_duplicates().groupby('user').agg(stat_list).reset_index()\n        tmp_df.columns = ['user'] + [f'{p}_{q}' for p in w2c_list for q in stat_list]\n        if m == 0:\n            new_all = pd.concat([new_all, tmp_df], axis=1)\n        else:\n            new_all = pd.merge(new_all, tmp_df, how='left', on='user')\n    return new_all\ndef gen_user_lda_features(df, value,num_topics):\n    df[value] = df[value].astype(str)\n    df[value].fillna('-1', inplace=True)\n    group_df = df.groupby(['user']).apply(lambda x: x[value].tolist()).reset_index()\n    group_df.columns = ['user', 'list']\n    \n    texts=group_df['list']\n    dictionary = corpora.Dictionary(texts)\n    corpus = [dictionary.doc2bow(text) for text in texts]\n#    tfidf = models.TfidfModel(corpus)\n#   corpus_tfidf = tfidf[corpus]\n    lda_model = models.LdaMulticore(corpus, id2word=dictionary, num_topics=num_topics,workers=multiprocessing.cpu_count(),random_state=2020)\n    corpus_lda = lda_model.get_document_topics(corpus,minimum_probability=0)\n    \n    new_all = pd.DataFrame(corpus_lda)\n    new_all.columns = [\"LDA_{}_{}\".format(value,i) for i in range(num_topics)]\n    for col in tqdm(new_all.columns):\n        try:\n           new_all[col]=new_all[col].apply(lambda x:x[1])\n        except:\n            None\n             \n    new_all['user']=group_df['user']\n    \n    return new_all\ndef fasttext(df, feat, mode):\n    data_frame=df.copy()\n    # 转化为str\n    for i in feat:\n        if data_frame[i].dtype != 'object':\n            data_frame[i] = data_frame[i].astype(str)\n    data_frame.fillna('nan', inplace=True) # 甜宠\n\n    print(f'Start {mode} FastText ...')\n    model = FastText(data_frame[feat].values.tolist(),  size=10, window=3, min_count=1, \n                     workers=multiprocessing.cpu_count(), iter=10,min_n = 3 , max_n = 6,word_ngrams = 0)\n    stat_list = ['min', 'max', 'mean', 'std']\n    new_all = pd.DataFrame()\n    for m, t in enumerate(feat):\n        print(f'Start gen feat of {t} ...')\n        tmp = []\n        for i in data_frame[t].unique():\n            tmp_v = [i]\n            tmp_v.extend(model[i])\n            tmp.append(tmp_v)\n        tmp_df = pd.DataFrame(tmp)\n        w2c_list = [f'fast_text_{t}_{n}' for n in range(10)]\n        tmp_df.columns = [t] + w2c_list\n        tmp_df = data_frame[['user', t]].merge(tmp_df, on=t)\n        tmp_df = tmp_df.drop_duplicates().groupby('user').agg(stat_list).reset_index()\n        tmp_df.columns = ['user'] + [f'{p}_{q}' for p in w2c_list for q in stat_list]\n        if m == 0:\n            new_all = pd.concat([new_all, tmp_df], axis=1)\n        else:\n            new_all = pd.merge(new_all, tmp_df, how='left', on='user')\n    return new_all\n\n\n","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode_LE(data,test):\n    for f in tqdm([f for f in data.select_dtypes('object').columns if f not in ['user','label']]):\n        le = LabelEncoder()\n        \n        le.fit(list(data[f].values) + list(test[f].values))\n        data[f] = le.transform(list(data[f].values))\n        test[f] = le.transform(list(test[f].values))\n        \n    return data,test\n        \ndef encode_CB(df,col1_list,col2_list):\n    uids=[]\n    for col1 in col1_list:\n        for col2 in col2_list:\n            name = col1+'_'+col2\n            uids.append(name)\n            df[name] = df[col1].astype(str)+'_'+df[col2].astype(str)\n            df[name+'_FE'] = df[name].map(df[name].value_counts(dropna=True,normalize=True))\n            df[name+'_FE'] = df[name+\"_FE\"].astype('float32')\n    return df,uids\n\ndef encode_Cat(train,test, k,sigma=2.0,a=2.0):\n    \n    kf = StratifiedKFold(n_splits=k, shuffle=True, random_state=2020)    \n    \n    print(\"Cat encoding\")\n    for col in tqdm([f for f in data.select_dtypes('object').columns if f not in ['user','label']]):\n        train['catboost_'+col] = 0\n        test['catboost_' + col] =0\n        if col not in origin_features:\n            sigma=10\n            a=20\n        for trn_idx,val_idx in kf.split(train,train['label']):\n            X = train.iloc[trn_idx][col]\n            enc_cat = CatBoostEncoder(sigma=sigma).fit(X,train.iloc[trn_idx]['label'])\n            train['catboost_'+col] += enc_cat.transform(train[col])[col] / k\n            test['catboost_'+col] += enc_cat.transform(test[col])[col] / k\n    print('Cat encoding finish')\n    return  train,test\n            ","execution_count":3,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"%%time\ndef gen_user_status_features(df,index,value,aggregations=['mean','std','min','max','sum']):\n        \n    group_df = df.groupby([index])[value].agg(aggregations).reset_index() \n    if('mean' in aggregations and 'std' in aggregations):\n        group_df['CV']=group_df['std'] / group_df['mean']\n    group_df.rename(columns={i:\"{}_{}_\".format(index,value)+i for i in group_df.columns if i not in[index]},\n                   inplace=True\n                   )\n                \n    return group_df\n\n\ndef gen_user_group_features(df, col,value,aggregations=['mean','std','min','max','sum']):  \n    group_df = df.pivot_table(index='user',\n                              columns=col,\n                              values=value,\n                              dropna=False,\n                              aggfunc=aggregations).fillna(0)\n    cols=group_df.columns\n    group_df.columns = ['user_group_{}_{}_{}_{}'.format(col,value, f[1], f[0]) for f in group_df.columns]\n    for f in cols:\n        group_df['user_group_{}_{}_{}_CV'.format(col,value,f[1])] = group_df['user_group_{}_{}_{}_std'.format(col,value,f[1])] / group_df['user_group_{}_{}_{}_mean'.format(col,value,f[1])]\n    group_df.reset_index(inplace=True)\n    return group_df\n\ndef gen_user_group_nunique_features(df, col,value):\n    \n    group_df = df.pivot_table(index='user',\n                              columns=col,\n                              values=value,\n                              dropna=False,\n                              aggfunc=['count','nunique']).fillna(0)\n    group_df.columns = ['user_{}_{}_{}_{}'.format(col,value, f[1], f[0]) for f in group_df.columns]\n    group_df.reset_index(inplace=True)\n    return group_df\n\ndef gen_user_nunique_features(df, value, prefix):\n    a=\"user_{}_{}_nuniq\".format(prefix, value)\n    b=\"user_{}_{}_count\".format(prefix, value)\n    group_df = df.groupby(['user'])[value].agg(\n        a='nunique',b='count'\n    ).reset_index()\n    \n    group_df.rename(columns={'a':a,'b':b},inplace=True) \n    return group_df\n\ndef gen_user_null_features(df, value, prefix):\n    df['is_null'] = 0\n    df.loc[df[value].isnull(), 'is_null'] = 1\n\n    group_df = df.groupby(['user'])['is_null'].agg(a='sum',\n                                                    b='mean').reset_index()\n    \n    group_df.rename(columns={'a':'user_{}_{}_null_cnt'.format(prefix, value),\n                    'b':'user_{}_{}_null_ratio'.format(prefix, value)},\n                   inplace=True\n                   )\n\n    return group_df\n\n    \ndef gen_user_window_amount_features(df, window):\n    \n    group_df = df[df['days_diff']>window].groupby('user')['amount'].agg(\n        a='mean',\n         b='std',\n         c='max',\n         d='min',\n         e='sum',\n        f='median',\n         g='count',\n        ).reset_index()\n    \n    group_df.rename(columns={'a':'user_amount_mean_{}d'.format(window),\n                    'b':'user_amount_std_{}d'.format(window),\n                    'c':'user_amount_max_{}d'.format(window),\n                    'd':'user_amount_min_{}d'.format(window),\n                    'e':'user_amount_sum_{}d'.format(window),\n                    'f':'user_amount_med_{}d'.format(window),\n                    'g':'user_amount_cnt_{}d'.format(window)},\n                   inplace=True\n                   )\n    \n    return group_df\n\ndef gen_user_window_hour_amount_features(df, window):\n    \n    group_df = df[df['hour']>window].groupby('user')['amount'].agg(\n        a='mean',\n         b='std',\n         c='max',\n         d='min',\n         e='sum',\n         f='median',\n         g='count',\n        ).reset_index()\n    \n    group_df.rename(columns={'a':'user_amount_mean_{}h'.format(window),\n                    'b':'user_amount_std_{}h'.format(window),\n                    'c':'user_amount_max_{}h'.format(window),\n                    'd':'user_amount_min_{}h'.format(window),\n                    'e':'user_amount_sum_{}h'.format(window),\n                    'f':'user_amount_med_{}h'.format(window),\n                    'g':'user_amount_cnt_{}h'.format(window)},\n                   inplace=True\n                   )\n    \n    return group_df\n\n\ndef gen_user_tfidf_features(df, value):\n    #填充缺失值\n    df[value].replace(' ', np.nan, inplace=True)\n    df[value] = df[value].astype(str)\n    df[value].fillna('-1', inplace=True)\n\n    #\n    group_df = df.groupby(['user']).apply(lambda x: x[value].tolist()).reset_index()\n    group_df.columns = ['user', 'list']\n    group_df['list'] = group_df['list'].apply(lambda x: ','.join(x))\n    enc_vec = TfidfVectorizer()\n    tfidf_vec = enc_vec.fit_transform(group_df['list'])\n    svd_enc = TruncatedSVD(n_components=10, n_iter=20, random_state=2020)\n    vec_svd = svd_enc.fit_transform(tfidf_vec)\n    vec_svd = pd.DataFrame(vec_svd)\n    vec_svd.columns = ['svd_tfidf_{}_{}'.format(value, i) for i in range(10)]\n    group_df = pd.concat([group_df, vec_svd], axis=1)\n    del group_df['list']\n    return group_df\n\ndef gen_user_countvec_features(df, value):\n    df[value] = df[value].astype(str)\n    df[value].fillna('-1', inplace=True)\n    group_df = df.groupby(['user']).apply(lambda x: x[value].tolist()).reset_index()\n    group_df.columns = ['user', 'list']\n    group_df['list'] = group_df['list'].apply(lambda x: ','.join(x))\n    enc_vec = CountVectorizer()\n    tfidf_vec = enc_vec.fit_transform(group_df['list'])\n    svd_enc = TruncatedSVD(n_components=5, n_iter=20, random_state=2020)\n    vec_svd = svd_enc.fit_transform(tfidf_vec)\n    vec_svd = pd.DataFrame(vec_svd)\n    vec_svd.columns = ['svd_countvec_{}_{}'.format(value, i) for i in range(5)]\n    group_df = pd.concat([group_df, vec_svd], axis=1)\n    del group_df['list']\n    return group_df\n\n#定义加载函数\ndef load_dataset(DATA_PATH):\n    train_label = pd.read_csv(DATA_PATH+'train_label.csv')\n    test_base = pd.read_csv(DATA_PATH+'testb_base.csv')\n    \n    train_base = pd.read_csv(DATA_PATH+'train_base.csv')\n    train_op = pd.read_csv(DATA_PATH+'train_op.csv')\n    train_trans = pd.read_csv(DATA_PATH+'train_trans.csv')\n    test_op = pd.read_csv(DATA_PATH+'testb_op.csv')\n    test_trans = pd.read_csv(DATA_PATH+'testb_trans.csv')\n    \n    origin_features = list(train_base.columns)+list(train_trans.columns)+list(train_op.columns)\n    return train_label, train_base, test_base, train_op, train_trans, test_op, test_trans\n\n#tran_trans和train_op文件中的tm_diff时间转换\ndef transform_time(x):\n    day = int(x.split(' ')[0])\n    hour = int(x.split(' ')[2].split('.')[0].split(':')[0])\n    minute = int(x.split(' ')[2].split('.')[0].split(':')[1])\n    second = int(x.split(' ')[2].split('.')[0].split(':')[2])\n    return 86400*day+3600*hour+60*minute+second\n\n\n\n#讲所有的数据进行初步的整合，并且把时间days_diff进行了数字化提取\ndef data_preprocess(DATA_PATH):\n    train_label, train_base, test_base, train_op, train_trans, test_op, test_trans = load_dataset(DATA_PATH=DATA_PATH)\n    # 拼接数据\n    train_df = train_base.copy()\n    test_df = test_base.copy()\n        #将train_base数据和train_label整合\n    train_df = train_label.merge(train_df, on=['user'], how='left')\n    del train_base, test_base\n        #将train_op数据和test_op整合--方便将train和test中所有的数据进行统一的处理\n    op_df = pd.concat([train_op, test_op], axis=0, ignore_index=True)\n        #将train_trans数据和test_trans整合--方便将train和test中所有的数据进行统一的处理\n    trans_df = pd.concat([train_trans, test_trans], axis=0, ignore_index=True)\n        #将train_base数据和test_base整合\n    data = pd.concat([train_df, test_df], axis=0, ignore_index=True) \n    del train_op, test_op, train_df, test_df\n    \n    \n    # 时间维度的处理\n    op_df['days_diff'] = op_df['tm_diff'].apply(lambda x: int(x.split(' ')[0]))\n    trans_df['days_diff'] = trans_df['tm_diff'].apply(lambda x: int(x.split(' ')[0]))\n    \n    op_df['timestamp'] = op_df['tm_diff'].apply(lambda x: transform_time(x))\n    trans_df['timestamp'] = trans_df['tm_diff'].apply(lambda x: transform_time(x))\n    op_df['hour'] = op_df['tm_diff'].apply(lambda x: int(x.split(' ')[2].split('.')[0].split(':')[0]))\n    trans_df['hour'] = trans_df['tm_diff'].apply(lambda x: int(x.split(' ')[2].split('.')[0].split(':')[0]))\n    trans_df['week'] = trans_df['days_diff'].apply(lambda x: x % 7)\n    \n    \n    # 排序\n    trans_df = trans_df.sort_values(by=['user', 'timestamp'])\n    op_df = op_df.sort_values(by=['user', 'timestamp'])\n    trans_df.reset_index(inplace=True, drop=True)\n    op_df.reset_index(inplace=True, drop=True)\n\n    gc.collect()\n    return data, op_df, trans_df\n\ndef gen_features(df_, op, trans):\n    df=df_\n    # base数据处理\n    print(\"base logistic features\")\n    df['card_a_cnt_avg'] = df[\"card_a_cnt\"]/df[\"using_time\"]\n    df['product7_fail_ratio'] = df['product7_fail_cnt'] / df['product7_cnt']\n    \n    df['city_count'] = df.groupby(['city'])['user'].transform('count')  ## 与原始数据集相同数量的项目\n    df['city_product7_fail_ratio'] =  df.groupby(['city'])['product7_fail_cnt'].transform('sum')/df.groupby(['city'])['product7_cnt'].transform('sum')\n    df['province_count'] = df.groupby(['province'])['user'].transform('count')\n    df['province_product7_fail_ratio'] = df.groupby(['province'])['product7_fail_cnt'].transform('sum')/df.groupby(['province'])['product7_cnt'].transform('sum')\n    \n    df['login_cnt_period_var']=df[['login_cnt_period1','login_cnt_period2']].std(axis=1)\n    df['login_cnt_period_CV']=df['login_cnt_period_var'] / df[['login_cnt_period1','login_cnt_period2']].mean(axis=1)\n    df['card_var']=df[['card_a_cnt','card_b_cnt','card_c_cnt','card_d_cnt']].std(axis=1)\n    df['card_CV']=df['card_var'] / df[['card_a_cnt','card_b_cnt','card_c_cnt','card_d_cnt']].mean(axis=1)\n    df['op_cnt_var']=df[['op1_cnt','op2_cnt']].std(axis=1)\n    df['op_cnt_CV']=df['op_cnt_var'] / df[['op1_cnt','op2_cnt']].mean(axis=1)\n    df['login_var']=df[['login_cnt_period1','login_cnt_period2','login_cnt_avg']].std(axis=1)\n    df['login_CV']=df['login_var'] / df[['login_cnt_period1','login_cnt_period2','login_cnt_avg']].mean(axis=1)\n    \n    \n    df['ip_cnt_per_login_cnt_avg'] = df['ip_cnt']/df[['login_cnt_period1','login_cnt_period2','login_cnt_avg']].mean(axis=1)\n    df['ip_cnt_per_login_days_cnt'] = df['ip_cnt']/df['login_days_cnt']\n    df['acc_count_per_time'] = df[\"acc_count\"]/df[\"using_time\"]\n    df['ip_cnt_per_using_time'] = df[\"ip_cnt\"]/df[\"using_time\"]\n    df['agreement_total_per_using_time'] = df[\"agreement_total\"]/df[\"using_time\"]\n    df['agreement_total_per_account_cnt'] = df[\"agreement_total\"]/df[\"acc_count\"]\n    df['ip_cnt_per_acc_count'] = df[\"ip_cnt\"]/df[\"acc_count\"]\n    df['service1_amt_per_using_time'] = df[\"service1_amt\"]/df[\"using_time\"]\n    df[\"service_cnt_per_time\"]=(df[\"service1_cnt\"]+df[\"service2_cnt\"])/df[\"using_time\"]\n    \n    \n    df['op_cnt_per_usign_time'] = (df[\"op1_cnt\"]+df[\"op2_cnt\"])/df[\"using_time\"]\n    df['op_cnt_per_login_cnt_avg'] = (df[\"op1_cnt\"]+df[\"op2_cnt\"])/df['login_cnt_avg']\n    df['op_cnt_per_login_cnt_p1'] =  (df[\"op1_cnt\"]+df[\"op2_cnt\"])/df['login_cnt_period1']\n    df['op_cnt_per_login_cnt_p2'] =  (df[\"op1_cnt\"]+df[\"op2_cnt\"])/df['login_cnt_period2']\n    \n    # trans\n    df = df.merge(gen_user_status_features(trans,'user','amount'), on=['user'], how='left')\n    df = df.merge(gen_user_status_features(trans,'user','days_diff'), on=['user'], how='left')\n    df = df.merge(gen_user_status_features(trans,'user','hour'), on=['user'], how='left')\n\n  \n    print(\"trans nunique && null features\")\n    for col in tqdm(['days_diff', 'platform', 'tunnel_in', 'tunnel_out', 'type1', 'type2','ip', 'ip_3']):\n        df = df.merge(gen_user_nunique_features(df=trans, value=col, prefix='trans'), on=['user'], how='left')\n        df = df.merge(gen_user_null_features(df=trans, value=col, prefix='trans'), on=['user'], how='left')\n    df['user_amount_per_days'] = df['user_amount_sum'].div(df['user_trans_days_diff_nuniq'])\n    df['user_amount_per_tunnel_in'] = df['user_amount_sum'] / df['user_trans_tunnel_in_nuniq']\n    df['user_amount_per_ip_cnt']=df['user_amount_sum']/df['ip_cnt']\n    temp=trans[trans['ip'].isnull()].groupby(['user'])['amount'].agg(user_null_ip_amount_sum='sum',\n                                                                     user_null_ip_amount_mea='mean'\n                                                                    \n                                                                    ).reset_index()\n    \n    df=df.merge(temp,on=['user'],how='left')\n\n            \n    print(\"trans group amount features\")\n    trans_type1=trans.query(\"type1==['f67d4b5a05a1352a','19d44f1a51919482','674e8d5860bc033d', '45a1168437c708ff']\")\n    trans_type2=trans.query(\"type2==['11a213398ee0c623']\")\n    trans_plat=trans.query(\"platform==['46c69cbbce5f1568','42573d7287a8c9c2']\")\n    trans_tunnel_in=trans.query(\"tunnel_in==['b2e7fa260df4998d','9c5701005a2d85bd']\")\n    df = df.merge(gen_user_group_features(df=trans_type2,col='type2',      value='amount'),on=['user'],how='left')\n    df = df.merge(gen_user_group_features(df=trans_type1,col='type1',      value='amount'),on=['user'],how='left')\n    df = df.merge(gen_user_group_features(df=trans_tunnel_in,col='tunnel_in',  value='amount'),on=['user'],how='left')\n    df = df.merge(gen_user_group_features(df=trans,col='tunnel_out',value='amount'), on=['user'], how='left')\n    df = df.merge(gen_user_group_features(df=trans_plat,col='platform',   value='amount'),on=['user'],how='left')\n    \n    print(\"trans group days_diff features\")\n    df = df.merge(gen_user_group_features(df=trans_type2,col='type2',      value='days_diff'),on=['user'],how='left')\n    df = df.merge(gen_user_group_features(df=trans_type1,col='type1',      value='days_diff'),on=['user'],how='left')\n    df = df.merge(gen_user_group_features(df=trans_tunnel_in,col='tunnel_in',  value='days_diff'),on=['user'],how='left')\n    df = df.merge(gen_user_group_features(df=trans,col='tunnel_out',value='days_diff'), on=['user'], how='left')\n    df = df.merge(gen_user_group_features(df=trans_plat,col='platform',   value='days_diff'),on=['user'],how='left')\n    \n    \n    print(\"trans group nunique features\")\n    df = df.merge(gen_user_group_nunique_features(df=trans,col='platform',   value='type1'),on=['user'],how='left')\n    df = df.merge(gen_user_group_nunique_features(df=trans,col='platform',   value='ip'),on=['user'],how='left')\n    \n    print(\"trans days window features\")\n    for window in tqdm([20,13,15]):\n        df = df.merge(gen_user_window_amount_features(df=trans, window=window), on=['user'], how='left')\n        \n    print(\"trans hour window features\")\n    for win_hour in tqdm([9,18,19,20]):\n        df = df.merge(gen_user_window_hour_amount_features(df=trans, window=win_hour), on=['user'], how='left')\n        \n    print(\"trans tfidf--count features\")    \n    df = df.merge(gen_user_tfidf_features(df=trans, value='type1'), on=['user'], how='left')\n    df = df.merge(gen_user_tfidf_features(df=trans, value='type2'), on=['user'], how='left')\n    \n    print(\"trans LSI features\")\n    #for col in ['ip','platform','tunnel_in','tunnel_out','ip_3','type1','type2']:\n    #    df=df.merge(gen_user_lda_features(trans,col,5),on=['user'],how='left')\n        \n    #df=df.merge(fasttext(trans,['ip','type1',\"type2\",'tunnel_out','tunnel_in'],\"train\"),on=['user'],how='left')\n    # op\n    print(\"op nuique features\")\n    for col in tqdm(['op_type','op_mode','channel']):\n        df = df.merge(gen_user_nunique_features(df=op, value=col, prefix='op'), on=['user'], how='left')\n    \n    print(\"op null features\")\n    for col in tqdm(['op_device', 'ip', 'net_type', 'channel', 'ip_3']):\n        df = df.merge(gen_user_null_features(df=op, value=col, prefix='op'), on=['user'], how='left')\n    \n    print(\"op days_diff features\")\n    df = df.merge(gen_user_status_features(op,'user','days_diff'), on=['user'], how='left')\n    df = df.merge(gen_user_status_features(op,'user','hour'), on=['user'], how='left')\n\n    print(\"op tfidf--cnt features\")\n    df = df.merge(gen_user_tfidf_features(df=op, value='op_mode'), on=['user'], how='left')\n    df = df.merge(gen_user_tfidf_features(df=op, value='op_type'), on=['user'], how='left')\n    df = df.merge(gen_user_tfidf_features(df=op, value='op_device'), on=['user'], how='left')\n    df = df.merge(gen_user_countvec_features(df=op, value='op_mode'), on=['user'], how='left')\n    df = df.merge(gen_user_countvec_features(df=op, value='op_type'), on=['user'], how='left')\n    df = df.merge(gen_user_countvec_features(df=op, value='op_device'), on=['user'], how='left')\n    \n    print(\"op LSI features\")\n    #for col in [\"op_device\",\"ip\",\"channel\",\"net_type\",\"op_type\",\"op_mode\"]:\n    #    df=df.merge(gen_user_lda_features(op,col,5),on=['user'],how='left')\n    #df=df.merge(fasttext(op,['op_mode','op_type',\"op_device\",\"channel\",'ip_3',],\"train\"),on=['user'],how='left')\n    return df\ndef kfold_stats_feature(train, test, feats, k):\n    folds = StratifiedKFold(n_splits=k, shuffle=True, random_state=2020)  # 这里最好和后面模型的K折交叉验证保持一致\n\n    train['fold'] = None\n    for fold_, (trn_idx, val_idx) in enumerate(folds.split(train, train['label'])):\n        train.loc[val_idx, 'fold'] = fold_\n\n    kfold_features = []\n    for feat in feats:\n        nums_columns = ['label']\n        for f in nums_columns:\n            colname = feat + '_' + f + '_kfold_mean'\n            kfold_features.append(colname)\n            train[colname] = None\n            for fold_, (trn_idx, val_idx) in enumerate(folds.split(train, train['label'])):\n                tmp_trn = train.iloc[trn_idx]\n                order_label = tmp_trn.groupby([feat])[f].mean()\n                tmp = train.loc[train.fold == fold_, [feat]]\n                train.loc[train.fold == fold_, colname] = tmp[feat].map(order_label)\n                # fillna\n                global_mean = train[f].mean()\n                train.loc[train.fold == fold_, colname] = train.loc[train.fold == fold_, colname].fillna(global_mean)\n            train[colname] = train[colname].astype(float)\n\n        for f in nums_columns:\n            colname = feat + '_' + f + '_kfold_mean'\n            test[colname] = None\n            order_label = train.groupby([feat])[f].mean()\n            test[colname] = test[feat].map(order_label)\n            # fillna\n            global_mean = train[f].mean()\n            test[colname] = test[colname].fillna(global_mean)\n            test[colname] = test[colname].astype(float)\n    del train['fold']\n    return train, test\n\ndef lgb_model_origin(train, target, test, k):\n    feats = [f for f in train.columns if f not in ['user', 'label']]\n    print('Current num of features:', len(feats))\n    folds = StratifiedKFold(n_splits=k, shuffle=True, random_state=2020)\n    oof_probs = np.zeros(train.shape[0])\n    output_preds = 0\n    offline_score = []\n    valid_score = []\n    train_score = []\n    feature_importance_df = pd.DataFrame()\n    parameters = {\n        'learning_rate': 0.05,\n        'boosting_type': 'gbdt',\n        'objective': 'binary',\n        'metric': 'auc',\n        'num_leaves': 63,\n        'bagging_fraction': 0.9,\n        'bagging_seed':2020,\n        'min_data_in_leaf': 50,\n        'tree_learner':'voting',\n        'reg_alpha':10,\n        'reg_lambda':8,\n        'verbose': -1,\n        'nthread': 8,\n        'colsample_bytree':0.77,\n        'min_child_weight':4,\n        'min_child_samples':10,\n        'min_split_gain':0,\n        'lambda_l1': 0.8,\n        'max_bin':300\n    }\n\n    for i, (train_index, test_index) in enumerate(folds.split(train, target)):\n        train_y, test_y = target[train_index], target[test_index]\n        train_X, test_X = train[feats].iloc[train_index, :], train[feats].iloc[test_index, :]\n        dtrain = lgb.Dataset(train_X,\n                             label=train_y)\n        dval = lgb.Dataset(test_X,\n                           label=test_y)\n        lgb_model = lgb.train(\n                parameters,\n                dtrain,\n                num_boost_round=2000,\n                valid_sets=[dval],\n                early_stopping_rounds=100,\n                verbose_eval=100\n        )\n        oof_probs[test_index] = lgb_model.predict(test_X[feats], num_iteration=lgb_model.best_iteration)\n        output_preds += lgb_model.predict(test[feats], num_iteration=lgb_model.best_iteration)/folds.n_splits\n        \n        offline_score.append(lgb_model.best_score['valid_0']['auc'])\n        print(offline_score)\n        \n        # feature importance\n        fold_importance_df = pd.DataFrame()\n        fold_importance_df[\"feature\"] = feats\n        fold_importance_df[\"importance\"] = lgb_model.feature_importance(importance_type='gain')\n        fold_importance_df[\"fold\"] = i + 1\n        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n\n    valid_auc = roc_auc_score(target, oof_probs)\n    print('OOF-MEAN-AUC:%.6f, OOF-STD-AUC:%.6f' % (np.mean(offline_score), np.std(offline_score)))\n    print(\"valid_auc:  {}\".format(valid_auc))\n    print('feature importance:')\n    print(feature_importance_df.groupby(['feature'])['importance'].mean().sort_values(ascending=False).head(40))\n\n    return output_preds, oof_probs, np.mean(offline_score),feature_importance_df.groupby(['feature'])['importance'].mean().sort_values(ascending=False)\n# This way we have randomness and are able to reproduce the behaviour within this cell.\n          ","execution_count":4,"outputs":[{"output_type":"stream","text":"CPU times: user 14 µs, sys: 0 ns, total: 14 µs\nWall time: 18.6 µs\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# 读取数据"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nDATA_PATH = '/kaggle/input/financial-risk/'\nprint('读取数据...')\nbase_df, op_df, trans_df = data_preprocess(DATA_PATH=DATA_PATH)\n","execution_count":5,"outputs":[{"output_type":"stream","text":"读取数据...\nCPU times: user 57.6 s, sys: 2.98 s, total: 1min\nWall time: 1min\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# 特征工程"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nprint('开始特征工程...')\ndata = gen_features(base_df, op_df, trans_df)\ndata.to_csv('data.csv')","execution_count":6,"outputs":[{"output_type":"stream","text":"开始特征工程...\nbase logistic features\n","name":"stdout"},{"output_type":"stream","text":"\r  0%|          | 0/8 [00:00<?, ?it/s]","name":"stderr"},{"output_type":"stream","text":"trans nunique && null features\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 8/8 [00:07<00:00,  1.08it/s]\n","name":"stderr"},{"output_type":"stream","text":"trans group amount features\ntrans group days_diff features\ntrans group nunique features\n","name":"stdout"},{"output_type":"stream","text":"\r  0%|          | 0/3 [00:00<?, ?it/s]","name":"stderr"},{"output_type":"stream","text":"trans days window features\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 3/3 [00:01<00:00,  2.29it/s]\n  0%|          | 0/4 [00:00<?, ?it/s]","name":"stderr"},{"output_type":"stream","text":"trans hour window features\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 4/4 [00:01<00:00,  2.89it/s]\n","name":"stderr"},{"output_type":"stream","text":"trans tfidf--count features\n","name":"stdout"},{"output_type":"stream","text":"\r  0%|          | 0/3 [00:00<?, ?it/s]","name":"stderr"},{"output_type":"stream","text":"trans LSI features\nop nuique features\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 3/3 [00:06<00:00,  2.05s/it]\n  0%|          | 0/5 [00:00<?, ?it/s]","name":"stderr"},{"output_type":"stream","text":"op null features\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 5/5 [00:06<00:00,  1.26s/it]\n","name":"stderr"},{"output_type":"stream","text":"op days_diff features\nop tfidf--cnt features\nop LSI features\nCPU times: user 3min 12s, sys: 2.32 s, total: 3min 14s\nWall time: 3min 9s\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_=data\ndrop_list=[]","execution_count":7,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# UID特征"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nprint(\"UID trans\")\ncol1_list=['platform']\ncol2_list=['ip','type2']\ntrans_temp ,uids= encode_CB(trans_df,col1_list,col2_list)\nuids_FE=[uid+\"_FE\" for uid in uids]\ndata_=data_.merge(gen_user_status_features(trans_temp,'user','platform_ip_FE',['mean','sum']),on=['user'],how='left')\ndata_=data_.merge(gen_user_status_features(trans_temp,'user','platform_type2_FE',['sum','min']),on=['user'],how='left')\n\n\ncol1_list=['type1']\ncol2_list=['hour']\ntrans_temp ,uids= encode_CB(trans_df,col1_list,col2_list)\nuid_FE=[uid+\"_FE\" for uid in uids]\ndata_=data_.merge(gen_user_status_features(trans_temp,'user',uids_FE[0],['mean']),on=['user'],how='left')\n#data_=data_.merge(gen_user_status_features(trans_temp,'user',uids_FE[1],['mean','std']),on=['user'],how='left')\n\n\n\ncol1_list=['type2']\ncol2_list=['hour']\ntrans_temp ,uids= encode_CB(trans_df,col1_list,col2_list)\nuid_FE=[uid+\"_FE\" for uid in uids]\ndata_=data_.merge(gen_user_status_features(trans_temp,'user',uids_FE[0],['sum']),on=['user'],how='left')\n#data_=data_.merge(gen_user_status_features(trans_temp,'user',uids_FE[1],['mean','sum','CV']),on=['user'],how='left')\n\n\ncol1_list=['days_diff']\ncol2_list=['hour','week']\ntrans_temp ,uids= encode_CB(trans_df,col1_list,col2_list)\nuid_FE=[uid+\"_FE\" for uid in uids]\ndata_=data_.merge(gen_user_status_features(trans_temp,'user',uids_FE[0],['mean','max']),on=['user'],how='left')\ndata_=data_.merge(gen_user_status_features(trans_temp,'user',uids_FE[1],['sum']),on=['user'],how='left')\n#data_=data_.merge(gen_user_status_features(trans_temp,'user',uids_FE[2],['mean']),on=['user'],how='left')\n\n\n","execution_count":8,"outputs":[{"output_type":"stream","text":"UID trans\nCPU times: user 9.17 s, sys: 499 ms, total: 9.67 s\nWall time: 9.66 s\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nprint(\"UID op\")\ncol1_list=['op_device']\ncol2_list=['net_type']\ntrans_temp ,uids= encode_CB(op_df,col1_list,col2_list)\nuids_FE=[uid+\"_FE\" for uid in uids]\ndata_=data_.merge(gen_user_status_features(trans_temp,'user',uids_FE[0],['std']),on=['user'],how='left')\n\ncol1_list=['net_type']\ncol2_list=['days_diff']\ntrans_temp ,uids= encode_CB(op_df,col1_list,col2_list)\nuids_FE=[uid+\"_FE\" for uid in uids]\ndata_=data_.merge(gen_user_status_features(trans_temp,'user',uids_FE[0],['min']),on=['user'],how='left')\n\ncol1_list=['days_diff']\ncol2_list=['hour']\ntrans_temp ,uids= encode_CB(op_df,col1_list,col2_list)\nuids_FE=[uid+\"_FE\" for uid in uids]\ndata_=data_.merge(gen_user_status_features(trans_temp,'user',uids_FE[0],['min']),on=['user'],how='left')\n","execution_count":9,"outputs":[{"output_type":"stream","text":"UID op\nCPU times: user 20.1 s, sys: 1.96 s, total: 22.1 s\nWall time: 22 s\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nprint(\"UID BASE\")\ndata_['uid1']=data_['agreement1']+data_['agreement2']+data_['agreement3']+data_['agreement4']\ndrop_list+=['uid1']\ndata_=data_.merge(gen_user_status_features(data_,'uid1','user_trans_ip_count',['std']),on=['uid1'],how='left')  \n\ndata_['uid2']=data_['product1_amount']+data_['product2_amount']+data_['product3_amount']+data_['product4_amount']+data_['product5_amount']+data_['product6_amount']\ndrop_list+=['uid2']\ndata_=data_.merge(gen_user_status_features(data_,'uid2','user_days_diff_sum_x',['mean']),on=['uid2'],how='left')\ndata_=data_.merge(gen_user_status_features(data_,'uid2','user_trans_ip_count',['mean']),on=['uid2'],how='left') \n\ndata_['uid3']=data_['product1_amount']+data_['product6_amount']+data_['product3_amount']+data_['product4_amount']+data_['product5_amount']\ndrop_list+=['uid3']\ndata_=data_.merge(gen_user_status_features(data_,'uid3','user_days_diff_sum_x',['mean']),on=['uid3'],how='left') \n\n\nzip_list=[\n            [['provider'],['using_time'],['user_amount_CV','svd_countvec_op_mode_4'],['mean']],\n          [['provider'],['province'],['product7_fail_cnt'],['mean']],\n          [['provider'],['service3'],['user_amount_sum'],['sum']],\n     [['level'],['province'],['product7_fail_cnt'],['std','mean']],\n     [['level'],['province'],['user_trans_ip_count'],['std','mean']],\n     [['level'],['province'],['user_days_diff_sum_x'],['std','mean','max']],\n     [['level'],['city'],['user_trans_ip_count'],['std','mean']],\n     [['level'],['city'],['user_days_diff_sum_x'],['std','mean']],\n     [['level'],['city'],['product7_fail_cnt'],['std','mean']],\n     [['level'],['city'],['user_days_diff_sum_x'],['std','mean','max']],\n     [['level'],['using_time'],['user_days_diff_sum_x'],['std','mean']],\n     [['level'],['service3'],['user_trans_ip_count'],['std','mean']],\n    [['age'],['service3'], ['user_trans_ip_count'],['mean']],\n    [['age'],['service3'], ['user_days_diff_sum_x'],['mean']],\n    [['age'],['city'], ['user_days_diff_sum_x'],['mean','std']],\n     \n     [['using_time'],['service3'], ['user_trans_ip_count'],['mean']],\n    [['using_time'],['service3'], ['user_days_diff_sum_x'],['mean']],\n    [['regist_type'],['city'], ['product7_fail_cnt'],['std']],\n    [['province'],['login_days_cnt'], ['user_trans_ip_count'],['mean','std']],\n    [['city'],['balance'], ['product7_fail_cnt'],['std']],\n    [['city'],['balance1'], ['product7_fail_cnt'],['std']],\n    [['acc_count'],['login_days_cnt'], ['user_trans_ip_count'],['mean','std']],\n    [['balance'],['ip_cnt'], ['user_days_diff_sum_x'],['mean','std','max','sum']],\n    [['balance'],['ip_cnt'], ['product7_fail_cnt'],['std','mean','sum']],\n    [['balance'],['login_cnt_avg'], ['user_trans_ip_count'],['mean','std']],\n    [['balance'],['login_cnt_avg'], ['product7_fail_cnt'],['std','sum','mean']],\n    [['balance'],['login_cnt_avg'], ['user_days_diff_sum_x'],['mean','std']],\n    \n    [['balance'],['login_days_cnt'], ['user_days_diff_sum_x'],['mean','std']],\n    [['balance'],['login_days_cnt'], ['user_trans_ip_count'],['mean','std']],\n    [['balance'],['balance_avg'], ['product7_fail_cnt'],['std']],\n     [['balance'],['balance1_avg'], ['product7_fail_cnt'],['std','mean']],\n    [['balance'],['balance1_avg'], ['user_trans_ip_count'],['mean']],\n    [['balance'],['balance1_avg'], ['user_days_diff_sum_x'],['mean']],\n    [['balance'],['balance_avg'], ['user_trans_ip_count'],['sum']],\n    [['balance'],['acc_count'], ['user_trans_ip_count'],['mean','std']],\n    [['balance'],['acc_count'], ['product7_fail_cnt'],['mean','std','sum']],\n    [['balance1_avg'],['service3'], ['user_trans_ip_count'],['mean']],\n         ]\n\nfor i in zip_list:\n    data_ ,uids= encode_CB(data_,i[0],i[1])\n    uid_FE=[uid+\"_FE\" for uid in uids]\n    drop_list+=uid_FE\n    drop_list+=uids\n    for j in i[2]:\n        data_=data_.merge(gen_user_status_features(data_,uids[0],j,i[3]),on=[uids[0]],how='left')  ","execution_count":10,"outputs":[{"output_type":"stream","text":"UID BASE\nCPU times: user 21.3 s, sys: 5.61 s, total: 26.9 s\nWall time: 26.9 s\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in drop_list:\n    if i not in data_.columns:\n        print(i)\n        drop_list.remove(i)\ndrop_list=list(set(drop_list))\n\ntemp_list=[]\nfor i in drop_list:\n    if \"FE\" not in i:\n        temp_list.append(i)\nuid2_list=[]\nfor i in temp_list:\n    a=(data_[i].value_counts()<=3).sum()\n    b=data_[i].nunique()\n    c=a/b*100\n    if(c<20):\n        uid2_list.append(i)\n        print(\"{}:{} ---{}%\".format(a,b,c))\n        ","execution_count":11,"outputs":[{"output_type":"stream","text":"0:44 ---0.0%\n252:1352 ---18.639053254437872%\n14:89 ---15.730337078651685%\n0:6 ---0.0%\n0:11 ---0.0%\n16:132 ---12.121212121212121%\n388:1662 ---23.345367027677497%\n1:93 ---1.0752688172043012%\n1:107 ---0.9345794392523363%\n201:900 ---22.333333333333332%\n8:94 ---8.51063829787234%\n1:6 ---16.666666666666664%\n22:147 ---14.965986394557824%\n9:373 ---2.4128686327077746%\n5:421 ---1.187648456057007%\n693:2474 ---28.011317704122877%\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nprint('开始模型训练...')\nk_fold=10\n\ntrain = data_[~data_['label'].isnull()].copy()\ntest = data_[data_['label'].isnull()].copy()\ntarget = train['label']\n#train,test = encode_Cat(train,test,k=k_fold)\ntrain, test = kfold_stats_feature(train, test, temp_list, k_fold)\n\nfor i in ['service3_level']+drop_list:    \n    train.drop([i], axis=1, inplace=True)\n    test.drop([i], axis=1, inplace=True)\n        \n\ntrain,test=encode_LE(train,test)\ntrain.to_csv('train.csv')\ntest.to_csv('test.csv')\ntarget.to_csv('target.csv')\n\nlgb_preds, lgb_oof, lgb_score,feature_importance_df = lgb_model_origin(train=train, target=target, test=test, k=k_fold)\n\nsub_df = test[['user']].copy()\nsub_df['prob'] = lgb_preds\nsub_df.to_csv('sub(10folds).csv', index=False)\nfeature_importance_df.to_csv('feature(1).csv')","execution_count":12,"outputs":[{"output_type":"stream","text":"开始模型训练...\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 24/24 [00:01<00:00, 14.01it/s]\n","name":"stderr"},{"output_type":"stream","text":"Current num of features: 578\nTraining until validation scores don't improve for 100 rounds\n[100]\tvalid_0's auc: 0.745137\n[200]\tvalid_0's auc: 0.745406\nEarly stopping, best iteration is:\n[136]\tvalid_0's auc: 0.745991\n[0.7459911390393821]\nTraining until validation scores don't improve for 100 rounds\n[100]\tvalid_0's auc: 0.748888\n[200]\tvalid_0's auc: 0.751369\nEarly stopping, best iteration is:\n[183]\tvalid_0's auc: 0.752326\n[0.7459911390393821, 0.7523259196535201]\nTraining until validation scores don't improve for 100 rounds\n[100]\tvalid_0's auc: 0.735646\n[200]\tvalid_0's auc: 0.738638\n[300]\tvalid_0's auc: 0.740881\n[400]\tvalid_0's auc: 0.739449\nEarly stopping, best iteration is:\n[320]\tvalid_0's auc: 0.740977\n[0.7459911390393821, 0.7523259196535201, 0.7409767025089605]\nTraining until validation scores don't improve for 100 rounds\n[100]\tvalid_0's auc: 0.742225\n[200]\tvalid_0's auc: 0.74554\n[300]\tvalid_0's auc: 0.746696\n[400]\tvalid_0's auc: 0.747705\nEarly stopping, best iteration is:\n[395]\tvalid_0's auc: 0.747881\n[0.7459911390393821, 0.7523259196535201, 0.7409767025089605, 0.7478812959818902]\nTraining until validation scores don't improve for 100 rounds\n[100]\tvalid_0's auc: 0.726873\n[200]\tvalid_0's auc: 0.730801\n[300]\tvalid_0's auc: 0.734466\n[400]\tvalid_0's auc: 0.735157\n[500]\tvalid_0's auc: 0.735227\nEarly stopping, best iteration is:\n[477]\tvalid_0's auc: 0.735765\n[0.7459911390393821, 0.7523259196535201, 0.7409767025089605, 0.7478812959818902, 0.735764714204867]\nTraining until validation scores don't improve for 100 rounds\n[100]\tvalid_0's auc: 0.740673\n[200]\tvalid_0's auc: 0.741841\n[300]\tvalid_0's auc: 0.743892\nEarly stopping, best iteration is:\n[295]\tvalid_0's auc: 0.744177\n[0.7459911390393821, 0.7523259196535201, 0.7409767025089605, 0.7478812959818902, 0.735764714204867, 0.7441768062629692]\nTraining until validation scores don't improve for 100 rounds\n[100]\tvalid_0's auc: 0.733777\n[200]\tvalid_0's auc: 0.737747\n[300]\tvalid_0's auc: 0.73711\nEarly stopping, best iteration is:\n[275]\tvalid_0's auc: 0.7382\n[0.7459911390393821, 0.7523259196535201, 0.7409767025089605, 0.7478812959818902, 0.735764714204867, 0.7441768062629692, 0.7382003395585739]\nTraining until validation scores don't improve for 100 rounds\n[100]\tvalid_0's auc: 0.738712\n[200]\tvalid_0's auc: 0.7427\nEarly stopping, best iteration is:\n[196]\tvalid_0's auc: 0.742899\n[0.7459911390393821, 0.7523259196535201, 0.7409767025089605, 0.7478812959818902, 0.735764714204867, 0.7441768062629692, 0.7382003395585739, 0.7428992171288437]\nTraining until validation scores don't improve for 100 rounds\n[100]\tvalid_0's auc: 0.722834\n[200]\tvalid_0's auc: 0.726233\n[300]\tvalid_0's auc: 0.72552\nEarly stopping, best iteration is:\n[220]\tvalid_0's auc: 0.727529\n[0.7459911390393821, 0.7523259196535201, 0.7409767025089605, 0.7478812959818902, 0.735764714204867, 0.7441768062629692, 0.7382003395585739, 0.7428992171288437, 0.7275292397660819]\nTraining until validation scores don't improve for 100 rounds\n[100]\tvalid_0's auc: 0.746233\n[200]\tvalid_0's auc: 0.747005\n[300]\tvalid_0's auc: 0.747194\nEarly stopping, best iteration is:\n[247]\tvalid_0's auc: 0.748456\n[0.7459911390393821, 0.7523259196535201, 0.7409767025089605, 0.7478812959818902, 0.735764714204867, 0.7441768062629692, 0.7382003395585739, 0.7428992171288437, 0.7275292397660819, 0.7484561875117902]\nOOF-MEAN-AUC:0.742420, OOF-STD-AUC:0.006829\nvalid_auc:  0.7420371477192975\nfeature importance:\nfeature\nuser_trans_ip_count                              11876.679083\nuser_days_diff_sum_x                              6011.289582\nproduct7_fail_ratio                               4817.355465\nuser_amount_med_9h                                2710.851565\nage_service3_label_kfold_mean                     2632.705306\nuser_trans_ip_3_count                             2538.996083\nproduct7_fail_cnt                                 2341.393032\nuser_op_device_net_type_FE_std                    1924.443499\nuser_net_type_days_diff_FE_min                    1840.053225\nprovince_login_days_cnt_label_kfold_mean          1483.357081\nuser_group_type1_amount_45a1168437c708ff_sum      1472.287268\nuid3_label_kfold_mean                             1458.757851\nprovider_province_label_kfold_mean                1457.370053\nregist_type_city_label_kfold_mean                 1442.793305\nuid1_user_trans_ip_count_std                      1437.443052\nlogin_cnt_period_CV                               1436.295379\nage_city_label_kfold_mean                         1328.102074\nsvd_tfidf_type1_1                                 1315.184698\nproduct7_cnt                                      1264.260172\nbalance_balance_avg_label_kfold_mean              1235.268506\nsvd_countvec_op_type_3                            1213.178394\ncity_balance1_product7_fail_cnt_std               1188.696157\ncity_balance1_label_kfold_mean                    1103.486547\nage_city_user_days_diff_sum_x_CV                  1093.108565\nacc_count_login_days_cnt_label_kfold_mean         1086.540790\nsvd_countvec_op_mode_3                            1058.141203\nlevel_city_label_kfold_mean                       1058.049124\ncity_balance_product7_fail_cnt_std                1034.800477\nuser_days_diff_hour_FE_min                        1034.617792\nsvd_countvec_op_mode_4                             992.791120\nbalance_acc_count_label_kfold_mean                 946.459720\nbalance1_avg_service3_label_kfold_mean             933.926816\nsvd_tfidf_op_type_3                                926.400356\nregist_type_city_product7_fail_cnt_std             910.844149\ncity_balance_label_kfold_mean                      895.530046\nusing_time_service3_label_kfold_mean               893.597393\nprovider_using_time_label_kfold_mean               865.897910\nusing_time_service3_user_days_diff_sum_x_mean      837.131088\nlogin_cnt_period_var                               826.294218\nip_cnt_per_login_days_cnt                          823.518952\nName: importance, dtype: float64\nCPU times: user 52min 28s, sys: 11min 2s, total: 1h 3min 31s\nWall time: 18min 16s\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"f=pd.DataFrame(feature_importance_df,columns=['importance']).reset_index()","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for j in f.feature:\n    if('uid_4_' in j):\n        if (f[f.feature==j].importance.values>300):\n            print(j,f[f.feature==j].importance.values)\n            ","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfor i in drop_list:\n    for j in f.feature:\n        if((i in j ) and col1_list[0] in j):\n            if (f[f.feature==j].importance.values >400):\n\n                print(\"[['{}'],['{}'], ['{}'],['{}']],\".format(col1_list[0],i.replace(col1_list[0]+\"_\",\"\"),\n                                                        j.replace(i+\"_\",'').replace(\"_\"+j.replace(i+\"_\",'').split(\"_\")[-1],'')\n                                                                                ,j.replace(i+\"_\",'').split(\"_\")[-1],))\n","execution_count":15,"outputs":[{"output_type":"stream","text":"[['days_diff'],['age_service3'], ['user_days_diff_sum_x'],['mean']],\n[['days_diff'],['balance_ip_cnt'], ['user_days_diff_sum_x'],['CV']],\n[['days_diff'],['age_city'], ['user_days_diff_sum_x'],['CV']],\n[['days_diff'],['age_city'], ['user_days_diff_sum_x'],['std']],\n[['days_diff'],['age_city'], ['user_days_diff_sum_x'],['mean']],\n[['days_diff'],['balance_login_days_cnt'], ['user_days_diff_sum_x'],['CV']],\n[['days_diff'],['balance_login_days_cnt'], ['user_days_diff_sum_x'],['std']],\n[['days_diff'],['using_time_service3'], ['user_days_diff_sum_x'],['mean']],\n[['days_diff'],['level_city'], ['user_days_diff_sum_mean'],['x']],\n[['days_diff'],['level_city'], ['user_days_diff_sum_CV'],['x']],\n[['days_diff'],['level_city'], ['user_days_diff_sum_x'],['max']],\n[['days_diff'],['level_using_time'], ['user_days_diff_sum_x'],['CV']],\n[['days_diff'],['balance_login_cnt_avg'], ['user_days_diff_sum_x'],['CV']],\n[['days_diff'],['balance_login_cnt_avg'], ['user_days_diff_sum_x'],['std']],\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# ---分界线---"},{"metadata":{},"cell_type":"markdown","source":"## 测试target encoder是否有效"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom collections import defaultdict\nfrom gensim import corpora\nfrom gensim import models\nDATA_PATH = '/kaggle/input/financial-risk/'\ntrain_trans = pd.read_csv(DATA_PATH+'train_trans.csv')\ntest_trans = pd.read_csv(DATA_PATH+'test_a_trans.csv')\ntrain_base = pd.read_csv(DATA_PATH+'train_base.csv')\ntrain_label = pd.read_csv(DATA_PATH+'train_label.csv')\ntrain_base = train_base.merge(train_label,on=['user'],how='left')\ntest_base= pd.read_csv(DATA_PATH+  'test_a_base.csv')\ntarger = pd.read_csv(DATA_PATH+'train_label.csv')\n\ntrain_op = pd.read_csv(DATA_PATH+'train_op.csv')\ntest_op = pd.read_csv(DATA_PATH+'test_a_op.csv')","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col1_list=['platform']\ncol2_list=['ip','type1','type2','tunnel_in','tunnel_out']\ntrans_temp ,uids= encode_CB(train_trans,col1_list,col2_list)\n#for index in uids:\n#    df = df.merge(gen_user_status_features(trans_temp,index,'amount'), on=['user'], how='left')\n#    drop_list.append(uids)\n            ","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"[\"LDS_{}_{}\".format('ip',i) for i in range(10)]","execution_count":19,"outputs":[{"output_type":"execute_result","execution_count":19,"data":{"text/plain":"['LDS_ip_0',\n 'LDS_ip_1',\n 'LDS_ip_2',\n 'LDS_ip_3',\n 'LDS_ip_4',\n 'LDS_ip_5',\n 'LDS_ip_6',\n 'LDS_ip_7',\n 'LDS_ip_8',\n 'LDS_ip_9']"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntrain_base['city_balance_avg'] = train_base['city'].map(str) + '_' + train_base['balance_avg'].map(str)\ntrain_base['city_level'] = train_base['city'].map(str) + '_' + train_base['level'].map(str)\ntrain_base['city_product1_amount'] = train_base['city'].map(str) + '_' + train_base['product1_amount'].map(str)\ntrain_base['city_balance1_avg'] = train_base['city'].map(str) + '_' + train_base['balance1_avg'].map(str)\ntrain_base['city_balance2_avg'] = train_base['city'].map(str) + '_' + train_base['balance2_avg'].map(str)\n\ntest_base['city_balance_avg'] =  test_base['city'].map(str) + '_' + test_base['balance_avg'].map(str)\ntest_base['city_level'] =        test_base['city'].map(str) + '_' + test_base['level'].map(str)\ntest_base['city_product1_amount'] = test_base['city'].map(str) + '_' + test_base['product1_amount'].map(str)\ntest_base['city_balance1_avg'] = test_base['city'].map(str) + '_' + test_base['balance1_avg'].map(str)\ntest_base['city_balance2_avg'] = test_base['city'].map(str) + '_' + test_base['balance2_avg'].map(str)\n","execution_count":20,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 自己定义的target encoder"},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode_Cat_1(train,test, feature, k,noise_level=0.01,smoothing=20):\n    \n    kf = StratifiedKFold(n_splits=k, shuffle=True, random_state=2020)    \n\n    for col in feature:\n        train['target_'+col] = 0\n        test['target_' + col] =0\n        if col not in orgin_features:\n            smoothing=50\n            noise_level=0.7\n        for trn_idx,val_idx in kf.split(train,train['label']):\n            X = train.iloc[trn_idx][col]\n            enc_tar = TargetEncoder(smoothing=smoothing).fit(X,train.iloc[trn_idx]['label'])\n            train['target_'+col] += add_noise(enc_tar.transform(train[col])[col],noise_level) / k\n            test['target_'+col] += add_noise(enc_tar.transform(test[col])[col],noise_level) / k\n       \n    return  train,test\n\n\n\ndef Catboost_encoding(train,test, feature, k,noise_level=0.01,sigma=2.0,a=1):\n    \n    kf = StratifiedKFold(n_splits=k, shuffle=True, random_state=2020)    \n\n    for col in feature:\n        train['catboost_'+col] = 0\n        test['catboost_' + col] =0\n        if col not in orgin_features:\n            sigma=5.0\n            a = 2\n        for trn_idx,val_idx in kf.split(train,train['label']):\n            X = train.iloc[trn_idx][col]\n            enc_tar = CatBoostEncoder(sigma=sigma,a=a).fit(X,train.iloc[trn_idx]['label'])\n            train['catboost_'+col] += add_noise(enc_tar.transform(train[col])[col],noise_level) / k\n            test['catboost_'+col] += add_noise(enc_tar.transform(test[col])[col],noise_level) / k        \n    return  train,test","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 评价编码函数\nfolds = StratifiedKFold(n_splits=10, shuffle=True, random_state=2020)\nf_cats = ['province','city',\"city_level\",'city_balance1_avg','city_balance2_avg',\n                      \"city_product1_amount\",'city_balance_avg']\n\nprint(\"%20s   %20s | %20s | %20s  | %20s\" % (\"\", \"Raw Categories\",\"Encoded Categories\",\"Baseline Categories\",\"CatBoost Categories\"))\nfor f in f_cats:\n    print(\"%-20s : \" % f, end=\"\")\n    origin_scores = []\n    baseline_scores = []\n    target_scores=[]   \n    catboost_scores=[]\n    for trn_idx, val_idx in folds.split(train_base, train_base.label):\n        \n        val_f = train_base.iloc[trn_idx].groupby([f])['label'].transform('mean')\n        val_f.name=f\n        val_tgt = train_base.label.iloc[trn_idx]\n        \n        trn_f = train_base.iloc[trn_idx][f]\n        trn_tgt = train_base.label.iloc[trn_idx]\n        \n        val_tf ,temp = encode_Cat_1(train_base.iloc[trn_idx],test_base,[f],5)\n        val_tf1 ,temp = kfold_stats_feature(train_base.iloc[trn_idx].reset_index(),test_base,[f],5)  \n        val_tf2 ,temp = Catboost_encoding(train_base.iloc[trn_idx],test_base,[f],5)\n        \n        origin_scores.append(max(roc_auc_score(val_tgt, val_f), 1 - roc_auc_score(val_tgt, val_f)))\n        target_scores.append(roc_auc_score(val_tgt, val_tf['target_'+f]))\n        baseline_scores.append(roc_auc_score(val_tgt,val_tf1[f+'_label_kfold_mean']))\n        catboost_scores.append(roc_auc_score(val_tgt, val_tf2['catboost_'+f]))\n    val_tf1 ,temp = kfold_stats_feature(train_base,test_base,[f],5)    \n    print(\" %.6f + %.6f | %6f + %.6f  | %.6f + %.6f  | %.6f + %.6f\" \n          % (np.mean( origin_scores), np.std( origin_scores), np.mean(target_scores), np.std(target_scores),\n             np.mean(baseline_scores), np.std(baseline_scores),np.mean(catboost_scores), np.std(catboost_scores)))","execution_count":22,"outputs":[{"output_type":"stream","text":"                             Raw Categories |   Encoded Categories |  Baseline Categories  |  CatBoost Categories\nprovince             : ","name":"stdout"},{"output_type":"error","ename":"NameError","evalue":"name 'orgin_features' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-7a4dfe501ed7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mtrn_tgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrn_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mval_tf\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencode_Cat_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrn_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_base\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mval_tf1\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkfold_stats_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrn_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_base\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mval_tf2\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCatboost_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrn_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_base\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-21-b6d676777de7>\u001b[0m in \u001b[0;36mencode_Cat_1\u001b[0;34m(train, test, feature, k, noise_level, smoothing)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0morgin_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0msmoothing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mnoise_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'orgin_features' is not defined"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  自己写target encoder的评价方法,没有采用5折方法\ndef encode_val(train_base,test_base,cols):\n    for col in cols:\n    \n        temp_train,temp_test=kfold_stats_feature(train_base,test_base,[col],5)\n        temp=train_base.groupby([col])['label'].agg(cnt='count',mean='mean')\n        temp=temp.reset_index()\n        temp_train=temp_train.merge(temp,on=[col],how='left')\n        \n        \n        print(\"BASELINE Encode score: {}\".format(roc_auc_score(temp_train['label'],temp_train['{}_label_kfold_mean'.format(col)])))\n        print(\"Myown Encode score:    {}\".format(roc_auc_score(temp_train['label'],temp_train['Mean_encoded_{}'.format(col)])))\n        print(\"Origin score:          {}\".format(roc_auc_score(temp_train['label'],temp_train['mean'])))\n        print()\ncols = ['province','city',\"city_level\",'city_balance1_avg','city_balance2_avg',\n                      \"city_product1_amount\",'city_balance_avg']\nencode_val(train_base,test_base,cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}