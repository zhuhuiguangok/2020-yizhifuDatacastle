{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "yi-pay-change-to-uid2-list (1).ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "uQhHY-fEhwdd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "332f37ce-374e-4262-b883-0d9b54fc1ac4"
      },
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.decomposition import TruncatedSVD,PCA\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from gensim.models import Word2Vec, FastText\n",
        "from gensim import corpora,models\n",
        "from sklearn import preprocessing\n",
        "from tqdm import tqdm\n",
        "import multiprocessing\n",
        "warnings.filterwarnings('ignore')\n",
        "print(multiprocessing.cpu_count())\n",
        "import gc\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "warnings.filterwarnings('ignore')\n",
        "import os\n",
        "os.chdir(r\"/content/drive/My Drive/Colab Notebooks/Datacastle/code\")\n"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "EOkIWxpMhwdk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def w2v_feat(df, feat, mode):\n",
        "    data_frame=df.copy()\n",
        "    for i in feat:\n",
        "        if data_frame[i].dtype != 'object':\n",
        "            data_frame[i] = data_frame[i].astype(str)\n",
        "    data_frame.fillna('nan', inplace=True)\n",
        "\n",
        "    print(f'Start {mode} word2vec ...')\n",
        "    model = Word2Vec(data_frame[feat].values.tolist(), size=10, window=2, min_count=1,\n",
        "                     workers=multiprocessing.cpu_count(), iter=10)\n",
        "    stat_list = ['min', 'max', 'mean', 'std']\n",
        "    new_all = pd.DataFrame()\n",
        "    for m, t in enumerate(feat):\n",
        "        print(f'Start gen feat of {t} ...')\n",
        "        tmp = []\n",
        "        for i in data_frame[t].unique():\n",
        "            tmp_v = [i]\n",
        "            tmp_v.extend(model[i])\n",
        "            tmp.append(tmp_v)\n",
        "        tmp_df = pd.DataFrame(tmp)\n",
        "        w2c_list = [f'w2c_{t}_{n}' for n in range(10)]\n",
        "        tmp_df.columns = [t] + w2c_list\n",
        "        tmp_df = data_frame[['user', t]].merge(tmp_df, on=t)\n",
        "        tmp_df = tmp_df.drop_duplicates().groupby('user').agg(stat_list).reset_index()\n",
        "        tmp_df.columns = ['user'] + [f'{p}_{q}' for p in w2c_list for q in stat_list]\n",
        "        if m == 0:\n",
        "            new_all = pd.concat([new_all, tmp_df], axis=1)\n",
        "        else:\n",
        "            new_all = pd.merge(new_all, tmp_df, how='left', on='user')\n",
        "    return new_all"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "losSbNQShwdm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode_LE(data,test):\n",
        "    for f in tqdm([f for f in data.select_dtypes('object').columns if f not in ['user','label']]):\n",
        "        le = LabelEncoder()\n",
        "        \n",
        "        le.fit(list(data[f].values) + list(test[f].values))\n",
        "        data[f] = le.transform(list(data[f].values))\n",
        "        test[f] = le.transform(list(test[f].values))\n",
        "        \n",
        "    return data,test\n",
        "        \n",
        "def encode_CB(df,col1_list,col2_list):\n",
        "    uids=[]\n",
        "    for col1 in col1_list:\n",
        "        for col2 in col2_list:\n",
        "            name = col1+'_'+col2\n",
        "            uids.append(name)\n",
        "            df[name] = df[col1].astype(str)+'_'+df[col2].astype(str)\n",
        "            df[name+'_FE'] = df[name].map(df[name].value_counts(dropna=True,normalize=True))\n",
        "            df[name+'_FE'] = df[name+\"_FE\"].astype('float32')\n",
        "    return df,uids\n",
        "\n",
        "def encode_Cat(train,test, k,sigma=2.0,a=2.0):\n",
        "    \n",
        "    kf = StratifiedKFold(n_splits=k, shuffle=True, random_state=2020)    \n",
        "    \n",
        "    print(\"Cat encoding\")\n",
        "    for col in tqdm([f for f in data.select_dtypes('object').columns if f not in ['user','label']]):\n",
        "        train['catboost_'+col] = 0\n",
        "        test['catboost_' + col] =0\n",
        "        if col not in origin_features:\n",
        "            sigma=10\n",
        "            a=20\n",
        "        for trn_idx,val_idx in kf.split(train,train['label']):\n",
        "            X = train.iloc[trn_idx][col]\n",
        "            enc_cat = CatBoostEncoder(sigma=sigma).fit(X,train.iloc[trn_idx]['label'])\n",
        "            train['catboost_'+col] += enc_cat.transform(train[col])[col] / k\n",
        "            test['catboost_'+col] += enc_cat.transform(test[col])[col] / k\n",
        "    print('Cat encoding finish')\n",
        "    return  train,test\n",
        "            "
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "2kxkRzNThwdp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "909787e7-6e3f-4dd6-98e7-062941424bdc"
      },
      "source": [
        "%%time\n",
        "def gen_user_status_features(df,index,value,aggregations=['mean','std','min','max','sum']):\n",
        "        \n",
        "    group_df = df.groupby([index])[value].agg(aggregations).reset_index() \n",
        "    if('mean' in aggregations and 'std' in aggregations):\n",
        "        group_df['CV']=group_df['std'] / group_df['mean']\n",
        "    group_df.rename(columns={i:\"{}_{}_\".format(index,value)+i for i in group_df.columns if i not in[index]},\n",
        "                   inplace=True\n",
        "                   )\n",
        "                \n",
        "    return group_df\n",
        "\n",
        "\n",
        "def gen_user_group_features(df, col,value,aggregations=['mean','std','min','max','sum']):  \n",
        "    group_df = df.pivot_table(index='user',\n",
        "                              columns=col,\n",
        "                              values=value,\n",
        "                              dropna=False,\n",
        "                              aggfunc=aggregations).fillna(0)\n",
        "    cols=group_df.columns\n",
        "    group_df.columns = ['user_group_{}_{}_{}_{}'.format(col,value, f[1], f[0]) for f in group_df.columns]\n",
        "    for f in cols:\n",
        "        group_df['user_group_{}_{}_{}_CV'.format(col,value,f[1])] = group_df['user_group_{}_{}_{}_std'.format(col,value,f[1])] / group_df['user_group_{}_{}_{}_mean'.format(col,value,f[1])]\n",
        "    group_df.reset_index(inplace=True)\n",
        "    return group_df\n",
        "\n",
        "def gen_user_group_nunique_features(df, col,value):\n",
        "    \n",
        "    group_df = df.pivot_table(index='user',\n",
        "                              columns=col,\n",
        "                              values=value,\n",
        "                              dropna=False,\n",
        "                              aggfunc=['count','nunique']).fillna(0)\n",
        "    group_df.columns = ['user_{}_{}_{}_{}'.format(col,value, f[1], f[0]) for f in group_df.columns]\n",
        "    group_df.reset_index(inplace=True)\n",
        "    return group_df\n",
        "\n",
        "def gen_user_nunique_features(df, value, prefix):\n",
        "    a=\"user_{}_{}_nuniq\".format(prefix, value)\n",
        "    b=\"user_{}_{}_count\".format(prefix, value)\n",
        "    group_df = df.groupby(['user'])[value].agg(\n",
        "        a='nunique',b='count'\n",
        "    ).reset_index()\n",
        "    \n",
        "    group_df.rename(columns={'a':a,'b':b},inplace=True) \n",
        "    return group_df\n",
        "\n",
        "def gen_user_null_features(df, value, prefix):\n",
        "    df['is_null'] = 0\n",
        "    df.loc[df[value].isnull(), 'is_null'] = 1\n",
        "\n",
        "    group_df = df.groupby(['user'])['is_null'].agg(a='sum',\n",
        "                                                    b='mean').reset_index()\n",
        "    \n",
        "    group_df.rename(columns={'a':'user_{}_{}_null_cnt'.format(prefix, value),\n",
        "                    'b':'user_{}_{}_null_ratio'.format(prefix, value)},\n",
        "                   inplace=True\n",
        "                   )\n",
        "\n",
        "    return group_df\n",
        "\n",
        "    \n",
        "def gen_user_window_amount_features(df, window):\n",
        "    \n",
        "    group_df = df[df['days_diff']>window].groupby('user')['amount'].agg(\n",
        "        a='mean',\n",
        "         b='std',\n",
        "         c='max',\n",
        "         d='min',\n",
        "         e='sum',\n",
        "        f='median',\n",
        "         g='count',\n",
        "        ).reset_index()\n",
        "    \n",
        "    group_df.rename(columns={'a':'user_amount_mean_{}d'.format(window),\n",
        "                    'b':'user_amount_std_{}d'.format(window),\n",
        "                    'c':'user_amount_max_{}d'.format(window),\n",
        "                    'd':'user_amount_min_{}d'.format(window),\n",
        "                    'e':'user_amount_sum_{}d'.format(window),\n",
        "                    'f':'user_amount_med_{}d'.format(window),\n",
        "                    'g':'user_amount_cnt_{}d'.format(window)},\n",
        "                   inplace=True\n",
        "                   )\n",
        "    \n",
        "    return group_df\n",
        "\n",
        "def gen_user_window_hour_amount_features(df, window):\n",
        "    \n",
        "    group_df = df[df['hour']>window].groupby('user')['amount'].agg(\n",
        "        a='mean',\n",
        "         b='std',\n",
        "         c='max',\n",
        "         d='min',\n",
        "         e='sum',\n",
        "         f='median',\n",
        "         g='count',\n",
        "        ).reset_index()\n",
        "    \n",
        "    group_df.rename(columns={'a':'user_amount_mean_{}h'.format(window),\n",
        "                    'b':'user_amount_std_{}h'.format(window),\n",
        "                    'c':'user_amount_max_{}h'.format(window),\n",
        "                    'd':'user_amount_min_{}h'.format(window),\n",
        "                    'e':'user_amount_sum_{}h'.format(window),\n",
        "                    'f':'user_amount_med_{}h'.format(window),\n",
        "                    'g':'user_amount_cnt_{}h'.format(window)},\n",
        "                   inplace=True\n",
        "                   )\n",
        "    \n",
        "    return group_df\n",
        "\n",
        "\n",
        "def gen_user_tfidf_features(df, value):\n",
        "    #填充缺失值\n",
        "    df[value].replace(' ', np.nan, inplace=True)\n",
        "    df[value] = df[value].astype(str)\n",
        "    df[value].fillna('-1', inplace=True)\n",
        "\n",
        "    #\n",
        "    group_df = df.groupby(['user']).apply(lambda x: x[value].tolist()).reset_index()\n",
        "    group_df.columns = ['user', 'list']\n",
        "    group_df['list'] = group_df['list'].apply(lambda x: ','.join(x))\n",
        "    enc_vec = TfidfVectorizer()\n",
        "    tfidf_vec = enc_vec.fit_transform(group_df['list'])\n",
        "    svd_enc = TruncatedSVD(n_components=10, n_iter=20, random_state=2020)\n",
        "    vec_svd = svd_enc.fit_transform(tfidf_vec)\n",
        "    vec_svd = pd.DataFrame(vec_svd)\n",
        "    vec_svd.columns = ['svd_tfidf_{}_{}'.format(value, i) for i in range(10)]\n",
        "    group_df = pd.concat([group_df, vec_svd], axis=1)\n",
        "    del group_df['list']\n",
        "    return group_df\n",
        "\n",
        "def gen_user_countvec_features(df, value):\n",
        "    df[value] = df[value].astype(str)\n",
        "    df[value].fillna('-1', inplace=True)\n",
        "    group_df = df.groupby(['user']).apply(lambda x: x[value].tolist()).reset_index()\n",
        "    group_df.columns = ['user', 'list']\n",
        "    group_df['list'] = group_df['list'].apply(lambda x: ','.join(x))\n",
        "    enc_vec = CountVectorizer()\n",
        "    tfidf_vec = enc_vec.fit_transform(group_df['list'])\n",
        "    svd_enc = TruncatedSVD(n_components=5, n_iter=20, random_state=2020)\n",
        "    vec_svd = svd_enc.fit_transform(tfidf_vec)\n",
        "    vec_svd = pd.DataFrame(vec_svd)\n",
        "    vec_svd.columns = ['svd_countvec_{}_{}'.format(value, i) for i in range(5)]\n",
        "    group_df = pd.concat([group_df, vec_svd], axis=1)\n",
        "    del group_df['list']\n",
        "    return group_df\n",
        "\n",
        "def CB_transform(df,col,values,transform=\"count\"):\n",
        "  df[col+\"_\"+\"count\"]=df.groupby([col])[values].transform(transform)\n",
        "  return df\n",
        "\n",
        "\n",
        "#定义加载函数\n",
        "def load_dataset(DATA_PATH):\n",
        "    train_label = pd.read_csv(DATA_PATH+'train_label.csv')\n",
        "    test_base = pd.read_csv(DATA_PATH+'testb_base.csv')\n",
        "    \n",
        "    train_base = pd.read_csv(DATA_PATH+'train_base.csv')\n",
        "    train_op = pd.read_csv(DATA_PATH+'train_op.csv')\n",
        "    train_trans = pd.read_csv(DATA_PATH+'train_trans.csv')\n",
        "    test_op = pd.read_csv(DATA_PATH+'testb_op.csv')\n",
        "    test_trans = pd.read_csv(DATA_PATH+'testb_trans.csv')\n",
        "    \n",
        "    origin_features = list(train_base.columns)+list(train_trans.columns)+list(train_op.columns)\n",
        "    return train_label, train_base, test_base, train_op, train_trans, test_op, test_trans\n",
        "\n",
        "#tran_trans和train_op文件中的tm_diff时间转换\n",
        "def transform_time(x):\n",
        "    day = int(x.split(' ')[0])\n",
        "    hour = int(x.split(' ')[2].split('.')[0].split(':')[0])\n",
        "    minute = int(x.split(' ')[2].split('.')[0].split(':')[1])\n",
        "    second = int(x.split(' ')[2].split('.')[0].split(':')[2])\n",
        "    return 86400*day+3600*hour+60*minute+second\n",
        "\n",
        "\n",
        "\n",
        "#讲所有的数据进行初步的整合，并且把时间days_diff进行了数字化提取\n",
        "def data_preprocess(DATA_PATH):\n",
        "    train_label, train_base, test_base, train_op, train_trans, test_op, test_trans = load_dataset(DATA_PATH=DATA_PATH)\n",
        "    # 拼接数据\n",
        "    train_df = train_base.copy()\n",
        "    test_df = test_base.copy()\n",
        "        #将train_base数据和train_label整合\n",
        "    train_df = train_label.merge(train_df, on=['user'], how='left')\n",
        "    del train_base, test_base\n",
        "        #将train_op数据和test_op整合--方便将train和test中所有的数据进行统一的处理\n",
        "    op_df = pd.concat([train_op, test_op], axis=0, ignore_index=True)\n",
        "        #将train_trans数据和test_trans整合--方便将train和test中所有的数据进行统一的处理\n",
        "    trans_df = pd.concat([train_trans, test_trans], axis=0, ignore_index=True)\n",
        "        #将train_base数据和test_base整合\n",
        "    data = pd.concat([train_df, test_df], axis=0, ignore_index=True) \n",
        "    del train_op, test_op, train_df, test_df\n",
        "    \n",
        "    \n",
        "    # 时间维度的处理\n",
        "    op_df['days_diff'] = op_df['tm_diff'].apply(lambda x: int(x.split(' ')[0]))\n",
        "    trans_df['days_diff'] = trans_df['tm_diff'].apply(lambda x: int(x.split(' ')[0]))\n",
        "    \n",
        "    op_df['timestamp'] = op_df['tm_diff'].apply(lambda x: transform_time(x))\n",
        "    trans_df['timestamp'] = trans_df['tm_diff'].apply(lambda x: transform_time(x))\n",
        "    op_df['hour'] = op_df['tm_diff'].apply(lambda x: int(x.split(' ')[2].split('.')[0].split(':')[0]))\n",
        "    trans_df['hour'] = trans_df['tm_diff'].apply(lambda x: int(x.split(' ')[2].split('.')[0].split(':')[0]))\n",
        "    trans_df['week'] = trans_df['days_diff'].apply(lambda x: x % 7)\n",
        "    \n",
        "    \n",
        "    # 排序\n",
        "    trans_df = trans_df.sort_values(by=['user', 'timestamp'])\n",
        "    op_df = op_df.sort_values(by=['user', 'timestamp'])\n",
        "    trans_df.reset_index(inplace=True, drop=True)\n",
        "    op_df.reset_index(inplace=True, drop=True)\n",
        "\n",
        "    gc.collect()\n",
        "    return data, op_df, trans_df\n",
        "\n",
        "def gen_features(df_, op, trans):\n",
        "    df=df_\n",
        "    # base数据处理\n",
        "    print(\"base logistic features\")\n",
        "    df['card_a_cnt_avg'] = df[\"card_a_cnt\"]/df[\"using_time\"]\n",
        "    df['product7_fail_ratio'] = df['product7_fail_cnt'] / df['product7_cnt']\n",
        "    \n",
        "\n",
        "    df['city_product7_fail_ratio'] =  df.groupby(['city'])['product7_fail_cnt'].transform('sum')/df.groupby(['city'])['product7_cnt'].transform('sum')\n",
        "    df['province_count'] = df.groupby(['province'])['user'].transform('count')\n",
        "    df['province_product7_fail_ratio'] = df.groupby(['province'])['product7_fail_cnt'].transform('sum')/df.groupby(['province'])['product7_cnt'].transform('sum')\n",
        "    \n",
        "    for f in tqdm([f for f in data.select_dtypes('object').columns if f not in ['user','label']]):\n",
        "      df=CB_transform(df,f,\"user\",transform='count')\n",
        "      \n",
        "    df['login_cnt_period_var']=df[['login_cnt_period1','login_cnt_period2']].std(axis=1)\n",
        "    df['login_cnt_period_CV']=df['login_cnt_period_var'] / df[['login_cnt_period1','login_cnt_period2']].mean(axis=1)\n",
        "    df['card_var']=df[['card_a_cnt','card_b_cnt','card_c_cnt','card_d_cnt']].std(axis=1)\n",
        "    df['card_CV']=df['card_var'] / df[['card_a_cnt','card_b_cnt','card_c_cnt','card_d_cnt']].mean(axis=1)\n",
        "    df['op_cnt_var']=df[['op1_cnt','op2_cnt']].std(axis=1)\n",
        "    df['op_cnt_CV']=df['op_cnt_var'] / df[['op1_cnt','op2_cnt']].mean(axis=1)\n",
        "    df['login_var']=df[['login_cnt_period1','login_cnt_period2','login_cnt_avg']].std(axis=1)\n",
        "    df['login_CV']=df['login_var'] / df[['login_cnt_period1','login_cnt_period2','login_cnt_avg']].mean(axis=1)\n",
        "    \n",
        "    \n",
        "    df['ip_cnt_per_login_cnt_avg'] = df['ip_cnt']/df[['login_cnt_period1','login_cnt_period2','login_cnt_avg']].mean(axis=1)\n",
        "    df['ip_cnt_per_login_days_cnt'] = df['ip_cnt']/df['login_days_cnt']\n",
        "    df['acc_count_per_time'] = df[\"acc_count\"]/df[\"using_time\"]\n",
        "    df['ip_cnt_per_using_time'] = df[\"ip_cnt\"]/df[\"using_time\"]\n",
        "    df['agreement_total_per_using_time'] = df[\"agreement_total\"]/df[\"using_time\"]\n",
        "    df['agreement_total_per_account_cnt'] = df[\"agreement_total\"]/df[\"acc_count\"]\n",
        "    df['ip_cnt_per_acc_count'] = df[\"ip_cnt\"]/df[\"acc_count\"]\n",
        "    df['service1_amt_per_using_time'] = df[\"service1_amt\"]/df[\"using_time\"]\n",
        "    df[\"service_cnt_per_time\"]=(df[\"service1_cnt\"]+df[\"service2_cnt\"])/df[\"using_time\"]\n",
        "    \n",
        "    df['op_cnt_per_usign_time'] = (df[\"op1_cnt\"]+df[\"op2_cnt\"])/df[\"using_time\"]\n",
        "    df['op_cnt_per_login_cnt_avg'] = (df[\"op1_cnt\"]+df[\"op2_cnt\"])/df['login_cnt_avg']\n",
        "    df['op_cnt_per_login_cnt_p1'] =  df[\"op1_cnt\"]/df['login_cnt_period1']\n",
        "    df['op_cnt_per_login_cnt_p2'] =  df[\"op2_cnt\"]/df['login_cnt_period2']\n",
        "    \n",
        "    print(\"trans\")\n",
        "    # trans\n",
        "    df = df.merge(gen_user_status_features(trans,'user','amount'), on=['user'], how='left')\n",
        "    df = df.merge(gen_user_status_features(trans,'user','days_diff'), on=['user'], how='left')\n",
        "    df = df.merge(gen_user_status_features(trans,'user','hour'), on=['user'], how='left')\n",
        "  \n",
        "    print(\"trans nunique && null features\")\n",
        "    for col in tqdm(['days_diff', 'platform', 'tunnel_in', 'tunnel_out', 'type1', 'type2','ip','ip_3']):\n",
        "        df = df.merge(gen_user_nunique_features(df=trans, value=col, prefix='trans'), on=['user'], how='left')\n",
        "        df = df.merge(gen_user_null_features(df=trans, value=col, prefix='trans'), on=['user'], how='left')\n",
        "    df['user_amount_per_days'] = df['user_amount_sum'].div(df['user_trans_days_diff_nuniq'])\n",
        "    df['user_amount_per_tunnel_in'] = df['user_amount_sum'] / df['user_trans_tunnel_in_nuniq']\n",
        "    df['user_amount_per_ip_cnt']=df['user_amount_sum']/df['ip_cnt']\n",
        "    temp=trans[trans['ip'].isnull()].groupby(['user'])['amount'].agg(user_null_ip_amount_sum='sum',user_null_ip_amount_mea='mean').reset_index()\n",
        "    \n",
        "    df=df.merge(temp,on=['user'],how='left')\n",
        "      \n",
        "    print(\"\\n trans group amount features\")\n",
        "    trans_type1=trans.query(\"type1==['f67d4b5a05a1352a','19d44f1a51919482','674e8d5860bc033d', '45a1168437c708ff']\")\n",
        "    trans_type2=trans.query(\"type2==['11a213398ee0c623']\")\n",
        "    trans_plat=trans.query(\"platform==['46c69cbbce5f1568','42573d7287a8c9c2']\")\n",
        "    trans_tunnel_in=trans.query(\"tunnel_in==['b2e7fa260df4998d','9c5701005a2d85bd']\")\n",
        "\n",
        "    df = df.merge(gen_user_group_features(df=trans_type2,col='type2',      value='amount'),on=['user'],how='left')\n",
        "    df = df.merge(gen_user_group_features(df=trans_type1,col='type1',      value='amount'),on=['user'],how='left')\n",
        "    df = df.merge(gen_user_group_features(df=trans_tunnel_in,col='tunnel_in',  value='amount'),on=['user'],how='left')\n",
        "    df = df.merge(gen_user_group_features(df=trans,col='tunnel_out',       value='amount'), on=['user'], how='left')\n",
        "    df = df.merge(gen_user_group_features(df=trans_plat,col='platform',     value='amount'),on=['user'],how='left')\n",
        "    \n",
        "    print(\"trans group days_diff features\")\n",
        "    df = df.merge(gen_user_group_features(df=trans_type2,col='type2',      value='days_diff'),on=['user'],how='left')\n",
        "    df = df.merge(gen_user_group_features(df=trans_type1,col='type1',      value='days_diff'),on=['user'],how='left')\n",
        "    df = df.merge(gen_user_group_features(df=trans_tunnel_in,col='tunnel_in',  value='days_diff'),on=['user'],how='left')\n",
        "    df = df.merge(gen_user_group_features(df=trans,col='tunnel_out',       value='days_diff'), on=['user'], how='left')\n",
        "    df = df.merge(gen_user_group_features(df=trans_plat,col='platform',     value='days_diff'),on=['user'],how='left')\n",
        "\n",
        "    print(\"trans group hour features\")\n",
        "    df = df.merge(gen_user_group_features(df=trans_type2,col='type2',      value='hour'),on=['user'],how='left')\n",
        "    df = df.merge(gen_user_group_features(df=trans_type1,col='type1',      value='hour'),on=['user'],how='left')\n",
        "    df = df.merge(gen_user_group_features(df=trans_tunnel_in,col='tunnel_in',  value='hour'),on=['user'],how='left')\n",
        "    df = df.merge(gen_user_group_features(df=trans,col='tunnel_out',       value='hour'), on=['user'], how='left')\n",
        "    df = df.merge(gen_user_group_features(df=trans_plat,col='platform',     value='hour'),on=['user'],how='left')\n",
        "\n",
        "    print(\"\\n trans group nunique features\")\n",
        "    df = df.merge(gen_user_group_nunique_features(df=trans,col='platform',   value='type1'),on=['user'],how='left')\n",
        "    df = df.merge(gen_user_group_nunique_features(df=trans,col='platform',   value='type2'),on=['user'],how='left')\n",
        "    \n",
        "    print(\"\\n trans days window features\")\n",
        "    for window in tqdm([19,27,15,9,24]):\n",
        "        df = df.merge(gen_user_window_amount_features(df=trans, window=window), on=['user'], how='left')\n",
        "        \n",
        "    print(\"\\n trans hour window features\")\n",
        "    for win_hour in tqdm([10,15,20]):\n",
        "        df = df.merge(gen_user_window_hour_amount_features(df=trans, window=win_hour), on=['user'], how='left')\n",
        "        \n",
        "    print(\"\\n trans tfidf--count features\")   \n",
        "    for i in tqdm(['type1','type2']):\n",
        "      df = df.merge(gen_user_tfidf_features(df=trans, value=i), on=['user'], how='left')\n",
        "\n",
        "    print(\"\\n trans w2v_feat features\")\n",
        "    df=df.merge(w2v_feat(trans, ['amount','tunnel_in','tunnel_out','platform'], \"trans\"),on=['user'],how='left')\n",
        "\n",
        "    # op\n",
        "    print(\"\\n op nuique features\")\n",
        "    for col in tqdm(['days_diff','hour', 'op_mode', 'op_type', 'op_device', 'channel',\"net_type\",'ip']):\n",
        "        df = df.merge(gen_user_nunique_features(df=op, value=col, prefix='op'), on=['user'], how='left')\n",
        "    \n",
        "    print(\"\\n op null features\")\n",
        "    for col in tqdm(['op_device', 'ip', 'net_type', 'channel', 'ip_3']):\n",
        "        df = df.merge(gen_user_null_features(df=op, value=col, prefix='op'), on=['user'], how='left')\n",
        "    \n",
        "    print(\"\\n op days_diff features\")\n",
        "    df = df.merge(gen_user_status_features(op,'user','days_diff'), on=['user'], how='left')\n",
        "    df = df.merge(gen_user_status_features(op,'user','hour'), on=['user'], how='left')\n",
        "\n",
        "    print(\"\\n op tfidf--cnt features\")\n",
        "    for i in tqdm(['op_mode','op_type','op_device']):\n",
        "      df = df.merge(gen_user_tfidf_features(df=op, value=i), on=['user'], how='left')\n",
        "      df = df.merge(gen_user_countvec_features(df=op, value=i), on=['user'], how='left')\n",
        "    \n",
        "    print(\"op w2v_feat features\")\n",
        "    df=df.merge(w2v_feat(op,[\"op_device\",\"net_type\",\"channel\",'ip_3'],\"op\"),on=['user'],how='left')\n",
        "\n",
        "    return df\n",
        "\n",
        "def kfold_stats_feature(train, test, feats, k):\n",
        "    folds = StratifiedKFold(n_splits=k, shuffle=True, random_state=2020)  # 这里最好和后面模型的K折交叉验证保持一致\n",
        "\n",
        "    train['fold'] = None\n",
        "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(train, train['label'])):\n",
        "        train.loc[val_idx, 'fold'] = fold_\n",
        "\n",
        "    kfold_features = []\n",
        "    for feat in feats:\n",
        "        nums_columns = ['label']\n",
        "        for f in nums_columns:\n",
        "            colname = feat + '_' + f + '_kfold_mean'\n",
        "            kfold_features.append(colname)\n",
        "            train[colname] = None\n",
        "            for fold_, (trn_idx, val_idx) in enumerate(folds.split(train, train['label'])):\n",
        "                tmp_trn = train.iloc[trn_idx]\n",
        "                order_label = tmp_trn.groupby([feat])[f].mean()\n",
        "                tmp = train.loc[train.fold == fold_, [feat]]\n",
        "                train.loc[train.fold == fold_, colname] = tmp[feat].map(order_label)\n",
        "                # fillna\n",
        "                global_mean = train[f].mean()\n",
        "                train.loc[train.fold == fold_, colname] = train.loc[train.fold == fold_, colname].fillna(global_mean)\n",
        "            train[colname] = train[colname].astype(float)\n",
        "\n",
        "        for f in nums_columns:\n",
        "            colname = feat + '_' + f + '_kfold_mean'\n",
        "            test[colname] = None\n",
        "            order_label = train.groupby([feat])[f].mean()\n",
        "            test[colname] = test[feat].map(order_label)\n",
        "            # fillna\n",
        "            global_mean = train[f].mean()\n",
        "            test[colname] = test[colname].fillna(global_mean)\n",
        "            test[colname] = test[colname].astype(float)\n",
        "    del train['fold']\n",
        "    return train, test\n",
        "\n",
        "         "
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 73 µs, sys: 0 ns, total: 73 µs\n",
            "Wall time: 77 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMIXOjREhwdt",
        "colab_type": "text"
      },
      "source": [
        "# 读取数据"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "aWFAwbA2hwdt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "e78fd3a5-9a5d-4c73-da50-86c3db01904b"
      },
      "source": [
        "%%time\n",
        "DATA_PATH = '../data/'\n",
        "print('读取数据...')\n",
        "base_df, op_df, trans_df = data_preprocess(DATA_PATH=DATA_PATH)\n"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "读取数据...\n",
            "CPU times: user 43.2 s, sys: 1.54 s, total: 44.8 s\n",
            "Wall time: 47.6 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igD9UCmWhwdw",
        "colab_type": "text"
      },
      "source": [
        "# 特征工程"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "fgHtV9OPhwdx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dbc6a818-032a-476f-b51d-b95347bc241f"
      },
      "source": [
        "%%time\n",
        "print('开始特征工程...')\n",
        "data = gen_features(base_df, op_df, trans_df)\n",
        "data.to_csv('../submission/data.csv')"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "开始特征工程...\n",
            "base logistic features\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
            " 24%|██▍       | 6/25 [00:00<00:00, 52.26it/s]\u001b[A\n",
            " 44%|████▍     | 11/25 [00:00<00:00, 50.61it/s]\u001b[A\n",
            " 60%|██████    | 15/25 [00:00<00:00, 45.16it/s]\u001b[A\n",
            " 76%|███████▌  | 19/25 [00:00<00:00, 42.32it/s]\u001b[A\n",
            "100%|██████████| 25/25 [00:00<00:00, 41.69it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "trans\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "trans nunique && null features\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 12%|█▎        | 1/8 [00:00<00:05,  1.36it/s]\u001b[A\n",
            " 25%|██▌       | 2/8 [00:01<00:04,  1.38it/s]\u001b[A\n",
            " 38%|███▊      | 3/8 [00:02<00:03,  1.38it/s]\u001b[A\n",
            " 50%|█████     | 4/8 [00:02<00:02,  1.38it/s]\u001b[A\n",
            " 62%|██████▎   | 5/8 [00:03<00:02,  1.35it/s]\u001b[A\n",
            " 75%|███████▌  | 6/8 [00:04<00:01,  1.35it/s]\u001b[A\n",
            " 88%|████████▊ | 7/8 [00:05<00:00,  1.33it/s]\u001b[A\n",
            "100%|██████████| 8/8 [00:06<00:00,  1.33it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " trans group amount features\n",
            "trans group days_diff features\n",
            "trans group hour features\n",
            "\n",
            " trans group nunique features\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " trans days window features\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 20%|██        | 1/5 [00:00<00:01,  2.26it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:00<00:01,  2.48it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:01<00:00,  2.33it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:01<00:00,  2.12it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:02<00:00,  2.27it/s]\n",
            "\n",
            "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " trans hour window features\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 33%|███▎      | 1/3 [00:00<00:01,  1.78it/s]\u001b[A\n",
            " 67%|██████▋   | 2/3 [00:01<00:00,  1.90it/s]\u001b[A\n",
            "100%|██████████| 3/3 [00:01<00:00,  2.32it/s]\n",
            "\n",
            "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " trans tfidf--count features\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 50%|█████     | 1/2 [00:04<00:04,  4.79s/it]\u001b[A\n",
            "100%|██████████| 2/2 [00:09<00:00,  4.71s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " trans w2v_feat features\n",
            "Start trans word2vec ...\n",
            "Start gen feat of amount ...\n",
            "Start gen feat of tunnel_in ...\n",
            "Start gen feat of tunnel_out ...\n",
            "Start gen feat of platform ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " op nuique features\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 12%|█▎        | 1/8 [00:01<00:09,  1.35s/it]\u001b[A\n",
            " 25%|██▌       | 2/8 [00:02<00:08,  1.40s/it]\u001b[A\n",
            " 38%|███▊      | 3/8 [00:04<00:07,  1.51s/it]\u001b[A\n",
            " 50%|█████     | 4/8 [00:06<00:06,  1.62s/it]\u001b[A\n",
            " 62%|██████▎   | 5/8 [00:08<00:04,  1.66s/it]\u001b[A\n",
            " 75%|███████▌  | 6/8 [00:10<00:03,  1.69s/it]\u001b[A\n",
            " 88%|████████▊ | 7/8 [00:11<00:01,  1.61s/it]\u001b[A\n",
            "100%|██████████| 8/8 [00:13<00:00,  1.64s/it]\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " op null features\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 20%|██        | 1/5 [00:00<00:03,  1.01it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:01<00:02,  1.06it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:02<00:01,  1.09it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:03<00:00,  1.13it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:04<00:00,  1.15it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " op days_diff features\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " op tfidf--cnt features\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 33%|███▎      | 1/3 [00:17<00:35, 17.80s/it]\u001b[A\n",
            " 67%|██████▋   | 2/3 [00:35<00:17, 17.73s/it]\u001b[A\n",
            "100%|██████████| 3/3 [00:52<00:00, 17.38s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "op w2v_feat features\n",
            "Start op word2vec ...\n",
            "Start gen feat of op_device ...\n",
            "Start gen feat of net_type ...\n",
            "Start gen feat of channel ...\n",
            "Start gen feat of ip_3 ...\n",
            "CPU times: user 11min 17s, sys: 16.6 s, total: 11min 33s\n",
            "Wall time: 9min 14s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "L9p91A2lhwdz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_=data\n",
        "drop_list=[]"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xh9e4h1ehwd3",
        "colab_type": "text"
      },
      "source": [
        "# UID特征"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "waYC4-f9hwd3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "0d0f4aea-ddf9-49ba-884a-f7f17b6b3404"
      },
      "source": [
        "%%time\n",
        "print(\"UID trans\")\n",
        "col1_list=['platform']\n",
        "col2_list=['ip','type2']\n",
        "trans_temp ,uids= encode_CB(trans_df,col1_list,col2_list)\n",
        "uids_FE=[uid+\"_FE\" for uid in uids]\n",
        "data_=data_.merge(gen_user_status_features(trans_temp,'user','platform_ip_FE',['mean','sum']),on=['user'],how='left')\n",
        "data_=data_.merge(gen_user_status_features(trans_temp,'user','platform_type2_FE',['sum','min']),on=['user'],how='left')\n",
        "\n",
        "\n",
        "col1_list=['type1']\n",
        "col2_list=['hour']\n",
        "trans_temp ,uids= encode_CB(trans_df,col1_list,col2_list)\n",
        "uid_FE=[uid+\"_FE\" for uid in uids]\n",
        "data_=data_.merge(gen_user_status_features(trans_temp,'user',uids_FE[0],['mean']),on=['user'],how='left')\n",
        "\n",
        "col1_list=['type2']\n",
        "col2_list=['hour']\n",
        "trans_temp ,uids= encode_CB(trans_df,col1_list,col2_list)\n",
        "uid_FE=[uid+\"_FE\" for uid in uids]\n",
        "data_=data_.merge(gen_user_status_features(trans_temp,'user',uids_FE[0],['sum']),on=['user'],how='left')\n",
        "\n",
        "\n",
        "\n",
        "col1_list=['days_diff']\n",
        "col2_list=['hour','week']\n",
        "trans_temp ,uids= encode_CB(trans_df,col1_list,col2_list)\n",
        "uid_FE=[uid+\"_FE\" for uid in uids]\n",
        "data_=data_.merge(gen_user_status_features(trans_temp,'user',uids_FE[0],['mean','max']),on=['user'],how='left')\n",
        "data_=data_.merge(gen_user_status_features(trans_temp,'user',uids_FE[1],['sum']),on=['user'],how='left')\n"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "UID trans\n",
            "CPU times: user 10.1 s, sys: 463 ms, total: 10.6 s\n",
            "Wall time: 10.6 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "L8KALlRRhwd6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "6fa8056e-eccc-4f13-f6ab-21e8982b748d"
      },
      "source": [
        "%%time\n",
        "print(\"UID op\")\n",
        "col1_list=['op_device']\n",
        "col2_list=['net_type']\n",
        "trans_temp ,uids= encode_CB(op_df,col1_list,col2_list)\n",
        "uids_FE=[uid+\"_FE\" for uid in uids]\n",
        "data_=data_.merge(gen_user_status_features(trans_temp,'user',uids_FE[0],['std']),on=['user'],how='left')\n",
        "\n",
        "col1_list=['net_type']\n",
        "col2_list=['days_diff']\n",
        "trans_temp ,uids= encode_CB(op_df,col1_list,col2_list)\n",
        "uids_FE=[uid+\"_FE\" for uid in uids]\n",
        "data_=data_.merge(gen_user_status_features(trans_temp,'user',uids_FE[0],['min']),on=['user'],how='left')\n",
        "\n",
        "col1_list=['days_diff']\n",
        "col2_list=['hour']\n",
        "trans_temp ,uids= encode_CB(op_df,col1_list,col2_list)\n",
        "uids_FE=[uid+\"_FE\" for uid in uids]\n",
        "data_=data_.merge(gen_user_status_features(trans_temp,'user',uids_FE[0],['min']),on=['user'],how='left')\n"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "UID op\n",
            "CPU times: user 18.6 s, sys: 1.71 s, total: 20.3 s\n",
            "Wall time: 20.6 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "hEJdgcfmhwd8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "94eefe2e-9e86-43c8-fcf4-c60b33bb8dfd"
      },
      "source": [
        "%%time\n",
        "print(\"UID BASE\")\n",
        "data_['uid1']=data_['agreement1']+data_['agreement2']+data_['agreement3']+data_['agreement4']\n",
        "drop_list+=['uid1']\n",
        "data_=data_.merge(gen_user_status_features(data_,'uid1','user_trans_ip_count',['std']),on=['uid1'],how='left')  \n",
        "\n",
        "data_['uid2']=data_['product1_amount']+data_['product2_amount']+data_['product3_amount']+data_['product4_amount']+data_['product5_amount']+data_['product6_amount']\n",
        "drop_list+=['uid2']\n",
        "data_=data_.merge(gen_user_status_features(data_,'uid2','user_days_diff_sum_x',['mean']),on=['uid2'],how='left')\n",
        "data_=data_.merge(gen_user_status_features(data_,'uid2','user_trans_ip_count',['mean']),on=['uid2'],how='left') \n",
        "\n",
        "data_['uid3']=data_['product1_amount']+data_['product6_amount']+data_['product3_amount']+data_['product4_amount']+data_['product5_amount']\n",
        "drop_list+=['uid3']\n",
        "data_=data_.merge(gen_user_status_features(data_,'uid3','user_days_diff_sum_x',['mean']),on=['uid3'],how='left') \n",
        "\n",
        "\n",
        "zip_list=[\n",
        "            [['provider'],['using_time'],['user_amount_CV','svd_countvec_op_mode_4'],['mean']],\n",
        "          [['provider'],['province'],['product7_fail_cnt'],['mean']],\n",
        "          [['provider'],['service3'],['user_amount_sum'],['sum']],\n",
        "     [['level'],['province'],['product7_fail_cnt'],['std','mean']],\n",
        "     [['level'],['province'],['user_trans_ip_count'],['std','mean']],\n",
        "     [['level'],['province'],['user_days_diff_sum_x'],['std','mean','max']],\n",
        "     [['level'],['city'],['user_trans_ip_count'],['std','mean']],\n",
        "     [['level'],['city'],['user_days_diff_sum_x'],['std','mean']],\n",
        "     [['level'],['city'],['product7_fail_cnt'],['std','mean']],\n",
        "     [['level'],['city'],['user_days_diff_sum_x'],['std','mean','max']],\n",
        "     [['level'],['using_time'],['user_days_diff_sum_x'],['std','mean']],\n",
        "     [['level'],['service3'],['user_trans_ip_count'],['std','mean']],\n",
        "    [['age'],['service3'], ['user_trans_ip_count'],['mean']],\n",
        "    [['age'],['service3'], ['user_days_diff_sum_x'],['mean']],\n",
        "    [['age'],['city'], ['user_days_diff_sum_x'],['mean','std']],\n",
        "     \n",
        "     [['using_time'],['service3'], ['user_trans_ip_count'],['mean']],\n",
        "    [['using_time'],['service3'], ['user_days_diff_sum_x'],['mean']],\n",
        "    [['regist_type'],['city'], ['product7_fail_cnt'],['std']],\n",
        "    [['province'],['login_days_cnt'], ['user_trans_ip_count'],['mean','std']],\n",
        "    [['city'],['balance'], ['product7_fail_cnt'],['std']],\n",
        "    [['city'],['balance1'], ['product7_fail_cnt'],['std']],\n",
        "    [['acc_count'],['login_days_cnt'], ['user_trans_ip_count'],['mean','std']],\n",
        "    [['balance'],['ip_cnt'], ['user_days_diff_sum_x'],['mean','std','max','sum']],\n",
        "    [['balance'],['ip_cnt'], ['product7_fail_cnt'],['std','mean','sum']],\n",
        "    [['balance'],['login_cnt_avg'], ['user_trans_ip_count'],['mean','std']],\n",
        "    [['balance'],['login_cnt_avg'], ['product7_fail_cnt'],['std','sum','mean']],\n",
        "    [['balance'],['login_cnt_avg'], ['user_days_diff_sum_x'],['mean','std']],\n",
        "    \n",
        "    [['balance'],['login_days_cnt'], ['user_days_diff_sum_x'],['mean','std']],\n",
        "    [['balance'],['login_days_cnt'], ['user_trans_ip_count'],['mean','std']],\n",
        "    [['balance'],['balance_avg'], ['product7_fail_cnt'],['std']],\n",
        "     [['balance'],['balance1_avg'], ['product7_fail_cnt'],['std','mean']],\n",
        "    [['balance'],['balance1_avg'], ['user_trans_ip_count'],['mean']],\n",
        "    [['balance'],['balance1_avg'], ['user_days_diff_sum_x'],['mean']],\n",
        "    [['balance'],['balance_avg'], ['user_trans_ip_count'],['sum']],\n",
        "    [['balance'],['acc_count'], ['user_trans_ip_count'],['mean','std']],\n",
        "    [['balance'],['acc_count'], ['product7_fail_cnt'],['mean','std','sum']],\n",
        "    [['balance1_avg'],['service3'], ['user_trans_ip_count'],['mean']],\n",
        "         ]\n",
        "\n",
        "for i in zip_list:\n",
        "    data_ ,uids= encode_CB(data_,i[0],i[1])\n",
        "    uid_FE=[uid+\"_FE\" for uid in uids]\n",
        "    drop_list+=uid_FE\n",
        "    drop_list+=uids\n",
        "    for j in i[2]:\n",
        "        data_=data_.merge(gen_user_status_features(data_,uids[0],j,i[3]),on=[uids[0]],how='left')  "
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "UID BASE\n",
            "CPU times: user 3min 18s, sys: 4.38 s, total: 3min 23s\n",
            "Wall time: 3min 23s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "xkU8bdrwhwd_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "outputId": "2cccdeaa-a94c-4ed4-86b0-5ad581c1bf25"
      },
      "source": [
        "for i in drop_list:\n",
        "    if i not in data_.columns:\n",
        "        print(i)\n",
        "        drop_list.remove(i)\n",
        "drop_list=list(set(drop_list))\n",
        "\n",
        "temp_list=[]\n",
        "for i in drop_list:\n",
        "    if \"FE\" not in i:\n",
        "        temp_list.append(i)\n",
        "uid2_list=[]\n",
        "for i in temp_list:\n",
        "    a=(data_[i].value_counts()<=3).sum()\n",
        "    b=data_[i].nunique()\n",
        "    c=a/b*100\n",
        "    if(c<20):\n",
        "        uid2_list.append(i)\n",
        "        print(\"{}:{} ---{}%\".format(a,b,c))\n",
        "        "
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16:132 ---12.121212121212121%\n",
            "1:93 ---1.0752688172043012%\n",
            "252:1352 ---18.639053254437872%\n",
            "5:421 ---1.187648456057007%\n",
            "0:11 ---0.0%\n",
            "1:107 ---0.9345794392523363%\n",
            "1:6 ---16.666666666666664%\n",
            "14:89 ---15.730337078651685%\n",
            "9:373 ---2.4128686327077746%\n",
            "0:44 ---0.0%\n",
            "0:6 ---0.0%\n",
            "22:147 ---14.965986394557824%\n",
            "8:94 ---8.51063829787234%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLY1sz5Ee1Gu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lgb_model_origin(train, target, test, k):\n",
        "    feats = [f for f in train.columns if f not in ['user', 'label']]\n",
        "    print('Current num of features:', len(feats))\n",
        "    folds = StratifiedKFold(n_splits=k, shuffle=True, random_state=2020)\n",
        "    oof_probs = np.zeros(train.shape[0])\n",
        "    output_preds = 0\n",
        "    offline_score = []\n",
        "    valid_score = []\n",
        "    train_score = []\n",
        "    feature_importance_df = pd.DataFrame()\n",
        "    parameters = {\n",
        "        'learning_rate': 0.03,\n",
        "        'boosting_type': 'gbdt',\n",
        "        'objective': 'binary',\n",
        "        'metric': 'auc',\n",
        "        'num_leaves': 63,\n",
        "        'bagging_fraction': 0.8,\n",
        "        'bagging_seed':2020,\n",
        "        'min_data_in_leaf': 50,\n",
        "        'tree_learner':'voting',\n",
        "        'reg_alpha':10,\n",
        "        'reg_lambda':8,\n",
        "        'verbose': 1,\n",
        "        'nthread': 8,\n",
        "        'colsample_bytree':0.77,\n",
        "        'min_child_weight':4,\n",
        "        'min_child_samples':10,\n",
        "        'min_split_gain':0,\n",
        "        'lambda_l1': 0.8,\n",
        "        'max_bin':300\n",
        "    }\n",
        "\n",
        "    for i, (train_index, test_index) in enumerate(folds.split(train, target)):\n",
        "        train_y, test_y = target[train_index], target[test_index]\n",
        "        train_X, test_X = train[feats].iloc[train_index, :], train[feats].iloc[test_index, :]\n",
        "        dtrain = lgb.Dataset(train_X,\n",
        "                             label=train_y)\n",
        "        dval = lgb.Dataset(test_X,\n",
        "                           label=test_y)\n",
        "        lgb_model = lgb.train(\n",
        "                parameters,\n",
        "                dtrain,\n",
        "                num_boost_round=2000,\n",
        "                valid_sets=[dval],\n",
        "                early_stopping_rounds=100,\n",
        "                verbose_eval=100\n",
        "        )\n",
        "        oof_probs[test_index] = lgb_model.predict(test_X[feats], num_iteration=lgb_model.best_iteration)\n",
        "        output_preds += lgb_model.predict(test[feats], num_iteration=lgb_model.best_iteration)/folds.n_splits\n",
        "        \n",
        "        offline_score.append(lgb_model.best_score['valid_0']['auc'])\n",
        "        print(offline_score)\n",
        "        \n",
        "        # feature importance\n",
        "        fold_importance_df = pd.DataFrame()\n",
        "        fold_importance_df[\"feature\"] = feats\n",
        "        fold_importance_df[\"importance\"] = lgb_model.feature_importance(importance_type='gain')\n",
        "        fold_importance_df[\"fold\"] = i + 1\n",
        "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
        "\n",
        "    valid_auc = roc_auc_score(target, oof_probs)\n",
        "    print('OOF-MEAN-AUC:%.6f, OOF-STD-AUC:%.6f' % (np.mean(offline_score), np.std(offline_score)))\n",
        "    print(\"valid_auc:  {}\".format(valid_auc))\n",
        "    print('feature importance:')\n",
        "    print(feature_importance_df.groupby(['feature'])['importance'].mean().sort_values(ascending=False).head(40))\n",
        "\n",
        "    return output_preds, oof_probs, np.mean(offline_score),feature_importance_df.groupby(['feature'])['importance'].mean().sort_values(ascending=False)\n",
        " "
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "uNx5GVp3hweC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "outputId": "92d310ff-e634-433b-9b99-54a5b8ec2ce4"
      },
      "source": [
        "%%time\n",
        "print('开始模型训练...')\n",
        "k_fold=10\n",
        "\n",
        "data_['city_level'] = data_['city'].map(str) + '_' + data_['level'].map(str)\n",
        "data_['city_product1_amount'] = data_['city'].map(str) + '_' + data_['product1_amount'].map(str)\n",
        "data_['city_balance1_avg'] = data_['city'].map(str) + '_' + data_['balance1_avg'].map(str)\n",
        "data_['city_balance2_avg'] = data_['city'].map(str) + '_' + data_['balance2_avg'].map(str)\n",
        "data_['city_card_a_cnt'] = data_['city'].map(str) + '_' + data_['card_a_cnt'].map(str)\n",
        "\n",
        "# for i in tqdm(drop_list):\n",
        "#   df = df.merge(gen_user_tfidf_features(df=data_, value=i), on=['user'], how='left')\n",
        "\n",
        "train = data_[~data_['label'].isnull()].copy()\n",
        "test = data_[data_['label'].isnull()].copy()\n",
        "target = train['label']\n",
        "\n",
        "uid2_list+=['city_level','city_product1_amount','city_balance2_avg','city_balance1_avg','city_card_a_cnt']\n",
        "drop_list+=['city_level','city_product1_amount','city_balance2_avg','city_balance1_avg','city_card_a_cnt']\n",
        "train, test = kfold_stats_feature(train, test, uid2_list, k_fold)\n",
        "\n",
        "for i in ['service3_level']+drop_list:    \n",
        "    train.drop([i], axis=1, inplace=True)\n",
        "    test.drop([i], axis=1, inplace=True)\n",
        "        \n",
        "train,test=encode_LE(train,test)\n",
        "train.to_csv('../submission/train.csv')\n",
        "test.to_csv('../submission/test.csv')\n",
        "target.to_csv('../submission/target.csv')"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "开始模型训练...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/24 [00:00<?, ?it/s]\u001b[A\n",
            "  8%|▊         | 2/24 [00:00<00:01, 15.10it/s]\u001b[A\n",
            " 17%|█▋        | 4/24 [00:00<00:01, 15.45it/s]\u001b[A\n",
            " 25%|██▌       | 6/24 [00:00<00:01, 15.39it/s]\u001b[A\n",
            " 33%|███▎      | 8/24 [00:00<00:01, 15.67it/s]\u001b[A\n",
            " 42%|████▏     | 10/24 [00:00<00:00, 15.20it/s]\u001b[A\n",
            " 50%|█████     | 12/24 [00:00<00:00, 14.54it/s]\u001b[A\n",
            " 58%|█████▊    | 14/24 [00:00<00:00, 14.71it/s]\u001b[A\n",
            " 67%|██████▋   | 16/24 [00:01<00:00, 15.15it/s]\u001b[A\n",
            " 75%|███████▌  | 18/24 [00:01<00:00, 16.03it/s]\u001b[A\n",
            " 88%|████████▊ | 21/24 [00:01<00:00, 17.44it/s]\u001b[A\n",
            "100%|██████████| 24/24 [00:01<00:00, 16.80it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2min 35s, sys: 4.76 s, total: 2min 40s\n",
            "Wall time: 2min 46s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lD0Urjq-SL4X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "60b7177e-020b-4507-f65d-ff7c92dcc2f0"
      },
      "source": [
        "lgb_preds, lgb_oof, lgb_score,feature_importance_df = lgb_model_origin(train=train, target=target, test=test, k=k_fold)\n",
        "sub_df = test[['user']].copy()\n",
        "sub_df['prob'] = lgb_preds\n",
        "sub_df.to_csv('../submission/sub{%.5f}.csv'%(lgb_score,), index=False)\n",
        "feature_importance_df.to_csv('../submission/feature(1).csv')"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current num of features: 1009\n",
            "Training until validation scores don't improve for 100 rounds.\n",
            "[100]\tvalid_0's auc: 0.745526\n",
            "[200]\tvalid_0's auc: 0.748916\n",
            "[300]\tvalid_0's auc: 0.750522\n",
            "Early stopping, best iteration is:\n",
            "[248]\tvalid_0's auc: 0.751302\n",
            "[0.7513023405590505]\n",
            "Training until validation scores don't improve for 100 rounds.\n",
            "[100]\tvalid_0's auc: 0.749418\n",
            "[200]\tvalid_0's auc: 0.756221\n",
            "[300]\tvalid_0's auc: 0.755224\n",
            "Early stopping, best iteration is:\n",
            "[243]\tvalid_0's auc: 0.756403\n",
            "[0.7513023405590505, 0.7564030270784055]\n",
            "Training until validation scores don't improve for 100 rounds.\n",
            "[100]\tvalid_0's auc: 0.735691\n",
            "[200]\tvalid_0's auc: 0.740618\n",
            "[300]\tvalid_0's auc: 0.74196\n",
            "[400]\tvalid_0's auc: 0.741934\n",
            "Early stopping, best iteration is:\n",
            "[310]\tvalid_0's auc: 0.74247\n",
            "[0.7513023405590505, 0.7564030270784055, 0.7424695812110923]\n",
            "Training until validation scores don't improve for 100 rounds.\n",
            "[100]\tvalid_0's auc: 0.742229\n",
            "[200]\tvalid_0's auc: 0.745846\n",
            "[300]\tvalid_0's auc: 0.746821\n",
            "[400]\tvalid_0's auc: 0.747703\n",
            "[500]\tvalid_0's auc: 0.748443\n",
            "[600]\tvalid_0's auc: 0.747499\n",
            "Early stopping, best iteration is:\n",
            "[500]\tvalid_0's auc: 0.748443\n",
            "[0.7513023405590505, 0.7564030270784055, 0.7424695812110923, 0.7484429824561404]\n",
            "Training until validation scores don't improve for 100 rounds.\n",
            "[100]\tvalid_0's auc: 0.728833\n",
            "[200]\tvalid_0's auc: 0.734024\n",
            "[300]\tvalid_0's auc: 0.737929\n",
            "[400]\tvalid_0's auc: 0.739439\n",
            "[500]\tvalid_0's auc: 0.741423\n",
            "[600]\tvalid_0's auc: 0.742532\n",
            "[700]\tvalid_0's auc: 0.742144\n",
            "Early stopping, best iteration is:\n",
            "[607]\tvalid_0's auc: 0.7427\n",
            "[0.7513023405590505, 0.7564030270784055, 0.7424695812110923, 0.7484429824561404, 0.7426999622712696]\n",
            "Training until validation scores don't improve for 100 rounds.\n",
            "[100]\tvalid_0's auc: 0.730505\n",
            "[200]\tvalid_0's auc: 0.733493\n",
            "[300]\tvalid_0's auc: 0.735643\n",
            "[400]\tvalid_0's auc: 0.736377\n",
            "[500]\tvalid_0's auc: 0.736735\n",
            "[600]\tvalid_0's auc: 0.736758\n",
            "Early stopping, best iteration is:\n",
            "[541]\tvalid_0's auc: 0.737503\n",
            "[0.7513023405590505, 0.7564030270784055, 0.7424695812110923, 0.7484429824561404, 0.7426999622712696, 0.7375028296547821]\n",
            "Training until validation scores don't improve for 100 rounds.\n",
            "[100]\tvalid_0's auc: 0.732783\n",
            "[200]\tvalid_0's auc: 0.736668\n",
            "[300]\tvalid_0's auc: 0.737013\n",
            "[400]\tvalid_0's auc: 0.737585\n",
            "[500]\tvalid_0's auc: 0.738345\n",
            "Early stopping, best iteration is:\n",
            "[451]\tvalid_0's auc: 0.738885\n",
            "[0.7513023405590505, 0.7564030270784055, 0.7424695812110923, 0.7484429824561404, 0.7426999622712696, 0.7375028296547821, 0.7388846444067158]\n",
            "Training until validation scores don't improve for 100 rounds.\n",
            "[100]\tvalid_0's auc: 0.734872\n",
            "[200]\tvalid_0's auc: 0.740767\n",
            "[300]\tvalid_0's auc: 0.741697\n",
            "[400]\tvalid_0's auc: 0.743392\n",
            "[500]\tvalid_0's auc: 0.744448\n",
            "[600]\tvalid_0's auc: 0.744367\n",
            "Early stopping, best iteration is:\n",
            "[530]\tvalid_0's auc: 0.745125\n",
            "[0.7513023405590505, 0.7564030270784055, 0.7424695812110923, 0.7484429824561404, 0.7426999622712696, 0.7375028296547821, 0.7388846444067158, 0.7451249764195434]\n",
            "Training until validation scores don't improve for 100 rounds.\n",
            "[100]\tvalid_0's auc: 0.715853\n",
            "[200]\tvalid_0's auc: 0.718298\n",
            "[300]\tvalid_0's auc: 0.71927\n",
            "[400]\tvalid_0's auc: 0.719807\n",
            "[500]\tvalid_0's auc: 0.719958\n",
            "[600]\tvalid_0's auc: 0.720806\n",
            "[700]\tvalid_0's auc: 0.721669\n",
            "[800]\tvalid_0's auc: 0.722085\n",
            "Early stopping, best iteration is:\n",
            "[777]\tvalid_0's auc: 0.722355\n",
            "[0.7513023405590505, 0.7564030270784055, 0.7424695812110923, 0.7484429824561404, 0.7426999622712696, 0.7375028296547821, 0.7388846444067158, 0.7451249764195434, 0.7223547443878513]\n",
            "Training until validation scores don't improve for 100 rounds.\n",
            "[100]\tvalid_0's auc: 0.742024\n",
            "[200]\tvalid_0's auc: 0.747736\n",
            "[300]\tvalid_0's auc: 0.74843\n",
            "Early stopping, best iteration is:\n",
            "[296]\tvalid_0's auc: 0.748498\n",
            "[0.7513023405590505, 0.7564030270784055, 0.7424695812110923, 0.7484429824561404, 0.7426999622712696, 0.7375028296547821, 0.7388846444067158, 0.7451249764195434, 0.7223547443878513, 0.7484979249198265]\n",
            "OOF-MEAN-AUC:0.743368, OOF-STD-AUC:0.008862\n",
            "valid_auc:  0.7427422797394108\n",
            "feature importance:\n",
            "feature\n",
            "user_trans_ip_count                                 17827.941703\n",
            "user_days_diff_sum_x                                10034.947567\n",
            "product7_fail_ratio                                  7615.411976\n",
            "user_trans_ip_3_count                                4218.679570\n",
            "user_group_type1_hour_45a1168437c708ff_sum           4194.705505\n",
            "product7_fail_cnt                                    3674.518963\n",
            "age_service3_label_kfold_mean                        3390.499856\n",
            "city_product1_amount_label_kfold_mean                2860.779925\n",
            "w2c_amount_2_max                                     2449.441889\n",
            "uid1_user_trans_ip_count_std                         2418.433371\n",
            "login_cnt_period_CV                                  2253.624163\n",
            "user_op_device_net_type_FE_std                       2214.258006\n",
            "user_net_type_days_diff_FE_min                       2176.968158\n",
            "city_card_a_cnt_label_kfold_mean                     2046.355811\n",
            "provider_province_label_kfold_mean                   2026.660632\n",
            "product7_cnt                                         1987.795176\n",
            "uid3_user_days_diff_sum_x_mean                       1902.879636\n",
            "user_amount_cnt_9d                                   1818.365992\n",
            "balance_balance_avg_label_kfold_mean                 1741.408616\n",
            "regist_type_city_label_kfold_mean                    1725.937079\n",
            "city_balance1_product7_fail_cnt_std                  1692.972937\n",
            "user_amount_med_10h                                  1681.248612\n",
            "svd_countvec_op_mode_3                               1637.893786\n",
            "login_cnt_period_var                                 1632.361596\n",
            "svd_countvec_op_type_3                               1570.812030\n",
            "balance1_avg_service3_label_kfold_mean               1562.387782\n",
            "age_city_user_days_diff_sum_x_CV                     1530.588277\n",
            "user_group_type1_amount_45a1168437c708ff_sum         1487.228244\n",
            "user_days_diff_hour_FE_min                           1403.880301\n",
            "using_time_service3_user_days_diff_sum_x_mean        1333.440851\n",
            "regist_type_city_product7_fail_cnt_std               1321.107837\n",
            "user_group_type1_days_diff_45a1168437c708ff_mean     1312.322167\n",
            "city_balance2_avg_label_kfold_mean                   1310.669109\n",
            "svd_tfidf_type2_3                                    1309.534143\n",
            "using_time_service3_label_kfold_mean                 1296.095257\n",
            "city_balance_product7_fail_cnt_std                   1292.928735\n",
            "w2c_channel_3_max                                    1271.353918\n",
            "w2c_amount_6_mean                                    1251.225198\n",
            "service3_level_count                                 1247.855819\n",
            "svd_tfidf_op_mode_5                                  1206.283998\n",
            "Name: importance, dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "vEM1VL8vhweF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f=pd.DataFrame(feature_importance_df,columns=['importance']).reset_index()"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "uYm9vXZkhweH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for j in f.feature:\n",
        "    if('uid_4_' in j):\n",
        "        if (f[f.feature==j].importance.values>300):\n",
        "            print(j,f[f.feature==j].importance.values)\n",
        "            "
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "m9cKXhLPhweK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "outputId": "19328d06-c5e8-41c7-8100-ea388a29ec1a"
      },
      "source": [
        "\n",
        "for i in drop_list:\n",
        "    for j in f.feature:\n",
        "        if((i in j ) and col1_list[0] in j):\n",
        "            if (f[f.feature==j].importance.values >400):\n",
        "\n",
        "                print(\"[['{}'],['{}'], ['{}'],['{}']],\".format(col1_list[0],i.replace(col1_list[0]+\"_\",\"\"),\n",
        "                                                        j.replace(i+\"_\",'').replace(\"_\"+j.replace(i+\"_\",'').split(\"_\")[-1],'')\n",
        "                                                                                ,j.replace(i+\"_\",'').split(\"_\")[-1],))\n"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['days_diff'],['balance_ip_cnt'], ['user_days_diff_sum_x'],['CV']],\n",
            "[['days_diff'],['balance_ip_cnt'], ['user_days_diff_sum_x'],['max']],\n",
            "[['days_diff'],['balance_ip_cnt'], ['user_days_diff_sum_x'],['std']],\n",
            "[['days_diff'],['balance_ip_cnt'], ['user_days_diff_x'],['sum']],\n",
            "[['days_diff'],['balance_ip_cnt'], ['user_days_diff_sum_x'],['mean']],\n",
            "[['days_diff'],['using_time_service3'], ['user_days_diff_sum_x'],['mean']],\n",
            "[['days_diff'],['balance_login_cnt_avg'], ['user_days_diff_sum_x'],['CV']],\n",
            "[['days_diff'],['balance_login_cnt_avg'], ['user_days_diff_sum_x'],['std']],\n",
            "[['days_diff'],['uid2'], ['user_days_diff_sum_x'],['mean']],\n",
            "[['days_diff'],['uid3'], ['user_days_diff_sum_x'],['mean']],\n",
            "[['days_diff'],['age_city'], ['user_days_diff_sum_x'],['CV']],\n",
            "[['days_diff'],['age_city'], ['user_days_diff_sum_x'],['mean']],\n",
            "[['days_diff'],['age_city'], ['user_days_diff_sum_x'],['std']],\n",
            "[['days_diff'],['level_city'], ['user_days_diff_sum_CV'],['x']],\n",
            "[['days_diff'],['level_city'], ['user_days_diff_sum_mean'],['x']],\n",
            "[['days_diff'],['level_city'], ['user_days_diff_sum_x'],['max']],\n",
            "[['days_diff'],['level_city'], ['user_days_diff_sum_std'],['x']],\n",
            "[['days_diff'],['age_service3'], ['user_days_diff_sum_x'],['mean']],\n",
            "[['days_diff'],['balance_login_days_cnt'], ['user_days_diff_sum_x'],['CV']],\n",
            "[['days_diff'],['balance_login_days_cnt'], ['user_days_diff_sum_x'],['std']],\n",
            "[['days_diff'],['level_using_time'], ['user_days_diff_sum_x'],['CV']],\n",
            "[['days_diff'],['level_using_time'], ['user_days_diff_sum_x'],['std']],\n",
            "[['days_diff'],['level_province'], ['user_days_diff_sum_x'],['CV']],\n",
            "[['days_diff'],['level_province'], ['user_days_diff_sum_x'],['max']],\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tb865aLw27VR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 133,
      "outputs": []
    }
  ]
}